{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pytz\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import pearsonr, mode\n",
    "from scipy.signal import savgol_filter\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "addressPrefix='C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Bobak/'\n",
    "if not os.path.exists(addressPrefix):\n",
    "    addressPrefix='C:/GDrive/Documents/Educational/TAMU/Research/CGM Dataset/Bobak/'\n",
    "\n",
    "addDataPrefix='/Users/sorush/My Drive/Documents/Educational/TAMU/Research/Trial/Data/11-5-21-11-15-21'\n",
    "\n",
    "addUserInput=os.path.join(addDataPrefix,'User inputted')\n",
    "addHKCM=os.path.join(addDataPrefix,'hk+cm')\n",
    "addCGM=os.path.join(addDataPrefix,'CGM')\n",
    "addE4=os.path.join(addDataPrefix,'E4')\n",
    "\n",
    "exempts=['p2']\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plt.style.use({'figure.facecolor':'white'})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(addDataPrefix,'All_meals.csv')):\n",
    "    os.chdir(addUserInput)\n",
    "    df=[]\n",
    "    for root, dirs, files in os.walk(addUserInput):\n",
    "        for file in files:\n",
    "            if '.csv' in file.lower():\n",
    "                if 'meals' in file.lower():\n",
    "                    participantName=file[:file.find('Meals')]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\",file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\",file)\n",
    "                    dfTemp = pd.read_csv(file)\n",
    "                    dfTemp.insert(0,'Participant',participantName)\n",
    "                    dfTemp.rename(columns={'startTime':'StartTime'}, inplace=True)\n",
    "                    dfTemp['StartTime']=pd.to_datetime(dfTemp['StartTime'])\n",
    "                    dfTemp['FinishTime']=pd.to_datetime(dfTemp['FinishTime'])\n",
    "\n",
    "                    dfTemp['StartTime']-=pd.DateOffset(hours=5)#fixing the time zone issue\n",
    "                    dfTemp['FinishTime']-=pd.DateOffset(hours=5)#fixing the time zone issue                    \n",
    "                    dfTemp.sort_values([\"Participant\",'StartTime'],ascending = (True, True),inplace=True)\n",
    "                    if len(dfTemp.columns)!=10:\n",
    "                        print(\"MAYDAY. Error in reading csv\")\n",
    "                        break\n",
    "                    if len(df)!=0:\n",
    "                        frames=[dfTemp,df]\n",
    "                        df=pd.concat(frames)\n",
    "                    else:\n",
    "                        df=dfTemp\n",
    "    print(\"reading is done\")\n",
    "    dfMeal=df\n",
    "    \n",
    "    # dfMeal.insert(2,'startDate',dfMeal['startTime'])\n",
    "    # dfMeal.insert(4,'finishDate',dfMeal['finishTime'])\n",
    "\n",
    "    # dfMeal['startTime']=dfMeal['startTime'].dt.time\n",
    "    # dfMeal['finishTime']=dfMeal['finishTime'].dt.time\n",
    "\n",
    "    # dfMeal['startTime']= dfMeal['startTime'].dt.hour*3600+dfMeal['startTime'].dt.minute*60+dfMeal['startTime'].dt.second+dfMeal['startTime'].dt.microsecond/1000/1000\n",
    "    # dfMeal['finishTime']= dfMeal['finishTime'].dt.hour*3600+dfMeal['finishTime'].dt.minute*60+dfMeal['finishTime'].dt.second+dfMeal['finishTime'].dt.microsecond/1000/1000\n",
    "\n",
    "    # dfMeal['startDate']=pd.to_datetime(dfMeal['startDate']).dt.date\n",
    "    # dfMeal['finishDate']=pd.to_datetime(dfMeal['finishDate']).dt.date\n",
    "    \n",
    "    dfMeal.to_csv(os.path.join(addDataPrefix,'All_meals.csv'),index=False)\n",
    "else:\n",
    "    dfMeal=pd.read_csv(os.path.join(addDataPrefix,'All_meals.csv'))\n",
    "    dfMeal['StartTime']=dfMeal['StartTime'].astype('datetime64[ns]')\n",
    "    dfMeal['FinishTime']=dfMeal['FinishTime'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(addDataPrefix,'All_cgm.csv')):\n",
    "    os.chdir(addCGM)\n",
    "    df=[]\n",
    "    for root, dirs, files in os.walk(addCGM):\n",
    "        for file in files:\n",
    "            if '.csv' in file.lower():\n",
    "                if 'processed' in file.lower():\n",
    "                    participantName=file[:file.find('Processed')]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\",file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\",file)\n",
    "                    dfTemp = pd.read_csv(file)\n",
    "                    dfTemp.insert(0,'Participant',participantName)\n",
    "                    dfTemp['Time']=pd.to_datetime(dfTemp['Time'])\n",
    "                    dfTemp.sort_values([\"Participant\",\"Time\"],ascending = (True,True),inplace=True)\n",
    "                    if len(dfTemp.columns)!=4:\n",
    "                        print(\"MAYDAY. Error in reading csv\")\n",
    "                        break\n",
    "                    if len(df)!=0:\n",
    "                        frames=[dfTemp,df]\n",
    "                        df=pd.concat(frames)\n",
    "                    else:\n",
    "                        df=dfTemp\n",
    "    print(\"reading is done\")\n",
    "    dfCGM=df\n",
    "    \n",
    "    # dfCGM.insert(1,'Date',dfCGM['Time'])\n",
    "    # dfCGM['Date']=pd.to_datetime(dfCGM['Date']).dt.date\n",
    "    # dfCGM['Time']=pd.to_datetime(dfCGM['Time'])\n",
    "    # dfCGM['Time']= dfCGM['Time'].dt.hour*3600+dfCGM['Time'].dt.minute*60\n",
    "    dfCGM['Abbot'].interpolate(method='linear',limit_direction='both',axis=0,inplace=True)\n",
    "    dfCGM['Dexcom'].interpolate(method='linear',limit_direction='both',axis=0,inplace=True)\n",
    "    dfCGM.to_csv(os.path.join(addDataPrefix,'All_cgm.csv'),index=False)\n",
    "else:\n",
    "    dfCGM=pd.read_csv(os.path.join(addDataPrefix,'All_cgm.csv'))\n",
    "    dfCGM['Time']=dfCGM['Time'].astype('datetime64[ns]')\n",
    "\n",
    "# mamad=dfCGM[dfCGM['Time']>dfCGM.iloc[5][1]]\n",
    "# print(mamad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ... aaa_cm.csv\n",
      "File is read\n",
      "column is added\n",
      "converted\n",
      "sorted\n",
      "reading is done\n",
      "Processing is done\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(addDataPrefix,'All_cm.csv')):\n",
    "    os.chdir(addHKCM)\n",
    "    df=[]\n",
    "    for root, dirs, files in os.walk(addHKCM):\n",
    "        for file in files:\n",
    "            if '.csv' in file.lower():\n",
    "                if 'cm' in file.lower() and 'aaa' in file:\n",
    "                    participantName=file[:file.find('_cm')]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\",file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\",file)\n",
    "                    dfTemp = pd.read_csv(file)\n",
    "                    print(\"File is read\")\n",
    "                    dfTemp.insert(0,'Participant',participantName)\n",
    "                    print(\"column is added\")\n",
    "                    dfTemp['Date']=pd.to_datetime(dfTemp['Date'])\n",
    "                    dfTemp['Date']=dfTemp['Date'].dt.tz_localize(None)\n",
    "                    print(\"converted\")\n",
    "                    dfTemp.sort_values(['Date'],ascending = (True),inplace=True)\n",
    "                    print(\"sorted\")\n",
    "                    dfTemp.rename(columns={'Date':'Time'}, inplace=True)\n",
    "                    if len(dfTemp.columns)!=13:\n",
    "                        print(\"MAYDAY. Error in reading csv\")\n",
    "                        break\n",
    "                    if len(df)!=0:\n",
    "                        frames=[dfTemp,df]\n",
    "                        df=pd.concat(frames)\n",
    "                    else:\n",
    "                        df=dfTemp\n",
    "    print(\"reading is done\")\n",
    "    dfCM=df\n",
    "    dfCM.drop(\"UID\",axis=1,inplace=True)\n",
    "    dfCM.drop(\"UnixTime\",axis=1,inplace=True)\n",
    "\n",
    "    # dfCM['Date']-=pd.DateOffset(hours=5)#fixing the time zone issue\n",
    "\n",
    "    # dfCM.insert(2,'Time',dfCM['Date'])\n",
    "    # dfCM['Date']=dfCM['Date'].dt.date\n",
    "    # dfCM['Time']=dfCM['Time'].dt.hour*3600+dfCM['Time'].dt.minute*60+dfCM['Time'].dt.second+dfCM['Time'].dt.microsecond/1000/1000\n",
    "    \n",
    "    print(\"Processing is done\")\n",
    "    dfCM.to_csv(os.path.join(addDataPrefix,'All_cm.csv'),index=False)\n",
    "else:\n",
    "    dfCM=pd.read_csv(os.path.join(addDataPrefix,'All_cm.csv'))\n",
    "    dfCM['Time']=dfCM['Time'].astype('datetime64[ns]')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Meal Format: Time, Carb, Fat, Protein to comply myfinesspal formating starting from Jan 31 til Feb 9\n",
    "# breakFast=[[31,'07-52-34',170,12,15],\n",
    "# [32,'07-09-13',170,12,60],\n",
    "# [33,'07-19-09',170,48,15],\n",
    "# [34,'06-47-38',170,12,15],\n",
    "# [35,'07-27-52',170,48,60],\n",
    "# [36,'07-18-46',42.5,12,15],\n",
    "# [37,'07-18-46',170,12,15],\n",
    "# [38,'07-44-07',170,12,60],\n",
    "# [39,'07-23-33',170,48,15],\n",
    "# [40,'06-50-00',170,12,15]]\n",
    "\n",
    "# lunch=[[31,'12-20-32',170,12,15],\n",
    "# [32,'12-19-09',170,12,60],\n",
    "# [33,'12-17-27',42.5,12,60],\n",
    "# [34,'12-11-35',170,12,60],\n",
    "# [35,'11-57-05',42.5,12,60],\n",
    "# [36,'12-35-21',170,48,60],\n",
    "# [37,'12-08-40',42.5,12,60],\n",
    "# [38,'11-22-23',170,12,15],\n",
    "# [39,'12-16-25',42.5,12,60],\n",
    "# [40,'12-12-34',170,48,15]]\n",
    "\n",
    "# dinner=[[31,'17-55-02',84,50,27],\n",
    "# [32,'17-21-14',49,28,41],\n",
    "# [33,'17-50-56',21,28,53],\n",
    "# [34,'17-45-17',70,35,39],\n",
    "# [35,'17-18-12',110,28,46],\n",
    "# [36,'17-41-39',53,51,67],\n",
    "# [37,'16-44-56',68,39,25],\n",
    "# [38,'15-58-07',71,52,82],\n",
    "# [39,'16-59-39',66,43,28],\n",
    "# [40,'17-44-56',122,47,51]]\n",
    "\n",
    "# exercise=[[30,'10-10-00','11-02-00'],\n",
    "#           [31,'16-07-00','17-10-00'],\n",
    "#           [33,'15-10-00','16-03-00'],\n",
    "#           [36,'15-30-00','16-23-00'],\n",
    "#           [37,'09-49-00','11-06-00'],\n",
    "#           [38,'09-34-00','10-34-00'],\n",
    "#           [40,'14-50-00','15-47-00'],\n",
    "#           [41,'16-12-00','17-07-00']]\n",
    "\n",
    "# mealData=[]\n",
    "# mealTemp=[]\n",
    "# mealTemp.extend(breakFast)\n",
    "# mealTemp.extend(lunch)\n",
    "# # mealTemp.extend(dinner)\n",
    "\n",
    "\n",
    "# for element in mealTemp:\n",
    "#     timeTemp = datetime.strptime(element[1], '%H-%M-%S')\n",
    "#     timeTemp=timeTemp.time()\n",
    "#     timeTemp=timeTemp.hour*3600+timeTemp.minute*60+timeTemp.second\n",
    "#     mealData.append([element[0],timeTemp,element[2],element[3],element[4]])\n",
    "\n",
    "# dfMacro=pd.DataFrame(data=mealData,columns=['Date','Time','Carb','Fat','Protein'])\n",
    "\n",
    "# activity=[]\n",
    "# for element in exercise:\n",
    "#     timeTempStart = datetime.strptime(element[1], '%H-%M-%S')\n",
    "#     timeTempStart=timeTempStart.time()\n",
    "#     timeTempStart=timeTempStart.hour*3600+timeTempStart.minute*60+timeTempStart.second\n",
    "\n",
    "#     timeTempEnd = datetime.strptime(element[2], '%H-%M-%S')\n",
    "#     timeTempEnd=timeTempEnd.time()\n",
    "#     timeTempEnd=timeTempEnd.hour*3600+timeTempEnd.minute*60+timeTempEnd.second\n",
    "#     activity.append([element[0],timeTempStart,timeTempEnd])\n",
    "# dfActivity=pd.DataFrame(activity,columns=['Date','Start','End'])\n",
    "# dfActivity.sort_values(by=['Date','Start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def loadingDataReader(fileAddress,parameter):\n",
    "#     try:\n",
    "#         jsonFile = open(fileAddress, )\n",
    "#         data = json.load(jsonFile)\n",
    "\n",
    "#         dateList=[]\n",
    "#         timeList=[]\n",
    "#         valList=[]\n",
    "#         for i in data:\n",
    "#             for j in data[i]:\n",
    "#                 timeTemp=j[\"time\"]\n",
    "#                 timeTemp=int(timeTemp[0:2])*3600+int(timeTemp[3:5])*60+int(timeTemp[6:8])\n",
    "#                 timeList.append(timeTemp)\n",
    "\n",
    "#                 dateTemp=i\n",
    "#                 dateTemp = datetime.strptime(dateTemp, '%m-%d')+relativedelta(years=+121)\n",
    "#                 dateTemp=dateTemp.timetuple().tm_yday\n",
    "#                 dateList.append(dateTemp)\n",
    "#                 valList.append(float(j[\"value\"]))\n",
    "#         jsonFile.close()\n",
    "#     except:\n",
    "#         print(\"An exception occurred\")\n",
    "\n",
    "#     parList = []\n",
    "#     for i in range (0,len(dateList)):\n",
    "#         parList.append(parameter)\n",
    "\n",
    "#     return dateList,timeList,valList,parList\n",
    "\n",
    "# def loadingDataAux(fileAddress,parameter,allDate,allTime,allVal,allPar):\n",
    "#     fileAddress=os.path.normpath(fileAddress)\n",
    "#     [tempDate,tempTime,tempVal,tempPar]=loadingDataReader(fileAddress,parameter)\n",
    "#     allDate.extend(tempDate)\n",
    "#     allTime.extend(tempTime)\n",
    "#     allVal.extend(tempVal)\n",
    "#     allPar.extend(tempPar)\n",
    "#     print(parameter,\" is done\")\n",
    "\n",
    "#     return allDate,allTime,allVal,allPar\n",
    "\n",
    "# def dataResEnhancer(df):\n",
    "#     gapLength=1\n",
    "#     df=df.sort_values(by=['Parameter', 'Date','Time'])\n",
    "#     gapList=[]\n",
    "#     highResSensor=['Acc','EDA','HR','Temp','CGM','Step','Cal']\n",
    "#     for sensor in highResSensor:\n",
    "#         newQuery=df[df['Parameter']==sensor]\n",
    "#         for counter in range(0,len(newQuery)-1):\n",
    "#             if newQuery.iloc[counter+1,0]==newQuery.iloc[counter+1,0]:\n",
    "#                 if newQuery.iloc[counter+1,1]-newQuery.iloc[counter,1]>gapLength:\n",
    "#                     gapList.append([newQuery.iloc[counter,0],newQuery.iloc[counter,1],newQuery.iloc[counter+1,1],newQuery.iloc[counter,3]])\n",
    "#     appendedData=[]\n",
    "#     for element in gapList:\n",
    "#         duration= element[2]-element[1]\n",
    "#         if duration%gapLength==0:\n",
    "#             fillerNumber=int(duration/gapLength-1)\n",
    "#         else:\n",
    "#             fillerNumber=int(duration/gapLength)\n",
    "#         for counter in range(0,fillerNumber):\n",
    "#             appendedData.append([element[0],element[1]+(counter+1)*gapLength,float('nan'),element[3]])\n",
    "\n",
    "#     df = df.append(pd.DataFrame(appendedData,columns=['Date','Time','Value','Parameter']),ignore_index = True)\n",
    "#     df=df.sort_values(by=['Parameter','Date','Time'])\n",
    "\n",
    "#     df=df.interpolate(method='linear')\n",
    "#     return df\n",
    "\n",
    "# def dataCleaner(df):\n",
    "#     i = df[df.Date == 30].index #first data and noisy\n",
    "#     df=df.drop(i)\n",
    "\n",
    "#     i = df[df.Date == 40].index #Partial E4\n",
    "#     df=df.drop(i)\n",
    "\n",
    "#     i = df[df.Date == 41].index #No E4\n",
    "#     df=df.drop(i)\n",
    "\n",
    "#     i = df[df.Date >= 42].index #CGM becomes too noisy at the end of the study\n",
    "#     df=df.drop(i)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def dataAmputator(df):\n",
    "#     amputations=[[31,38290,39010],[31,61937,62638],[31,66226,71099],\n",
    "#               [32,58667,59563],[32,63875,63902],\n",
    "#               [33,25127,34731],[33,57996,58812],\n",
    "#               [34,25556,30763],[34,58736,59877],[34,66069,66092],\n",
    "#               [35,27945,31416],[35,57283,58147],\n",
    "#               [36,29865,33926],[36,59231,60096],\n",
    "#               [37,26911,32544],[37,40214,41380],\n",
    "#               [38,26707,34560],[38,38172,39273],\n",
    "#               [39,31053,35335],[39,57408,58655]]\n",
    "\n",
    "#     for amputation in amputations:\n",
    "#         i=df[(df['Date']==amputation[0]) & (df['Time']>=amputation[1]) & (df['Time']<=amputation[2])].index\n",
    "#         df.loc[i,'Value']=float('nan')\n",
    "#         # df=df.drop(i)\n",
    "#     df=df.interpolate(method='linear')\n",
    "#     return df\n",
    "\n",
    "# def loadingData(addressPrefix,dfActivity):\n",
    "#     if not os.path.exists(addressPrefix+'Result-interpolated.csv'):\n",
    "#         allDate=[]\n",
    "#         allTime=[]\n",
    "#         allVal=[]\n",
    "#         allPar=[]\n",
    "\n",
    "#         sensorList=[['intraday-ACC_E4.json','Acc'],['intraday-calories.json','Cal'],\n",
    "#                     ['intraday-EDA_E4.json','EDA'],['intraday-HR_E4.json','HR'],\n",
    "#                     ['intraday-steps.json','Step'],['intraday-TEMP_E4.json','Temp'],\n",
    "#                     ['intraday-glucose.json','CGM']]\n",
    "\n",
    "#         for element in sensorList:\n",
    "#             fileAddress=addressPrefix+element[0]\n",
    "#             allDate,allTime,allVal,allPar=loadingDataAux(fileAddress,element[1],allDate,allTime,allVal,allPar)\n",
    "\n",
    "#         dfOriginal = pd.DataFrame(list(zip(allDate,allTime,allVal,allPar)),\n",
    "#                           columns =['Date','Time','Value','Parameter'])\n",
    "#         dfOriginal=dfOriginal.sort_values(by=['Parameter','Date','Time'])\n",
    "#         dfOriginal.to_csv(addressPrefix+'Result-original.csv', header=True,index=False)\n",
    "#         print(\"Result-Original is saved\")\n",
    "#         dfInterp=dataCleaner(dfOriginal)\n",
    "#         dfActivity=dataCleaner(dfActivity)\n",
    "#         # dfInterp=dataResEnhancer(dfInterp)\n",
    "#         dfInterp=dataAmputator(dfInterp)\n",
    "#         dfInterp.to_csv(addressPrefix+'Result-interpolated.csv', header=True,index=False)\n",
    "#         print(\"Result-Interpolated is saved\")\n",
    "#     else:\n",
    "#         dfOriginal=pd.read_csv(addressPrefix+'Result-original.csv')\n",
    "#         dfInterp=pd.read_csv(addressPrefix+'Result-interpolated.csv')\n",
    "#     return dfOriginal,dfInterp,dfActivity\n",
    "\n",
    "# dfOriginal,dfInterp,dfActivity=loadingData(addressPrefix,dfActivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def activityExtractor(dfInterp):\n",
    "#     data=[]\n",
    "#     for i in range(len(dfActivity)):\n",
    "#         currentDay=dfActivity.iloc[i,0]\n",
    "#         dfTemp=dfInterp[dfInterp['Date']==currentDay]\n",
    "#         dfTemp=dfTemp[dfTemp['Time']>=dfActivity.iloc[i,1]]\n",
    "#         dfTemp=dfTemp[dfTemp['Time']<=dfActivity.iloc[i,1]+15*60]\n",
    "#         for j in range(len(dfTemp)):\n",
    "#             data.append([currentDay,dfTemp.iloc[j,1],dfTemp.iloc[j,2],dfTemp.iloc[j,3],1])\n",
    "#     for i in range(len(dfActivity)):\n",
    "#         currentDay=dfActivity.iloc[i,0]\n",
    "#         dfTemp=dfInterp[dfInterp['Date']==currentDay]\n",
    "#         dfTemp=dfTemp[dfTemp['Time']>=dfActivity.iloc[i,1]-15*60]\n",
    "#         dfTemp=dfTemp[dfTemp['Time']<=dfActivity.iloc[i,1]]\n",
    "#         for j in range(len(dfTemp)):\n",
    "#             data.append([currentDay,dfTemp.iloc[j,1],dfTemp.iloc[j,2],dfTemp.iloc[j,3],0])\n",
    "#     dfData=pd.DataFrame(data,columns=['Date','Time','Value','Parameter','ActivityFlag'])\n",
    "#     return dfData\n",
    "\n",
    "# dfInterp=dfInterp.loc[(dfInterp['Parameter']=='EDA') | (dfInterp['Parameter']=='HR') | (dfInterp['Parameter']=='Temp')]\n",
    "# dfData=activityExtractor(dfInterp)\n",
    "# dfData.to_csv(os.path.join(addressPrefix,'activity-tTest.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def highResAccConvertor(data,startTime):\n",
    "#     processedData=[]\n",
    "#     for counter,row in enumerate(data):\n",
    "#         if counter%2==0:\n",
    "#             continue\n",
    "#         tempTime=startTime+counter/32\n",
    "#         tempTime=datetime.fromtimestamp(tempTime)\n",
    "#         tempDay=tempTime.timetuple().tm_yday\n",
    "#         tempDay=int(tempDay)\n",
    "#         tempTime=tempTime.microsecond/1000000+tempTime.second+tempTime.minute*60+tempTime.hour*3600\n",
    "#         axis=['AccX','AccY','AccZ']\n",
    "#         for axe in range(0,3):\n",
    "#             processedData.append([tempDay,tempTime,int(row[axe])/64,axis[axe]])\n",
    "#     return processedData\n",
    "\n",
    "# def highResAcc(addressPrefix):\n",
    "#     addressPrefix=os.path.join(addressPrefix,'E4_Data/')\n",
    "#     E4AccFiles=[]\n",
    "#     data=[]\n",
    "#     for root, dirs, files in os.walk(addressPrefix, topdown=False):\n",
    "#        for name in files:\n",
    "#            if 'ACC' in name:\n",
    "#                E4AccFiles.append(os.path.join(root,name))\n",
    "#     for element in E4AccFiles:\n",
    "#         print(element)\n",
    "#         rows = []\n",
    "#         with open(element, 'r') as csvfile:\n",
    "#             csvreader = csv.reader(csvfile)\n",
    "#             fields = next(csvreader)\n",
    "#             for row in csvreader:\n",
    "#                 rows.append(row)\n",
    "#         startTime=fields[0]\n",
    "#         startTime=float(startTime)\n",
    "#         startTime=int(startTime)\n",
    "#         data.extend(highResAccConvertor(rows,startTime))\n",
    "#     df = pd.DataFrame(data,columns=['Date','Time','Value','Parameter'])\n",
    "#     df=df.sort_values(by=['Date','Time','Parameter'])\n",
    "#     df.to_csv(os.path.join(addressPrefix,'AccHighRes'))\n",
    "#     return df\n",
    "\n",
    "# if os.path.exists(os.path.join(addressPrefix,'AccHighRes.csv')):\n",
    "#     dfAccHighRes=pd.read_csv(os.path.join(addressPrefix,'AccHighRes.csv'))\n",
    "# else:\n",
    "#     dfAccHighRes=highResAcc(addressPrefix)\n",
    "# print(dfAccHighRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def XGClassifier(dataList, labelList,randomSeed):\n",
    "#     trainData, testData, trainLabels, testLabels = train_test_split(dataList, labelList, test_size=0.33,random_state=randomSeed)\n",
    "#     accuracyBest=0\n",
    "#     for maxDepth in np.arange(3,30):\n",
    "#         for estimator in np.arange(5,50,2):\n",
    "#             clf = xgb.XGBClassifier(n_estimators=estimator,max_depth=maxDepth,objective = \"binary:logistic\",eval_metric = \"logloss\",use_label_encoder =False)\n",
    "#             clf.fit(trainData, trainLabels)\n",
    "#             slidingWindowPrediction = clf.predict_proba(testData)\n",
    "#             slidingWindowPrediction=slidingWindowPrediction[:,1]\n",
    "#             slidingWindowPrediction[slidingWindowPrediction>=0.5]=1\n",
    "#             slidingWindowPrediction[slidingWindowPrediction<0.5]=0\n",
    "\n",
    "#             confMatrix=sklearn.metrics.confusion_matrix(testLabels,slidingWindowPrediction)\n",
    "#             accuracy=sklearn.metrics.accuracy_score(testLabels,slidingWindowPrediction)\n",
    "#             recall=sklearn.metrics.recall_score(testLabels,slidingWindowPrediction)\n",
    "#             precision=sklearn.metrics.precision_score(testLabels,slidingWindowPrediction)\n",
    "\n",
    "#             if accuracy>accuracyBest:\n",
    "#                 confMatrixBest=confMatrix\n",
    "#                 accuracyBest=accuracy\n",
    "#                 modelBest=clf\n",
    "#                 recallBest=recall\n",
    "#                 precisionBest=precision\n",
    "#     # print(confMatrixBest)\n",
    "#     print(np.round(100*accuracyBest,0),np.round(100*recallBest,0),np.round(100*precisionBest,0))\n",
    "\n",
    "# def statAnalyzer(dfInterp,sensors):\n",
    "#     print(sensors)\n",
    "#     dfMeal=pd.DataFrame([],columns=['Date','Time','Value','Parameter'])\n",
    "#     dfBeforeMeal=pd.DataFrame([],columns=['Date','Time','Value','Parameter'])\n",
    "#     days=dfInterp['Date'].to_list()\n",
    "\n",
    "#     days=list(set(days))\n",
    "#     mealList=[]\n",
    "#     mealList.extend(breakFast)\n",
    "#     mealList.extend(lunch)\n",
    "#     mealList.extend(dinner)\n",
    "\n",
    "#     dataList=[]\n",
    "#     labelList=[]\n",
    "#     for mealCat in range(0,2):\n",
    "#         for element in mealList:\n",
    "#             sensorData=[]\n",
    "#             for sensor in sensors:\n",
    "#                 mealTime=datetime.strptime(element[1], '%H-%M-%S')\n",
    "#                 mealTime=mealTime.time().second+mealTime.time().minute*60+mealTime.time().hour*3600\n",
    "\n",
    "#                 dfTemp=dfInterp[dfInterp['Date']==element[0]]\n",
    "#                 dfTemp=dfTemp[dfTemp['Parameter']==sensor]\n",
    "#                 dfTemp=dfTemp[dfTemp['Time']>mealTime-mealCat*20*60]\n",
    "#                 dfTemp=dfTemp[dfTemp['Time']<=mealTime+(1-mealCat)*20*60]\n",
    "#                 dfTemp['Value']=dfTemp['Value']\n",
    "#                 dfTemp.insert(4,'mealMoment',1)\n",
    "#                 dfMeal = dfMeal.append(dfTemp, ignore_index=True)\n",
    "\n",
    "#                 tempData=dfTemp['Value'].to_list()\n",
    "#                 sensorData.extend(tempData)\n",
    "#             labelList.append(1-mealCat)\n",
    "#             dataList.append(sensorData)\n",
    "\n",
    "#     dfMeal=dfMeal.append(dfBeforeMeal,ignore_index=True)\n",
    "#     dfMeal=dfMeal.sort_values(by=['Parameter', 'Date','Time'])\n",
    "\n",
    "#     dataListNew=[]\n",
    "#     labelListNew=[]\n",
    "\n",
    "#     for counter in range(0,len(dataList)):\n",
    "#         if len(dataList[counter])==len(sensors)*1200:\n",
    "#             dataListNew.append(dataList[counter])\n",
    "#             labelListNew.append(labelList[counter])\n",
    "\n",
    "#     dataList=np.asarray(dataListNew)\n",
    "#     labelList=np.asarray(labelListNew)\n",
    "\n",
    "#     return dfMeal,dataList,labelList\n",
    "\n",
    "# sensorList=[['EDA','Temp'],['EDA'],['Temp']]\n",
    "# for randomSeed in np.arange(10,100,10):\n",
    "#     for sensors in sensorList:\n",
    "#         dfMeal,dataList,labelList=statAnalyzer(dfInterp,sensors)\n",
    "#         XGClassifier(dataList, labelList,randomSeed)\n",
    "#     print('------------------')\n",
    "# dfMeal.to_csv('lunch.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def featureExtractorMotion(df):\n",
    "    f1=0\n",
    "    f2=0\n",
    "    f3=0\n",
    "    for counter in range(len(df)):\n",
    "        temp1=abs(df['Yaw'][counter])+abs(df['Pitch'][counter])+abs(df['Roll'][counter])\n",
    "        temp2=abs(df['Ax'][counter])+abs(df['Ay'][counter])+abs(df['Az'][counter])\n",
    "        f2+=temp2\n",
    "        f3+=temp1\n",
    "        if(temp2==0):\n",
    "            temp2+=0.001#We had to truncate the sensor data in the transmition process. We are adding epsilon to fix it\n",
    "        f1+=temp1/temp2\n",
    "    f1/=len(df)\n",
    "    f2/=len(df)\n",
    "    return [f1,f2]\n",
    "\n",
    "def featureExtractorCGM(df):\n",
    "    mean=df['Abbot'].mean()\n",
    "    std=df['Abbot'].std()\n",
    "    max=df['Abbot'].max()\n",
    "    min=df['Abbot'].min()\n",
    "    return mean, std, min,max\n",
    "\n",
    "MINIMUM_POINT_CM=10\n",
    "WINDOW_LENGTH=timedelta(minutes=1)\n",
    "WINDOW_STEP=timedelta(minutes=1)\n",
    "MEAL_PORTION_RELAXATION=timedelta(seconds=10)\n",
    "\n",
    "participants=dfMeal['Participant'].to_list()\n",
    "participants=list(set(participants))\n",
    "startofTrial = datetime.strptime('11 06 2021-13:58:00', '%m %d %Y-%H:%M:%S')\n",
    "endofTrial = datetime.strptime('11 15 2021-00:00:00', '%m %d %Y-%H:%M:%S')\n",
    "\n",
    "windowStart=startofTrial\n",
    "windowEnd=windowStart+WINDOW_LENGTH\n",
    "for participant in participants:\n",
    "    participantList=[]\n",
    "    while(windowStart<endofTrial):\n",
    "        dfTempCM=dfCM.loc[(dfCM['Time']>=windowStart) & (dfCM['Time']<=windowEnd)& dfCM['Participant']==participant]\n",
    "        dfTempMeal=dfMeal.loc[(dfMeal['StartTime']-MEAL_PORTION_RELAXATION>=windowStart) & (dfTempMeal['FinishTime']+MEAL_PORTION_RELAXATION<=windowEnd)& (dfCM['Participant']==participant)]\n",
    "        if(len(dfTempCM)<MINIMUM_POINT_CM):\n",
    "            windowStart+=WINDOW_STEP\n",
    "            windowEnd+=WINDOW_STEP\n",
    "            continue\n",
    "        tempList=featureExtractorMotion(dfTempCM)\n",
    "        if(len(dfTempMeal)!=0):\n",
    "            tempList.append(1)\n",
    "        else:\n",
    "            tempList.append(0)\n",
    "        participantList.append(tempList)\n",
    "        windowStart+=WINDOW_STEP\n",
    "        windowEnd+=WINDOW_STEP  \n",
    "    break\n",
    "print(participantList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfInterp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r9/63tl846j3ksdwv99wh0gv21m0000gn/T/ipykernel_13142/1812573125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0mrandomSeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainLabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestTrainSplitFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfInterp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdfMacro\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandomSeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandomSeed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontextFlag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmreCGMCon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictionCGMCon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTMI_XGBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainLabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0mpearsonCGMCon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionCGMCon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfInterp' is not defined"
     ]
    }
   ],
   "source": [
    "def STMI_XGBoost(trainData,testData,trainLabels,testLabels):\n",
    "    mseBest=math.inf\n",
    "    depthBest=float('nan')\n",
    "    estimatorBest=float('nan')\n",
    "    modelBest=''\n",
    "\n",
    "    for numEstimator in np.arange(10,120,10):\n",
    "        for maxDepth in np.arange(3,10):\n",
    "            clf = xgb.XGBRegressor(n_estimators =numEstimator,max_depth=maxDepth,objective ='reg:squarederror',eval_metric = \"logloss\")\n",
    "            clf.fit(trainData, trainLabels)\n",
    "            testPrediction = clf.predict(testData)\n",
    "            mre=[]\n",
    "            for counter in range(0,len(testPrediction)):\n",
    "                mre.append(math.fabs(testPrediction[counter]-testLabels[counter])/testLabels[counter])\n",
    "            mre=np.asarray(mre)\n",
    "            mre=np.mean(mre)\n",
    "            if mre<mseBest:\n",
    "                mreBest=mre\n",
    "                depthBest=maxDepth\n",
    "                estimatorBest=numEstimator\n",
    "                modelBest=clf\n",
    "                predictionBest=testPrediction\n",
    "    print(clf.feature_importances_)\n",
    "    return modelBest,mreBest,predictionBest,depthBest,estimatorBest\n",
    "\n",
    "def aucCalculator(timeTemp,dataTemp,kernelNumber):\n",
    "    kernelMeans=np.linspace(0,24,kernelNumber+2)\n",
    "    kernelMeans=np.delete(kernelMeans,0)\n",
    "    kernelMeans=np.delete(kernelMeans,-1)\n",
    "    kernelMeans=kernelMeans*3600\n",
    "\n",
    "    kernelSTD=2*3600\n",
    "    aucData=[]\n",
    "    for kernelMean in kernelMeans:\n",
    "        kernelTemp=np.exp(-((timeTemp-kernelMean)**2)/(2*kernelSTD**2))\n",
    "        tempAuc=kernelTemp*dataTemp\n",
    "        tempAuc=np.trapz(tempAuc,x=timeTemp)\n",
    "        aucData.append(tempAuc/3600)\n",
    "\n",
    "    return aucData\n",
    "\n",
    "def statisticalFeature(timeTemp,dataTemp):\n",
    "    featureData=[]\n",
    "    featureList=[]\n",
    "\n",
    "    featureData.append(np.mean(dataTemp))\n",
    "    featureData.append(np.max(dataTemp)-np.min(dataTemp))\n",
    "    featureData.append(np.var(dataTemp))\n",
    "\n",
    "    featureData.append(np.mean(np.diff(dataTemp,n=1)))\n",
    "    # featureData.append(np.median(np.diff(dataTemp,n=1)))\n",
    "    featureData.append(np.max(np.diff(dataTemp,n=1)))\n",
    "    featureData.append(np.min(np.diff(dataTemp,n=1)))\n",
    "    # featureData.append(np.var(np.diff(dataTemp,n=1)))\n",
    "\n",
    "    featureData.append(np.mean(np.diff(dataTemp,n=2)))\n",
    "    # featureData.append(np.median(np.diff(dataTemp,n=2)))\n",
    "    # featureData.append(np.max(np.diff(dataTemp,n=2)))\n",
    "    # featureData.append(np.min(np.diff(dataTemp,n=2)))\n",
    "    # featureData.append(np.var(np.diff(dataTemp,n=2)))\n",
    "\n",
    "    featureData.append(np.trapz(dataTemp)/3600)\n",
    "    return featureData\n",
    "\n",
    "def featureExtractor(df,contextFlag):\n",
    "    featureList=[]\n",
    "    sensors=['CGM','EDA', 'Acc','Temp', 'HR','Cal']\n",
    "\n",
    "\n",
    "    #-----------------I removed the STEP as it was almost always zero. We should look into it later------------\n",
    "    featureLabels=[]\n",
    "    for sensor in sensors:\n",
    "\n",
    "        varData=df[df['Parameter']==sensor]\n",
    "        varTime=varData['Time'].tolist()\n",
    "        varData=varData['Value'].tolist()\n",
    "\n",
    "        varTime=np.asarray(varTime)\n",
    "        varData=np.asarray(varData)\n",
    "        if contextFlag:\n",
    "            featureList.extend(statisticalFeature(varTime,varData))\n",
    "            featureLabels.append([sensor,'mean'])\n",
    "            featureLabels.append([sensor,'max'])\n",
    "            featureLabels.append([sensor,'var'])\n",
    "            featureLabels.append([sensor,'S-mean'])\n",
    "            featureLabels.append([sensor,'S-max'])\n",
    "            featureLabels.append([sensor,'S-min'])\n",
    "            featureLabels.append([sensor,'SS-mean'])\n",
    "            featureLabels.append([sensor,'trapz'])\n",
    "        if sensor=='CGM':\n",
    "            featureList.extend(aucCalculator(varTime,varData,9))\n",
    "            for i in range(0,9):\n",
    "                featureLabels.append([sensor,'auc'+str(i)])\n",
    "            featureList.extend(aucCalculator(varTime,varData,6))\n",
    "            for i in range(0,6):\n",
    "                featureLabels.append([sensor,'auc'+str(i)])\n",
    "    print(\"Used features\",featureLabels)\n",
    "    #REDO IT WITH FITBIT AND E4\n",
    "    #FOR MEAL TO MEAL PERSPECTIVE, MAKE THE WHOLE DAY INTO 3 CHUNCKS (MEAL TO MEAL)\n",
    "    #PLOT DAY-DAY VS MEAL TO MEAL AND SHOW THEM HOW WE ARE SEAPRATING THINGS (CGM,EDA,...)\n",
    "\n",
    "    #https://github.com/fraunhoferportugal/tsfel/tree/9319db4368303cf10adb3aeb72cd4235a8085307\n",
    "    return featureList\n",
    "\n",
    "def testTrainSplitFuncAux(dfSensorTemp,dfMacroTemp,contextFlag):\n",
    "    sensorDataList=[]\n",
    "    macroDataList=[]\n",
    "    days=list(set(dfSensorTemp['Date']))\n",
    "\n",
    "    for day in days:\n",
    "        dfSensorTempDay=dfSensorTemp[dfSensorTemp['Date']==day]\n",
    "        sensorDataList.append(featureExtractor(dfSensorTempDay,contextFlag))\n",
    "        dfMacroTempDay=dfMacroTemp[dfMacroTemp['Date']==day]\n",
    "        totalCarb=dfMacroTempDay['Carb'].sum()\n",
    "        totalFat=dfMacroTempDay['Fat'].sum()\n",
    "        totalProtein=dfMacroTempDay['Protein'].sum()\n",
    "        macroDataList.append([totalCarb,totalFat,totalProtein])\n",
    "\n",
    "    return sensorDataList,macroDataList\n",
    "\n",
    "def featureNormalizer(trainData,testData):\n",
    "    # repeatingNum=trainData.shape\n",
    "    # repeatingNum=repeatingNum[0]\n",
    "    #\n",
    "    # columnMeans=np.mean(trainData,axis=0)\n",
    "    # columnMeans = np.repeat(columnMeans[:, np.newaxis], repeatingNum, axis=1)\n",
    "    # columnMeans=np.transpose(columnMeans)\n",
    "    # trainData-=columnMeans\n",
    "    #\n",
    "    # columnSTD=np.std(trainData,axis=0)\n",
    "    # columnSTD = np.repeat(columnSTD[:, np.newaxis], repeatingNum, axis=1)\n",
    "    # columnSTD=np.transpose(columnSTD)\n",
    "    # trainData/=columnSTD\n",
    "    #\n",
    "    # repeatingNum=testData.shape\n",
    "    # repeatingNum=repeatingNum[0]\n",
    "    #\n",
    "    # columnMeans=np.mean(trainData,axis=0)\n",
    "    # columnMeans = np.repeat(columnMeans[:, np.newaxis], repeatingNum, axis=1)\n",
    "    # columnMeans=np.transpose(columnMeans)\n",
    "    # testData-=columnMeans\n",
    "    #\n",
    "    # columnSTD=np.std(trainData,axis=0)\n",
    "    # columnSTD = np.repeat(columnSTD[:, np.newaxis], repeatingNum, axis=1)\n",
    "    # columnSTD=np.transpose(columnSTD)\n",
    "    # testData/=columnSTD\n",
    "    scaler = StandardScaler()\n",
    "    trainData = scaler.fit_transform(trainData)\n",
    "    testData = scaler.fit_transform(testData)\n",
    "\n",
    "    return trainData,testData\n",
    "\n",
    "def testTrainSplitFunc(dfSensor, dfMacro,randomSeed,contextFlag):\n",
    "    days=list(set(dfSensor['Date']))\n",
    "    random.seed(randomSeed)\n",
    "    random.shuffle(days)\n",
    "    daysTrain=days[0:6]\n",
    "    daysTest=days[6:9]\n",
    "\n",
    "    dfSensorTrain = dfSensor[dfSensor['Date'].isin(daysTrain)]\n",
    "    dfSensorTest = dfSensor[dfSensor['Date'].isin(daysTest)]\n",
    "\n",
    "    dfMacroTrain= dfMacro[dfMacro['Date'].isin(daysTrain)]\n",
    "    dfMacroTest= dfMacro[dfMacro['Date'].isin(daysTest)]\n",
    "\n",
    "    trainData,trainLabel=testTrainSplitFuncAux(dfSensorTrain,dfMacroTrain,contextFlag)\n",
    "    testData,testLabel=testTrainSplitFuncAux(dfSensorTest,dfMacroTest,contextFlag)\n",
    "\n",
    "    trainData=np.asarray(trainData)\n",
    "    testData=np.asarray(testData)\n",
    "\n",
    "    trainLabel=np.asarray(trainLabel)\n",
    "    testLabel=np.asarray(testLabel)\n",
    "\n",
    "    trainLabel=trainLabel[:,0]\n",
    "    testLabel=testLabel[:,0]\n",
    "    trainData,testData=featureNormalizer(trainData,testData)\n",
    "    # for i in range(0,testData.shape[1]):\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(testData[:,i])\n",
    "    #     plt.title(str(i))\n",
    "    return trainData,testData,trainLabel,testLabel\n",
    "\n",
    "randomSeed=random.randrange(100)\n",
    "trainData,testData,trainLabel,testLabel=testTrainSplitFunc(dfInterp,dfMacro,randomSeed=randomSeed,contextFlag=True)\n",
    "_,mreCGMCon,predictionCGMCon,_,_=STMI_XGBoost(trainData,testData,trainLabel,testLabel)\n",
    "pearsonCGMCon=pearsonr(predictionCGMCon,testLabel)\n",
    "pearsonCGMCon=pearsonCGMCon[0]\n",
    "\n",
    "trainData,testData,trainLabel,testLabel=testTrainSplitFunc(dfInterp,dfMacro,randomSeed=randomSeed,contextFlag=False)\n",
    "_,mreCGM,predictionCGM,_,_=STMI_XGBoost(trainData,testData,trainLabel,testLabel)\n",
    "pearsonCGM=pearsonr(predictionCGM,testLabel)\n",
    "pearsonCGM=pearsonCGM[0]\n",
    "\n",
    "plt.scatter(x=testLabel,y=predictionCGM,color='red',label='CGM')\n",
    "plt.scatter(x=testLabel,y=predictionCGMCon,color='blue',label='CGM+Context')\n",
    "plt.plot([200,600],[200,600],':k')\n",
    "# plt.scatter(x=daysTest,y=predictionBestCGMContext,label='Prediction CGM+Con')\n",
    "# plt.scatter(x=daysTest,y=predictionBestCGM,label='Prediction CGM')\n",
    "plt.annotate('CGM Error'+str(round(100*mreCGM,2))+'%', xy=(1.05, 0.95), xycoords='axes fraction')\n",
    "plt.annotate('CGMCon Error'+str(round(100*mreCGMCon,2))+'%', xy=(1.05, 0.85), xycoords='axes fraction')\n",
    "\n",
    "plt.annotate('CGM Pearson'+str(round(100*pearsonCGM,2))+'%', xy=(1.05, 0.75), xycoords='axes fraction')\n",
    "plt.annotate('CGMCon Pearson'+str(round(100*pearsonCGMCon,2))+'%', xy=(1.05, 0.65), xycoords='axes fraction')\n",
    "\n",
    "plt.xlabel('Actual Carb [gr]')\n",
    "plt.ylabel('Predicted Carb [gr]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('C:\\GitHub\\Carb.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65e249be015da37cfe4cfdd1da028fe4cb475c26be679b9e3e34696278a17c64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

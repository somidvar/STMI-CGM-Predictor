{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from dateutil.tz import tzutc\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from scipy.stats import pearsonr, mode, skew, kurtosis\n",
    "from scipy.signal import savgol_filter\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, ShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "\n",
    "addDataPrefix = \"/Users/sorush/My Drive/Documents/Educational/TAMU/Research/Trial/Data/11-5-21-11-15-21\"\n",
    "if not os.path.exists(addDataPrefix):\n",
    "    addDataPrefix = \"/home/grads/s/sorush.omidvar/CGMDataset/Trial/Data/11-5-21-11-15-21\"\n",
    "if not os.path.exists(addDataPrefix):\n",
    "    addDataPrefix = \"C:\\\\GDrive\\\\Documents\\\\Educational\\\\TAMU\\\\Research\\\\Trial\\\\Data\\\\11-5-21-11-15-21\"\n",
    "\n",
    "addUserInput = os.path.join(addDataPrefix, \"User inputted\")\n",
    "addHKCM = os.path.join(addDataPrefix, \"hk+cm\")\n",
    "addCGM = os.path.join(addDataPrefix, \"CGM\")\n",
    "addE4 = os.path.join(addDataPrefix, \"E4\")\n",
    "\n",
    "exempts = [\"p2\"]\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plt.style.use({\"figure.facecolor\": \"white\"})\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # no GPU\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_rows\", 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ... p3Meals-Modified.xlsx\n",
      "Reading ... p4Meals-Modified.xlsx\n",
      "Reading ... p1Meals-Modified.xlsx\n",
      "reading is done\n",
      "Meal database is limited to the trial period\n",
      "Reading ... p3_fl.txt\n",
      "Exemption... p2_fl.txt\n",
      "Reading ... p1_fl.txt\n",
      "Reading ... p4_fl.txt\n",
      "reading is done\n",
      "CGM database is limited to the trial period\n",
      "Reading ... p4_cm_all_modified.csv\n",
      "File is read\n",
      "modified\n",
      "sorted\n",
      "Reading ... p1_cm_all_modified.csv\n",
      "File is read\n",
      "modified\n",
      "sorted\n",
      "Exemption... p2_cm_all_modified.csv\n",
      "Reading ... p3_cm_all_modified.csv\n",
      "File is read\n",
      "modified\n",
      "sorted\n",
      "Processing is done\n",
      "CM database is limited to the trial period\n",
      "File name does not comply with analyzed fields FinalReport.csv\n",
      "File name does not comply with analyzed fields mamad.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p4 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p4 HR\n",
      "Reading ... HR.csv\n",
      "p4 BVP\n",
      "Reading ... BVP.csv\n",
      "p4 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p4 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p4 HR\n",
      "Reading ... HR.csv\n",
      "p4 BVP\n",
      "Reading ... BVP.csv\n",
      "p4 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p4 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p4 HR\n",
      "Reading ... HR.csv\n",
      "p4 BVP\n",
      "Reading ... BVP.csv\n",
      "p4 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p4 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p4 HR\n",
      "Reading ... HR.csv\n",
      "p4 BVP\n",
      "Reading ... BVP.csv\n",
      "p4 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p4 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p4 HR\n",
      "Reading ... HR.csv\n",
      "p4 BVP\n",
      "Reading ... BVP.csv\n",
      "p4 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p4 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p4 HR\n",
      "Reading ... HR.csv\n",
      "p4 BVP\n",
      "Reading ... BVP.csv\n",
      "p4 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p4 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p4 HR\n",
      "Reading ... HR.csv\n",
      "p4 BVP\n",
      "Reading ... BVP.csv\n",
      "p4 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p4 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p4 HR\n",
      "Reading ... HR.csv\n",
      "p4 BVP\n",
      "Reading ... BVP.csv\n",
      "p4 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p3 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p3 HR\n",
      "Reading ... HR.csv\n",
      "p3 BVP\n",
      "Reading ... BVP.csv\n",
      "p3 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p1 TEMP\n",
      "Reading ... TEMP.csv\n",
      "p1 HR\n",
      "Reading ... HR.csv\n",
      "p1 BVP\n",
      "Reading ... BVP.csv\n",
      "p1 EDA\n",
      "Reading ... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "File name does not comply with analyzed fields tags.csv\n",
      "File name does not comply with analyzed fields IBI.csv\n",
      "p2 TEMP\n",
      "Exemption... TEMP.csv\n",
      "p2 HR\n",
      "Exemption... HR.csv\n",
      "p2 BVP\n",
      "Exemption... BVP.csv\n",
      "p2 EDA\n",
      "Exemption... EDA.csv\n",
      "File name does not comply with analyzed fields ACC.csv\n",
      "reading is done\n",
      "E4 database is limited to the trial period\n"
     ]
    }
   ],
   "source": [
    "START_OF_TRIAL = datetime.strptime(\"11 06 2021-02:00:00\", \"%m %d %Y-%H:%M:%S\")  # to handle the daylight saving issue in apple watches\n",
    "END_OF_TRIAL = datetime.strptime(\"11 15 2021-00:00:00\", \"%m %d %Y-%H:%M:%S\")\n",
    "\n",
    "if os.path.exists(os.path.join(addDataPrefix, \"Results\", \"All_meals.pkl\")):\n",
    "    os.remove(os.path.join(addDataPrefix, \"Results\", \"All_meals.pkl\"))\n",
    "os.chdir(addUserInput)\n",
    "if not os.path.exists(os.path.join(addDataPrefix, \"Results\", \"All_meals.pkl\")):\n",
    "    dfMeal = []\n",
    "    for root, dirs, files in os.walk(addUserInput):\n",
    "        for file in files:\n",
    "            if \".xlsx\" in file.lower():\n",
    "                if \"meals-modified\" in file.lower():\n",
    "                    participantName = file[: file.find(\"Meals\")]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\", file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\", file)\n",
    "                    dfTemp = pd.read_excel(file)\n",
    "                    dfTemp.insert(0, \"Participant\", participantName)\n",
    "                    dfTemp.drop(columns=[\"startTime\", \"FinishTime\", \"Duration\"], inplace=True)\n",
    "                    dfTemp.rename(columns={\"startTimeModified\": \"StartTime\"}, inplace=True)\n",
    "                    dfTemp.rename(columns={\"FinishTimeModified\": \"FinishTime\"}, inplace=True)\n",
    "                    dfTemp[\"StartTime\"] = pd.to_datetime(dfTemp[\"StartTime\"])\n",
    "                    dfTemp[\"FinishTime\"] = pd.to_datetime(dfTemp[\"FinishTime\"])\n",
    "                    dfTemp = dfTemp[dfTemp[\"StartTime\"].notna()]\n",
    "                    # dfTemp['StartTime']-=pd.DateOffset(hours=5)#fixing the time zone issue\n",
    "                    # dfTemp['FinishTime']-=pd.DateOffset(hours=5)#fixing the time zone issue\n",
    "                    dfTemp.sort_values([\"Participant\", \"StartTime\"], ascending=(True, True), inplace=True)\n",
    "                    dfTemp.reset_index(drop=True, inplace=True)\n",
    "                    for counter in range(1, len(dfTemp)):\n",
    "                        if dfTemp[\"StartTime\"].iloc[counter] == dfTemp[\"StartTime\"].iloc[counter - 1]:\n",
    "                            dfTemp[\"Carbs\"].iloc[counter] += dfTemp[\"Carbs\"].iloc[counter - 1]\n",
    "                            dfTemp[\"Fat\"].iloc[counter] += dfTemp[\"Fat\"].iloc[counter - 1]\n",
    "                            dfTemp[\"Protein\"].iloc[counter] += dfTemp[\"Protein\"].iloc[counter - 1]\n",
    "                            dfTemp[\"StartTime\"].iloc[counter] = \"\"\n",
    "                    dfTemp = dfTemp[dfTemp[\"StartTime\"].notna()]\n",
    "                    dfTemp.reset_index(drop=True, inplace=True)\n",
    "                    for counter in range(1, len(dfTemp)):\n",
    "                        if dfTemp[\"StartTime\"].iloc[counter] - dfTemp[\"StartTime\"].iloc[counter - 1] <= timedelta(hours=1):\n",
    "                            print(\"The meals are not compressed for participant:\", participantName, dfTemp[\"StartTime\"].iloc[counter], dfTemp[\"StartTime\"].iloc[counter - 1])\n",
    "                            print(dfTemp)\n",
    "                            os._exit()\n",
    "                    if len(dfMeal) != 0:\n",
    "                        frames = [dfTemp, dfMeal]\n",
    "                        dfMeal = pd.concat(frames)\n",
    "                    else:\n",
    "                        dfMeal = dfTemp\n",
    "    print(\"reading is done\")\n",
    "    dfMeal = dfMeal[dfMeal[\"StartTime\"] >= START_OF_TRIAL]\n",
    "    dfMeal = dfMeal[dfMeal[\"FinishTime\"] < END_OF_TRIAL]\n",
    "    dfMeal.reset_index(drop=True, inplace=True)\n",
    "    print(\"Meal database is limited to the trial period\")\n",
    "    dfMeal.to_pickle(os.path.join(addDataPrefix, \"Results\", \"All_meals.pkl\"))\n",
    "else:\n",
    "    dfMeal = pd.read_pickle(os.path.join(addDataPrefix, \"Results\", \"All_meals.pkl\"))\n",
    "\n",
    "\n",
    "def pdInterpolation(dfTemp):\n",
    "    index = dfTemp[\"Time\"]\n",
    "    seriesParticipant = pd.Series(dfTemp[\"Abbot\"].to_list(), index=index)\n",
    "    seriesParticipant = seriesParticipant.resample(\"1T\").asfreq()\n",
    "    seriesParticipant.interpolate(method=\"polynomial\", order=3, inplace=True)\n",
    "    tempTime = seriesParticipant.index\n",
    "    tempVal = seriesParticipant.values\n",
    "    dfTemp = pd.DataFrame(zip(tempTime, tempVal), columns=[\"Time\", \"Abbot\"])\n",
    "    return dfTemp\n",
    "\n",
    "\n",
    "# if os.path.exists(os.path.join(addDataPrefix,\"Results\",'All_cgm.pkl')):\n",
    "#     os.remove(os.path.join(addDataPrefix,\"Results\",'All_cgm.pkl'))\n",
    "if not os.path.exists(os.path.join(addDataPrefix, \"Results\", \"All_cgm.pkl\")):\n",
    "    os.chdir(addCGM)\n",
    "    dfCGM = []\n",
    "    for root, dirs, files in os.walk(addCGM):\n",
    "        for file in files:\n",
    "            if \".txt\" in file.lower():\n",
    "                if \"_fl\" in file.lower():\n",
    "                    participantName = file[: file.find(\"_fl\")]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\", file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\", file)\n",
    "                    dfTemp = pd.read_csv(file, sep=\"\\t\", skiprows=1)\n",
    "                    if len(dfTemp.columns) != 4:\n",
    "                        print(\"MAYDAY. Error in reading csv\")\n",
    "                        break\n",
    "                    dfTemp.columns.values[0] = \"ID\"\n",
    "                    dfTemp.columns.values[1] = \"Time\"\n",
    "                    dfTemp.columns.values[2] = \"Record\"\n",
    "                    dfTemp.columns.values[3] = \"Abbot\"\n",
    "                    dfTemp.drop(columns=[\"ID\", \"Record\"], inplace=True)\n",
    "                    dfTemp[\"Time\"] = pd.to_datetime(dfTemp[\"Time\"])\n",
    "                    dfTemp[\"Abbot\"] = pd.to_numeric(dfTemp[\"Abbot\"])\n",
    "                    dfTemp.sort_values([\"Time\"], ascending=(True), inplace=True)\n",
    "                    dfTemp = pdInterpolation(dfTemp)\n",
    "                    dfTemp.insert(0, \"Participant\", participantName)\n",
    "                    if len(dfTemp.columns) != 3:\n",
    "                        print(\"MAYDAY. Error in processing csv\")\n",
    "                        break\n",
    "                    if len(dfCGM) != 0:\n",
    "                        frames = [dfTemp, dfCGM]\n",
    "                        dfCGM = pd.concat(frames)\n",
    "                    else:\n",
    "                        dfCGM = dfTemp\n",
    "    print(\"reading is done\")\n",
    "    dfCGM = dfCGM[dfCGM[\"Time\"] >= START_OF_TRIAL]\n",
    "    dfCGM = dfCGM[dfCGM[\"Time\"] < END_OF_TRIAL]\n",
    "    print(\"CGM database is limited to the trial period\")\n",
    "    dfCGM.to_pickle(os.path.join(addDataPrefix, \"Results\", \"All_cgm.pkl\"))\n",
    "else:\n",
    "    dfCGM = pd.read_pickle(os.path.join(addDataPrefix, \"Results\", \"All_cgm.pkl\"))\n",
    "\n",
    "\n",
    "# if os.path.exists(os.path.join(addDataPrefix,\"Results\",'All_cm.pkl')):\n",
    "#     os.remove(os.path.join(addDataPrefix,\"Results\",'All_cm.pkl'))\n",
    "os.chdir(addHKCM)\n",
    "if not os.path.exists(os.path.join(addDataPrefix, \"Results\", \"All_cm.pkl\")):\n",
    "    df = []\n",
    "    for root, dirs, files in os.walk(addHKCM):\n",
    "        for file in files:\n",
    "            if \".csv\" in file.lower():\n",
    "                if \"cm\" in file.lower() and \"modified\" in file.lower():\n",
    "                    participantName = file[: file.find(\"_cm\")]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\", file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\", file)\n",
    "                    dfTemp = pd.read_csv(file)\n",
    "                    print(\"File is read\")\n",
    "                    dfTemp[\"Time\"] = pd.to_datetime(dfTemp[\"Time\"])\n",
    "                    dfTemp.insert(0, \"Participant\", participantName)\n",
    "                    dfTemp.insert(len(dfTemp.columns), \"|Ax|+|Ay|+|Az|\", dfTemp[\"Ax\"].abs() + dfTemp[\"Ay\"].abs() + dfTemp[\"Az\"].abs() + 0.001)  # this is to avoid 0 later on for feature calculation\n",
    "                    dfTemp.insert(len(dfTemp.columns), \"|Yaw|+|Roll|+|Pitch|\", dfTemp[\"Yaw\"].abs() + dfTemp[\"Roll\"].abs() + dfTemp[\"Pitch\"].abs())\n",
    "                    dfTemp.insert(len(dfTemp.columns), \"RotationalToLinear\", dfTemp[\"|Yaw|+|Roll|+|Pitch|\"] / dfTemp[\"|Ax|+|Ay|+|Az|\"])\n",
    "                    print(\"modified\")\n",
    "                    dfTemp.sort_values([\"Time\"], ascending=(True), inplace=True)\n",
    "                    print(\"sorted\")\n",
    "                    if len(dfTemp.columns) != 14:\n",
    "                        print(\"MAYDAY. Error in reading csv\")\n",
    "                        print(dfTemp.columns)\n",
    "                        break\n",
    "                    if len(df) != 0:\n",
    "                        frames = [dfTemp, df]\n",
    "                        df = pd.concat(frames)\n",
    "                    else:\n",
    "                        df = dfTemp\n",
    "    dfCM = df\n",
    "    print(\"Processing is done\")\n",
    "    dfCM = dfCM[dfCM[\"Time\"] >= START_OF_TRIAL]\n",
    "    dfCM = dfCM[dfCM[\"Time\"] < END_OF_TRIAL]\n",
    "    print(\"CM database is limited to the trial period\")\n",
    "    dfCM.to_pickle(os.path.join(addDataPrefix, \"Results\", \"All_cm.pkl\"))\n",
    "else:\n",
    "    dfCM = pd.read_pickle(os.path.join(addDataPrefix, \"Results\", \"All_cm.pkl\"))\n",
    "\n",
    "\n",
    "# if os.path.exists(os.path.join(addDataPrefix, \"Results\",'All_E4.pkl')):\n",
    "#     os.remove(os.path.join(addDataPrefix, \"Results\",'All_E4.pkl'))\n",
    "os.chdir(addE4)\n",
    "# fields=['ACC','BVP','EDA','HR','IBI','TEMP']\n",
    "fields = [\"BVP\", \"EDA\", \"HR\", \"TEMP\"]\n",
    "if not os.path.exists(os.path.join(addDataPrefix, \"Results\", \"All_E4.pkl\")):\n",
    "    dfE4 = []\n",
    "    for root, dirs, files in os.walk(addE4):\n",
    "        for file in files:\n",
    "            if \".csv\" in file.lower():\n",
    "                participantName = root[root.find(\"E4\") + 3 :]\n",
    "                participantName = participantName[:2]\n",
    "                field = file[: file.find(\".csv\")]\n",
    "                if field not in fields:\n",
    "                    print(\"File name does not comply with analyzed fields\", file)\n",
    "                    continue\n",
    "                print(participantName, field)\n",
    "                if participantName in exempts:\n",
    "                    print(\"Exemption...\", file)\n",
    "                    continue\n",
    "                print(\"Reading ...\", file)\n",
    "                os.chdir(root)\n",
    "                dfTemp = pd.read_csv(file, header=None)\n",
    "                # if field=='ACC':\n",
    "                #     assert len(dfTemp.columns)==3\n",
    "                #     timeBase=dfTemp.iloc[0,0]\n",
    "                #     timeStep=1/dfTemp.iloc[1,0]\n",
    "                #     dfTemp.drop([0,1],inplace=True)\n",
    "                #     dfTemp.rename(columns={0:'Data1',1:'Data2',2:'Data3'}, inplace=True)#x,y,z for data1,data2,data3\n",
    "                #     timeTemp=[]\n",
    "                #     for counter in range(len(dfTemp)):\n",
    "                #         timeTemp.append(timeBase+counter*timeStep)\n",
    "                #     dfTemp.insert(0,'Time',timeTemp)\n",
    "                #     dfTemp.insert(0,'Field',\"Acceleration\")\n",
    "                #     dfTemp['Time'] = pd.to_datetime(dfTemp['Time'],unit='s')\n",
    "                if field == \"BVP\":\n",
    "                    assert len(dfTemp.columns) == 1\n",
    "                    timeBase = dfTemp.iloc[0, 0]\n",
    "                    timeStep = 1 / dfTemp.iloc[1, 0]\n",
    "                    dfTemp.drop([0, 1], inplace=True)\n",
    "                    dfTemp.rename(columns={0: \"Data1\"}, inplace=True)\n",
    "                    dfTemp[\"Data2\"] = \"\"\n",
    "                    dfTemp[\"Data3\"] = \"\"\n",
    "                    timeTemp = []\n",
    "                    for counter in range(len(dfTemp)):\n",
    "                        timeTemp.append(timeBase + counter * timeStep)\n",
    "                    dfTemp.insert(0, \"Time\", timeTemp)\n",
    "                    dfTemp.insert(0, \"Field\", \"BVP\")\n",
    "                    dfTemp[\"Time\"] = pd.to_datetime(dfTemp[\"Time\"], unit=\"s\")\n",
    "                elif field == \"HR\":\n",
    "                    assert len(dfTemp.columns) == 1\n",
    "                    timeBase = dfTemp.iloc[0, 0]\n",
    "                    timeStep = 1 / dfTemp.iloc[1, 0]\n",
    "                    dfTemp.drop([0, 1], inplace=True)\n",
    "                    dfTemp.rename(columns={0: \"Data1\"}, inplace=True)\n",
    "                    dfTemp[\"Data2\"] = \"\"\n",
    "                    dfTemp[\"Data3\"] = \"\"\n",
    "                    timeTemp = []\n",
    "                    for counter in range(len(dfTemp)):\n",
    "                        timeTemp.append(timeBase + counter * timeStep)\n",
    "                    dfTemp.insert(0, \"Time\", timeTemp)\n",
    "                    dfTemp.insert(0, \"Field\", \"HR\")\n",
    "                    dfTemp[\"Time\"] = pd.to_datetime(dfTemp[\"Time\"], unit=\"s\")\n",
    "                elif field == \"EDA\":\n",
    "                    assert len(dfTemp.columns) == 1\n",
    "                    timeBase = dfTemp.iloc[0, 0]\n",
    "                    timeStep = 1 / dfTemp.iloc[1, 0]\n",
    "                    dfTemp.drop([0, 1], inplace=True)\n",
    "                    dfTemp.rename(columns={0: \"Data1\"}, inplace=True)\n",
    "                    dfTemp[\"Data2\"] = \"\"\n",
    "                    dfTemp[\"Data3\"] = \"\"\n",
    "                    timeTemp = []\n",
    "                    for counter in range(len(dfTemp)):\n",
    "                        timeTemp.append(timeBase + counter * timeStep)\n",
    "                    dfTemp.insert(0, \"Time\", timeTemp)\n",
    "                    dfTemp.insert(0, \"Field\", \"EDA\")\n",
    "                    dfTemp[\"Time\"] = pd.to_datetime(dfTemp[\"Time\"], unit=\"s\")\n",
    "                # elif field=='IBI':\n",
    "                #     assert len(dfTemp.columns)==2\n",
    "                #     timeBase=dfTemp.iloc[0,0]\n",
    "                #     dfTemp.drop([0],inplace=True)\n",
    "                #     dfTemp.rename(columns={0:'Time',1:'Data1'}, inplace=True)\n",
    "                #     dfTemp[\"Data2\"]=\"\"\n",
    "                #     dfTemp[\"Data3\"]=\"\"\n",
    "                #     timeTemp=[]\n",
    "                #     dfTemp['Time']+=timeBase\n",
    "                #     dfTemp.insert(0,'Field',\"IBI\")\n",
    "                #     dfTemp['Time'] = pd.to_datetime(dfTemp['Time'],unit='s')\n",
    "                elif field == \"TEMP\":\n",
    "                    assert len(dfTemp.columns) == 1\n",
    "                    timeBase = dfTemp.iloc[0, 0]\n",
    "                    timeStep = 1 / dfTemp.iloc[1, 0]\n",
    "                    dfTemp.drop([0, 1], inplace=True)\n",
    "                    dfTemp.rename(columns={0: \"Data1\"}, inplace=True)\n",
    "                    dfTemp[\"Data2\"] = \"\"\n",
    "                    dfTemp[\"Data3\"] = \"\"\n",
    "                    timeTemp = []\n",
    "                    for counter in range(len(dfTemp)):\n",
    "                        timeTemp.append(timeBase + counter * timeStep)\n",
    "                    dfTemp.insert(0, \"Time\", timeTemp)\n",
    "                    dfTemp.insert(0, \"Field\", \"Temperature\")\n",
    "                    dfTemp[\"Time\"] = pd.to_datetime(dfTemp[\"Time\"], unit=\"s\")\n",
    "                dfTemp.insert(0, \"Participant\", participantName)\n",
    "\n",
    "                dfTemp[\"Time\"] -= pd.DateOffset(hours=6)  # Empatica records in GMT and also during the trial we had daylight saving\n",
    "                dfTemp.sort_values([\"Participant\", \"Field\", \"Time\"], ascending=(True, True, True), inplace=True)\n",
    "                if len(dfTemp.columns) != 6:\n",
    "                    print(\"MAYDAY. Error in reading csv\")\n",
    "                    break\n",
    "                if len(dfE4) != 0:\n",
    "                    frames = [dfTemp, dfE4]\n",
    "                    dfE4 = pd.concat(frames)\n",
    "                else:\n",
    "                    dfE4 = dfTemp\n",
    "\n",
    "    print(\"reading is done\")\n",
    "    dfE4 = dfE4[dfE4[\"Time\"] >= START_OF_TRIAL]\n",
    "    dfE4 = dfE4[dfE4[\"Time\"] < END_OF_TRIAL]\n",
    "    print(\"E4 database is limited to the trial period\")\n",
    "    dfE4.to_pickle(os.path.join(addDataPrefix, \"Results\", \"All_E4.pkl\"))\n",
    "else:\n",
    "    dfE4 = pd.read_pickle(os.path.join(addDataPrefix, \"Results\", \"All_E4.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant  p4  is started\n",
      "Positive windows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:16<00:00,  9.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative windows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data for E4 2021-11-07 03:25:00 2021-11-07 03:55:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [04:08<29:00, 124.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data for E4 2021-11-07 03:55:00 2021-11-07 04:25:00\n",
      "Not enough data for E4 2021-11-07 06:47:00 2021-11-07 07:17:00\n",
      "Not enough data for E4 2021-11-07 07:17:00 2021-11-07 07:47:00\n",
      "Not enough data for E4 2021-11-07 07:47:00 2021-11-07 08:17:00\n",
      "Not enough data for E4 2021-11-07 08:17:00 2021-11-07 08:47:00\n",
      "Not enough data for E4 2021-11-07 08:47:00 2021-11-07 09:17:00\n",
      "Not enough data for E4 2021-11-07 09:17:00 2021-11-07 09:47:00\n",
      "Not enough data for E4 2021-11-07 09:47:00 2021-11-07 10:17:00\n",
      "Not enough data for E4 2021-11-07 10:17:00 2021-11-07 10:47:00\n",
      "Not enough data for E4 2021-11-07 10:47:00 2021-11-07 11:17:00\n",
      "Not enough data for E4 2021-11-07 11:17:00 2021-11-07 11:47:00\n",
      "Not enough data for E4 2021-11-07 11:47:00 2021-11-07 12:17:00\n",
      "Not enough data for E4 2021-11-07 12:17:00 2021-11-07 12:47:00\n",
      "Not enough data for E4 2021-11-07 12:47:00 2021-11-07 13:17:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [04:48<19:21, 89.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data for E4 2021-11-07 13:17:00 2021-11-07 13:47:00\n",
      "Not enough data for E4 2021-11-07 17:02:00 2021-11-07 17:32:00\n",
      "Not enough data for E4 2021-11-07 17:32:00 2021-11-07 18:02:00\n",
      "Not enough data for E4 2021-11-07 18:02:00 2021-11-07 18:32:00\n",
      "Not enough data for E4 2021-11-07 18:32:00 2021-11-07 19:02:00\n",
      "Not enough data for E4 2021-11-07 19:02:00 2021-11-07 19:32:00\n",
      "Not enough data for E4 2021-11-07 19:32:00 2021-11-07 20:02:00\n",
      "Not enough data for E4 2021-11-07 20:02:00 2021-11-07 20:32:00\n",
      "Not enough data for E4 2021-11-07 20:32:00 2021-11-07 21:02:00\n",
      "Not enough data for E4 2021-11-07 21:02:00 2021-11-07 21:32:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [05:24<13:52, 69.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data for E4 2021-11-07 21:32:00 2021-11-07 22:02:00\n",
      "Not enough data for E4 2021-11-08 01:19:00 2021-11-08 01:49:00\n",
      "Not enough data for E4 2021-11-08 01:49:00 2021-11-08 02:19:00\n",
      "Not enough data for E4 2021-11-08 02:19:00 2021-11-08 02:49:00\n",
      "Not enough data for E4 2021-11-08 02:49:00 2021-11-08 03:19:00\n"
     ]
    }
   ],
   "source": [
    "OUTTER_WINDOW_LENGTH = timedelta(minutes=30)\n",
    "INNER_WINDOW_LENGTH = timedelta(minutes=1)\n",
    "COMPLEX_MEAL_LENGTH = timedelta(minutes=180)\n",
    "FASTING_LENGTH = timedelta(hours=3)\n",
    "MINIMUM_POINT = INNER_WINDOW_LENGTH.total_seconds()\n",
    "\n",
    "\n",
    "def e4Reporter(df):\n",
    "    topics = [\"BVP\", \"EDA\", \"HR\", \"Temperature\"]\n",
    "    report = []\n",
    "    for topic in topics:\n",
    "        dfTemp = df[df[\"Field\"] == topic]\n",
    "        if topic == \"BVP\":\n",
    "            MIN_POINT = MINIMUM_POINT * 64 * 0.3\n",
    "        elif topic == \"EDA\":\n",
    "            MIN_POINT = MINIMUM_POINT * 4 * 0.3\n",
    "        elif topic == \"HR\":\n",
    "            MIN_POINT = MINIMUM_POINT / 10 * 0.3\n",
    "        elif topic == \"Temperature\":\n",
    "            MIN_POINT = MINIMUM_POINT * 4 * 0.3\n",
    "        else:\n",
    "            print(\"MAYDAY at sensor reader\")\n",
    "            os._exit()\n",
    "        if len(dfTemp) < MIN_POINT:\n",
    "            report.append(\"Nan\")\n",
    "        else:\n",
    "            val = dfTemp[\"Data1\"].mean()\n",
    "            report.append(val)\n",
    "    return report\n",
    "\n",
    "\n",
    "def motionCalculator(df):\n",
    "    f1 = df[\"RotationalToLinear\"].mean()\n",
    "    f2 = df[\"|Ax|+|Ay|+|Az|\"].mean()\n",
    "    f5 = df[\"|Yaw|+|Roll|+|Pitch|\"].mean()\n",
    "    return [f1, f2, f5]\n",
    "\n",
    "\n",
    "def statFeatures(dataList):\n",
    "    dataList = np.asarray(dataList).astype(float)\n",
    "    result = []\n",
    "    dataDim = dataList.ndim\n",
    "    if dataDim > 1:\n",
    "        for counter in range(dataList.shape[1]):\n",
    "            if not np.isnan(dataList[:, counter]).all():\n",
    "                meanVal = np.nanmean(dataList[:, counter], axis=0)\n",
    "                stdVal = np.nanstd(dataList[:, counter], axis=0)\n",
    "                minVal = np.nanmin(dataList[:, counter], axis=0)\n",
    "                maxVal = np.nanmax(dataList[:, counter], axis=0)\n",
    "                rangeVal = maxVal - minVal\n",
    "                skewnessVal = skew(dataList[:, counter], nan_policy=\"omit\", axis=0)\n",
    "                kurtosisVal = kurtosis(dataList[:, counter], nan_policy=\"omit\", axis=0)\n",
    "                result.extend([rangeVal, meanVal, stdVal, minVal, maxVal, skewnessVal, kurtosisVal])\n",
    "            else:\n",
    "                result.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    else:\n",
    "        if not np.isnan(dataList).all():\n",
    "            meanVal = np.nanmean(dataList)\n",
    "            stdVal = np.nanstd(dataList)\n",
    "            minVal = np.nanmin(dataList)\n",
    "            maxVal = np.nanmax(dataList)\n",
    "            rangeVal = maxVal - minVal\n",
    "            skewnessVal = skew(dataList, nan_policy=\"omit\")\n",
    "            kurtosisVal = kurtosis(dataList, nan_policy=\"omit\")\n",
    "            result.extend([rangeVal, meanVal, stdVal, minVal, maxVal, skewnessVal, kurtosisVal])\n",
    "        else:\n",
    "            result.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    return result\n",
    "\n",
    "\n",
    "def innerWindowExtractor(outterWindowStart, innerWindowNumber, dfParticipantCM, dfParticipantE4):\n",
    "    tempListCM = []\n",
    "    tempListE4 = []\n",
    "    for counterInner in range(0, innerWindowNumber, 1):\n",
    "        innerWindowStart = outterWindowStart + counterInner * INNER_WINDOW_LENGTH\n",
    "        innerWindowEnd = innerWindowStart + INNER_WINDOW_LENGTH\n",
    "        dfTempCM = dfParticipantCM[(dfParticipantCM[\"Time\"] >= innerWindowStart) & (dfParticipantCM[\"Time\"] < innerWindowEnd)]\n",
    "\n",
    "        if len(dfTempCM) < MINIMUM_POINT * 10 * 0.3:\n",
    "            tempListCM.append([\"Nan\", \"Nan\", \"Nan\"])\n",
    "        else:\n",
    "            tempListCM.append(motionCalculator(dfTempCM))\n",
    "\n",
    "        dfTempE4 = dfParticipantE4[(dfParticipantE4[\"Time\"] >= innerWindowStart) & (dfParticipantE4[\"Time\"] < innerWindowEnd)]\n",
    "        if len(dfTempE4) < MINIMUM_POINT * 32 * 0.3:\n",
    "            tempListE4.append([\"Nan\", \"Nan\", \"Nan\", \"Nan\"])\n",
    "        else:\n",
    "            tempListE4.append(e4Reporter(dfTempE4))\n",
    "\n",
    "    return tempListCM, tempListE4\n",
    "\n",
    "\n",
    "def outterPosWindowExtractor(dfParticipantMeal, dfParticipantCM, dfParticipantE4, dfParticipantCGM, participant):\n",
    "    print(\"Positive windows:\")\n",
    "    participantDataList = []\n",
    "    for counterOuter in tqdm(range(0, len(dfParticipantMeal))):\n",
    "        outterWindowStart = dfParticipantMeal[\"StartTime\"].iloc[counterOuter]\n",
    "        outterWindowEnd = outterWindowStart + OUTTER_WINDOW_LENGTH\n",
    "        innerWindowNumber = int(OUTTER_WINDOW_LENGTH.total_seconds() / INNER_WINDOW_LENGTH.total_seconds())\n",
    "        tempListInfo = [outterWindowStart, outterWindowEnd, participant]\n",
    "        tempList = []\n",
    "        tempListCM, tempListE4 = innerWindowExtractor(outterWindowStart, innerWindowNumber, dfParticipantCM, dfParticipantE4)\n",
    "\n",
    "        carbs = dfParticipantMeal[\"Carbs\"].iloc[counterOuter]\n",
    "        protein = dfParticipantMeal[\"Protein\"].iloc[counterOuter]\n",
    "        fat = dfParticipantMeal[\"Fat\"].iloc[counterOuter]\n",
    "\n",
    "        tempListCM = statFeatures(tempListCM)\n",
    "        tempList.extend(tempListCM)  # 7*3=21\n",
    "\n",
    "        tempListE4 = statFeatures(tempListE4)\n",
    "        tempList.extend(tempListE4)  # 7*4=28\n",
    "\n",
    "        dfTempCGM = dfParticipantCGM[(dfParticipantCGM[\"Time\"] >= outterWindowStart) & (dfParticipantCGM[\"Time\"] < outterWindowEnd)]\n",
    "        tempListCGM = dfTempCGM[\"Abbot\"].to_list()\n",
    "        tempListCGM = statFeatures(tempListCGM)\n",
    "        tempList.extend(tempListCGM)  # 7\n",
    "        tempList.extend(tempListInfo)  # 3\n",
    "        tempList.append(carbs)\n",
    "        tempList.append(fat)\n",
    "        tempList.append(protein)\n",
    "        tempList.append(1)  # mealFlag\n",
    "        assert len(tempList) == 7 * 3 + 7 * 4 + 7 + 3 + 4\n",
    "        participantDataList.append(tempList)\n",
    "    return participantDataList\n",
    "\n",
    "\n",
    "def outterNegWindowExtractor(dfParticipantMeal, dfParticipantCM, dfParticipantE4, dfParticipantCGM, participant):\n",
    "    print(\"Negative windows:\")\n",
    "    participantDataList = []\n",
    "    gaps = []\n",
    "\n",
    "    for counterOuter in range(1, len(dfParticipantMeal)):\n",
    "        if dfParticipantMeal[\"StartTime\"].iloc[counterOuter] - dfParticipantMeal[\"StartTime\"].iloc[counterOuter - 1] >= FASTING_LENGTH:\n",
    "            gaps.append([dfParticipantMeal[\"StartTime\"].iloc[counterOuter - 1], dfParticipantMeal[\"StartTime\"].iloc[counterOuter]])\n",
    "\n",
    "    for counterOuter in tqdm(range(len(gaps))):\n",
    "        querryTemp = gaps[counterOuter]\n",
    "        querryStart = querryTemp[0] + FASTING_LENGTH\n",
    "        querryEnd = querryTemp[1]\n",
    "        querryWindNum = datetime.timestamp(querryEnd) - datetime.timestamp(querryStart)\n",
    "        querryWindNum = int(querryWindNum / OUTTER_WINDOW_LENGTH.total_seconds())\n",
    "\n",
    "        for counter in range(querryWindNum):\n",
    "            outterWindowStart = querryStart + counter * OUTTER_WINDOW_LENGTH\n",
    "            outterWindowEnd = outterWindowStart + OUTTER_WINDOW_LENGTH\n",
    "\n",
    "            dfTempE4 = dfParticipantE4[dfParticipantE4[\"Time\"] >= outterWindowStart]\n",
    "            dfTempE4 = dfTempE4[dfTempE4[\"Time\"] < outterWindowEnd]\n",
    "            if len(dfTempE4) < OUTTER_WINDOW_LENGTH.total_seconds() * (64 + 4 + 4) * 0.3:\n",
    "                print(\"Not enough data for E4\", outterWindowStart, outterWindowEnd)\n",
    "                continue\n",
    "\n",
    "            dfTempCM = dfParticipantCM[dfParticipantCM[\"Time\"] >= outterWindowStart]\n",
    "            dfTempCM = dfTempCM[dfTempCM[\"Time\"] < outterWindowEnd]\n",
    "            if len(dfTempCM) < OUTTER_WINDOW_LENGTH.total_seconds() * 10 * 0.3:\n",
    "                print(\"Not enough data for Apple\", outterWindowStart, outterWindowEnd)\n",
    "                continue\n",
    "\n",
    "            innerWindowNumber = int(OUTTER_WINDOW_LENGTH.total_seconds() / INNER_WINDOW_LENGTH.total_seconds())\n",
    "            tempListInfo = [outterWindowStart, outterWindowEnd, participant]\n",
    "            tempList = []\n",
    "            tempListCM, tempListE4 = innerWindowExtractor(outterWindowStart, innerWindowNumber, dfParticipantCM, dfParticipantE4)\n",
    "\n",
    "            carbs = 0\n",
    "            protein = 0\n",
    "            fat = 0\n",
    "\n",
    "            tempListCM = statFeatures(tempListCM)\n",
    "            tempList.extend(tempListCM)  # 7*3=21\n",
    "\n",
    "            tempListE4 = statFeatures(tempListE4)\n",
    "            tempList.extend(tempListE4)  # 7*4=28\n",
    "\n",
    "            dfTempCGM = dfParticipantCGM[(dfParticipantCGM[\"Time\"] >= outterWindowStart) & (dfParticipantCGM[\"Time\"] < outterWindowEnd)]\n",
    "            tempListCGM = dfTempCGM[\"Abbot\"].to_list()\n",
    "            tempListCGM = statFeatures(tempListCGM)\n",
    "            tempList.extend(tempListCGM)  # 7\n",
    "            tempList.extend(tempListInfo)  # 3\n",
    "\n",
    "            tempList.append(carbs)\n",
    "            tempList.append(fat)\n",
    "            tempList.append(protein)\n",
    "            tempList.append(0)  # mealFlag\n",
    "            assert len(tempList) == 7 * 3 + 7 * 4 + 7 + 3 + 4\n",
    "            participantDataList.append(tempList)\n",
    "    return participantDataList\n",
    "\n",
    "\n",
    "def participantFeatureWriter(participantDataList, participant):\n",
    "    participantDataArray = np.asarray(participantDataList)\n",
    "    columnTopics = [\"F1\", \"F2\", \"F5\", \"BVP\", \"EDA\", \"HR\", \"Temperature\", \"CGM\"]\n",
    "    columnStats = [\"Range\", \"Mean\", \"Std\", \"Min\", \"Max\", \"Skewness\", \"Kurtosis\"]\n",
    "    columns = []\n",
    "    for topic in columnTopics:\n",
    "        for stat in columnStats:\n",
    "            columns.append(topic + \"-\" + stat)\n",
    "    columns.extend([\"StartTime\", \"EndTime\", \"Participant\", \"Carb\", \"Fat\", \"Protein\", \"MealLabel\"])\n",
    "    dfFeatures = pd.DataFrame(participantDataArray, columns=columns)\n",
    "    dfFeatures.sort_values([\"Participant\", \"StartTime\"], ascending=(True, True), inplace=True)\n",
    "    dfFeatures.reset_index(drop=True, inplace=True)\n",
    "    dfFeatures.insert(len(dfFeatures.columns), \"ComplexFlag\", 0)\n",
    "    for counter in range(1, len(dfFeatures)):\n",
    "        if dfFeatures[\"StartTime\"].iloc[counter] <= dfFeatures[\"StartTime\"].iloc[counter - 1] + COMPLEX_MEAL_LENGTH:\n",
    "            dfFeatures[\"ComplexFlag\"].iloc[counter - 1] = 1\n",
    "    dfFeatures.to_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + participant + \"-Features.xlsx\")), index=False)\n",
    "\n",
    "\n",
    "def allFeatureWriter(participants):\n",
    "    dfFeatures = []\n",
    "    for participant in participants:\n",
    "        if participant in exempts:\n",
    "            continue\n",
    "        if not os.path.exists(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + participant+\"-Features.xlsx\"))):\n",
    "            print(\"MAYDAY, no feature for participant:\", participant)\n",
    "            continue\n",
    "        dfTemp = pd.read_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + participant+\"-Features.xlsx\")))\n",
    "        if len(dfFeatures) == 0:\n",
    "            dfFeatures = dfTemp\n",
    "        else:\n",
    "            frames = [dfTemp, dfFeatures]\n",
    "            dfFeatures = pd.concat(frames)\n",
    "    dfFeatures.sort_values([\"Participant\", \"StartTime\"], ascending=(True, True), inplace=True)\n",
    "    dfFeatures.reset_index(drop=True, inplace=True)\n",
    "    dfFeatures.to_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-All-Features.xlsx\")), index=False)\n",
    "    return dfFeatures\n",
    "\n",
    "\n",
    "participants = dfCM[\"Participant\"].to_list()\n",
    "participants = list(set(participants))\n",
    "# if os.path.exists(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-All-Features.xlsx\"))):\n",
    "#     os.remove(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-All-Features.xlsx\")))\n",
    "if not os.path.exists(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-All-Features.xlsx\"))):\n",
    "    for participant in participants:\n",
    "        if participant in exempts:\n",
    "            continue\n",
    "        # if os.path.exists(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + participant+\"-Features.xlsx\"))):\n",
    "        #     os.remove(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + participant+\"-Features.xlsx\")))\n",
    "        if not os.path.exists(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + participant+\"-Features.xlsx\"))):\n",
    "            print(\"Participant \", participant, \" is started\")\n",
    "            dfParticipantCM = dfCM[dfCM[\"Participant\"] == participant]\n",
    "            dfParticipantMeal = dfMeal[dfMeal[\"Participant\"] == participant]\n",
    "            dfParticipantCGM = dfCGM[dfCGM[\"Participant\"] == participant]\n",
    "            dfParticipantE4 = dfE4[dfE4[\"Participant\"] == participant]\n",
    "\n",
    "            participantDataList = outterPosWindowExtractor(dfParticipantMeal, dfParticipantCM, dfParticipantE4, dfParticipantCGM, participant)\n",
    "            participantDataList.extend(outterNegWindowExtractor(dfParticipantMeal, dfParticipantCM, dfParticipantE4, dfParticipantCGM, participant))\n",
    "            participantFeatureWriter(participantDataList, participant)\n",
    "    dfFeatures = allFeatureWriter(participants)\n",
    "else:\n",
    "    dfFeatures = pd.read_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-All-Features.xlsx\")))\n",
    "print(dfFeatures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmseCalculator(trueGround, prediction):\n",
    "    rmse = 0\n",
    "    assert len(trueGround) == len(prediction)\n",
    "    for counter in range(len(trueGround)):\n",
    "        tempVal = trueGround[counter] - prediction[counter]\n",
    "        tempVal *= tempVal\n",
    "        rmse += tempVal\n",
    "    rmse /= len(trueGround)\n",
    "    rmse = np.sqrt(rmse)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def rmsleCalculator(trueGround, prediction):\n",
    "    rmsle = 0\n",
    "    assert len(trueGround) == len(prediction)\n",
    "    for counter in range(len(trueGround)):\n",
    "        tempVal = np.log(1 + prediction[counter]) - np.log(1 + trueGround[counter])\n",
    "        tempVal *= tempVal\n",
    "        rmsle += tempVal\n",
    "    rmsle /= len(trueGround)\n",
    "    rmsle = np.sqrt(rmsle)\n",
    "    return rmsle\n",
    "\n",
    "\n",
    "def xgRegressor(xTrain, xVal, xTest, yTrain, yVal, yTest, headerNames):\n",
    "    rmseBest = 100000000000\n",
    "    for maxDepth in np.arange(2, 8, 1):\n",
    "        for estimator in np.arange(50, 200, 50):\n",
    "            clf = xgb.XGBRegressor(n_jobs=24, n_estimators=estimator, max_depth=maxDepth, objective=\"reg:squarederror\")\n",
    "            # clf = xgb.XGBRegressor(n_jobs=24, n_estimators=estimator, max_depth=maxDepth, objective=\"reg:squaredlogerror\")\n",
    "            clf.fit(xTrain, yTrain)\n",
    "            predictionsVal = clf.predict(xVal)\n",
    "            rmse = rmseCalculator(yVal, predictionsVal)\n",
    "            rmsle = rmsleCalculator(yVal, predictionsVal)\n",
    "            if rmse < rmseBest:\n",
    "                rmseBest = rmse\n",
    "                modelBest = clf\n",
    "\n",
    "    predictionsTest = modelBest.predict(xTest)\n",
    "    rmse = rmseCalculator(yTest, predictionsTest)\n",
    "    rmsle = rmsleCalculator(yTest, predictionsTest)\n",
    "    featureImportance = modelBest.feature_importances_\n",
    "    return [rmse, rmsle, headerNames, featureImportance, yTest, predictionsTest]\n",
    "\n",
    "\n",
    "def xgClassifier(xTrain, xVal, xTest, yTrain, yVal, yTest):\n",
    "    f1ScoreBest = -1\n",
    "    for maxDepth in np.arange(2, 8, 1):\n",
    "        for estimator in np.arange(50, 200, 50):\n",
    "            for posWeight in np.arange(1, 10, 0.5):\n",
    "                clf = xgb.XGBClassifier(scale_pos_weight=posWeight, n_jobs=24, n_estimators=estimator, max_depth=maxDepth, objective=\"binary:logistic\", eval_metric=\"error\")\n",
    "                clf.fit(xTrain, yTrain)\n",
    "                predictionsVal = clf.predict(xVal)\n",
    "\n",
    "                accuracy = sklearn.metrics.accuracy_score(yVal, predictionsVal)\n",
    "                recall = sklearn.metrics.recall_score(yVal, predictionsVal)\n",
    "                precision = sklearn.metrics.precision_score(yVal, predictionsVal)\n",
    "                f1Score = sklearn.metrics.f1_score(yVal, predictionsVal)\n",
    "\n",
    "                if f1Score > f1ScoreBest:\n",
    "                    modelBest = clf\n",
    "                    f1ScoreBest = f1Score\n",
    "    predictionsTest = modelBest.predict(xTest)\n",
    "    confMatrix = sklearn.metrics.confusion_matrix(yTest, predictionsTest)\n",
    "    accuracy = sklearn.metrics.accuracy_score(yTest, predictionsTest)\n",
    "    recall = sklearn.metrics.recall_score(yTest, predictionsTest)\n",
    "    precision = sklearn.metrics.precision_score(yTest, predictionsTest)\n",
    "    f1Score = sklearn.metrics.f1_score(yTest, predictionsTest)\n",
    "    return [accuracy, recall, precision, f1Score]\n",
    "\n",
    "\n",
    "def dataBalancer(xTrain, xVal, yTrain, yVal):\n",
    "    oversample = SMOTE()\n",
    "    xVal, yVal = oversample.fit_resample(xVal, yVal)\n",
    "    xTrain, yTrain = oversample.fit_resample(xTrain, yTrain)\n",
    "\n",
    "    return xTrain, xVal, yTrain, yVal\n",
    "\n",
    "\n",
    "def testTrainSplitFunc(dfCombination, randomSeed, normalFlag, combination):\n",
    "    participants = list(set(dfCombination[\"Participant\"].to_list()))\n",
    "    dataColumnIndex = dfCombination.columns.get_loc(\"StartTime\")\n",
    "    regressorReports = []\n",
    "    classifierReports = []\n",
    "    for participantCounter in tqdm(range(len(participants) + 1)):\n",
    "\n",
    "        if participantCounter == len(participants):  # General Model (one model for all participants)\n",
    "            dfParticipant = dfCombination\n",
    "            participant = \"All\"\n",
    "        else:  # Personal Model (each participant have a his/her own model)\n",
    "            participant = participants[participantCounter]\n",
    "            dfParticipant = dfCombination[dfCombination[\"Participant\"] == participant]\n",
    "\n",
    "        print(\"Participant:\", participant)\n",
    "        headerNames = dfParticipant[dfParticipant.columns[0:dataColumnIndex]]\n",
    "        headerNames = headerNames.columns.to_list()\n",
    "\n",
    "        dataXBinary = dfParticipant[dfParticipant.columns[0:dataColumnIndex]].to_numpy()\n",
    "        dataYBinary = dfParticipant[\"MealLabel\"].to_numpy()\n",
    "        dataTimeBinary = dfParticipant[[\"StartTime\", \"EndTime\"]].to_numpy()\n",
    "        dataXRegression = dfParticipant[dfParticipant.columns[0:dataColumnIndex]].to_numpy()\n",
    "        dataYRegression = dfParticipant[[\"Carb\", \"Fat\", \"Protein\"]].to_numpy()\n",
    "        dataTimeRegression = dfParticipant[[\"StartTime\", \"EndTime\"]].to_numpy()\n",
    "\n",
    "        dataXBinary = dataXBinary.astype(float)\n",
    "        dataYBinary = dataYBinary.astype(float)\n",
    "        dataXRegression = dataXRegression.astype(float)\n",
    "        dataYRegression = dataYRegression.astype(float)\n",
    "\n",
    "        if normalFlag:\n",
    "            dataXBinary -= -dataXBinary.mean(axis=0)\n",
    "            dataXBinary /= dataXBinary.std(axis=0)\n",
    "            dataXRegression -= -dataXRegression.mean(axis=0)\n",
    "            dataXRegression /= dataXRegression.std(axis=0)\n",
    "\n",
    "        splitterOuter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=randomSeed)\n",
    "        setNumber = 0\n",
    "        for trainIndex1, testIndex in splitterOuter.split(dataXBinary, dataYBinary):\n",
    "            xTrainBinary, xTestBinary = (dataXBinary[trainIndex1, :], dataXBinary[testIndex, :])\n",
    "            yTrainBinary, yTestBinary = dataYBinary[trainIndex1], dataYBinary[testIndex]\n",
    "            timeTrainBinary, timeTestBinary = dataTimeBinary[trainIndex1, :], dataTimeBinary[testIndex, :]\n",
    "\n",
    "            splitterInner = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=randomSeed)\n",
    "            for trainIndex2, valIndex in splitterInner.split(xTrainBinary, yTrainBinary):\n",
    "                xTrainBinary, xValBinary = xTrainBinary[trainIndex2, :], xTrainBinary[valIndex, :]\n",
    "                yTrainBinary, yValBinary = yTrainBinary[trainIndex2], yTrainBinary[valIndex]\n",
    "                timeTrainBinary, timeValBinary = timeTrainBinary[trainIndex2, :], timeTrainBinary[valIndex, :]\n",
    "\n",
    "            tempListReport = xgClassifier(xTrainBinary, xValBinary, xTestBinary, yTrainBinary, yValBinary, yTestBinary)\n",
    "            tempListReport.extend([participant, combination, timeTestBinary[:, 0], timeTestBinary[:, 1], setNumber, \"Classification\"])\n",
    "            classifierReports.append(tempListReport)\n",
    "            setNumber += 1\n",
    "\n",
    "        # print(\"All winodws:\", len(dataXRegression))\n",
    "        positiveWindowIndex = []\n",
    "        for counter in range(len(dataYRegression)):\n",
    "            if np.sum(dataYRegression[counter, :]) != 0:  # this is a positive window as fat+protein+carb!=0\n",
    "                positiveWindowIndex.append(counter)\n",
    "        dataXRegression = dataXRegression[positiveWindowIndex, :]\n",
    "        dataYRegression = dataYRegression[positiveWindowIndex, :]\n",
    "        # print(\"Positive winodws:\", len(dataXRegression))\n",
    "\n",
    "        setNumber = 0\n",
    "        components = [\"Carb\", \"Fat\", \"Protein\"]\n",
    "        splitterOuter = ShuffleSplit(n_splits=1, test_size=0.3, random_state=randomSeed)\n",
    "        for trainIndex1, testIndex in splitterOuter.split(dataXRegression, dataYRegression):\n",
    "            xTrainRegression, xTestRegression = dataXRegression[trainIndex1, :], dataXRegression[testIndex, :]\n",
    "            yTrainRegression, yTestRegression = dataYRegression[trainIndex1, :], dataYRegression[testIndex, :]\n",
    "            timeTrainRegression, timeTestRegression = dataTimeRegression[trainIndex1, :], dataTimeRegression[testIndex, :]\n",
    "\n",
    "            splitterInner = ShuffleSplit(n_splits=1, test_size=0.3, random_state=randomSeed)\n",
    "            for trainIndex2, valIndex in splitterInner.split(xTrainRegression, yTrainRegression):\n",
    "                xTrainRegression, xValRegression = xTrainRegression[trainIndex2, :], xTrainRegression[valIndex, :]\n",
    "                yTrainRegression, yValRegression = yTrainRegression[trainIndex2, :], yTrainRegression[valIndex, :]\n",
    "                timeTrainRegression, timeValRegression = timeTrainRegression[trainIndex2, :], timeTrainRegression[valIndex, :]\n",
    "\n",
    "            for counter in range(3):\n",
    "                tempListReport = xgRegressor(xTrainRegression, xValRegression, xTestRegression, yTrainRegression[:, counter], yValRegression[:, counter], yTestRegression[:, counter], headerNames)\n",
    "                tempListReport.extend([participant, combination, timeTestRegression[:, 0], timeTestRegression[:, 1], setNumber, components[counter]])\n",
    "                regressorReports.append(tempListReport)\n",
    "            setNumber += 1\n",
    "    return classifierReports, regressorReports\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Classifier.xlsx\"))):\n",
    "    os.remove(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Classifier.xlsx\")))\n",
    "    os.remove(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Regressor.xlsx\")))\n",
    "if not os.path.exists(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Classifier.xlsx\"))):\n",
    "    combinations = [\n",
    "        [\"CGM\"],\n",
    "        [\"CGM\", \"F1\", \"F2\", \"F5\"],\n",
    "        [\"CGM\", \"BVP\", \"EDA\", \"HR\", \"Temperature\"],\n",
    "        [\"CGM\", \"BVP\", \"EDA\", \"HR\", \"Temperature\", \"F1\", \"F2\", \"F5\"],\n",
    "    ]\n",
    "    columns = dfFeatures.columns\n",
    "    headersClassifier = [\n",
    "        \"Accuracy\",\n",
    "        \"Recall\",\n",
    "        \"Precision\",\n",
    "        \"F1\",\n",
    "        \"Participant\",\n",
    "        \"Combination\",\n",
    "        \"StartTime\",\n",
    "        \"EndTime\",\n",
    "        \"SetNumber\",\n",
    "        \"Component\",\n",
    "    ]\n",
    "    headersRegressor = [\n",
    "        \"RMSquaredError\",\n",
    "        \"RMSquaredLogError\",\n",
    "        \"Features\",\n",
    "        \"FeatureImportance\",\n",
    "        \"TrueMacro\",\n",
    "        \"PredictedMacro\",\n",
    "        \"Participant\",\n",
    "        \"Combination\",\n",
    "        \"StartTime\",\n",
    "        \"EndTime\",\n",
    "        \"SetNumber\",\n",
    "        \"Component\",\n",
    "    ]\n",
    "    dfClassifier = []\n",
    "    dfRegressor = []\n",
    "    for combination in combinations:\n",
    "        columnList = [\"StartTime\", \"EndTime\", \"Participant\", \"Carb\", \"Fat\", \"Protein\", \"MealLabel\"]\n",
    "        for topic in combination:\n",
    "            for column in columns:\n",
    "                if topic in column:\n",
    "                    columnList.append(column)\n",
    "\n",
    "        dfCombination = dfFeatures[dfFeatures.columns.intersection(columnList)]\n",
    "\n",
    "        randomSeed = random.randrange(50)\n",
    "        print(\"----------------------\")\n",
    "        print(\"Combination:\", \"+\".join(combination))\n",
    "        NORMALIZED_FLAG = False\n",
    "        classifierReports, regressorReports = testTrainSplitFunc(dfCombination, randomSeed, NORMALIZED_FLAG, \"+\".join(combination))\n",
    "        dfTempClassifier = pd.DataFrame(classifierReports, columns=headersClassifier)\n",
    "        dfTempRegressor = pd.DataFrame(regressorReports, columns=headersRegressor)\n",
    "        if len(dfClassifier) > 0:\n",
    "            frames = [dfTempClassifier, dfClassifier]\n",
    "            dfClassifier = pd.concat(frames)\n",
    "\n",
    "            frames = [dfTempRegressor, dfRegressor]\n",
    "            dfRegressor = pd.concat(frames)\n",
    "        else:\n",
    "            dfClassifier = dfTempClassifier\n",
    "            dfRegressor = dfTempRegressor\n",
    "    dfClassifier.reset_index(drop=True, inplace=True)\n",
    "    dfRegressor.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    dfClassifier.to_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Classifier.xlsx\")), index=False)\n",
    "    dfRegressor.to_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Regressor.xlsx\")), index=False)\n",
    "else:\n",
    "    dfClassifier = pd.read_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Classifier.xlsx\")))\n",
    "    dfRegressor = pd.read_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Regressor.xlsx\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringFixer(df):\n",
    "    dfColumns = [\"TrueMacro\", \"PredictedMacro\"]\n",
    "    for dfColumn in dfColumns:\n",
    "        for counter in range(len(df)):\n",
    "            tempVal = df[dfColumn].iloc[counter]\n",
    "            tempVal = tempVal.replace(\"[\", \"\")\n",
    "            tempVal = tempVal.replace(\"]\", \"\")\n",
    "            tempVal = list(tempVal.split(\" \"))\n",
    "            tempVal = list(filter(None, tempVal))\n",
    "            tempVal = np.asarray(tempVal).astype(float)\n",
    "            df[dfColumn].iloc[counter] = tempVal\n",
    "    return df\n",
    "\n",
    "\n",
    "dfRegressor = pd.read_excel(os.path.join(addDataPrefix, \"Results\", (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Regressor.xlsx\")))\n",
    "dfRegressor = stringFixer(dfRegressor)\n",
    "\n",
    "dfRegressor.reset_index(drop=True, inplace=True)\n",
    "dfRegressor.replace(\"CGM\", \"C\", inplace=True)\n",
    "dfRegressor.replace(\"CGM+F1+F2+F5\", \"CM\", inplace=True)\n",
    "dfRegressor.replace(\"CGM+BVP+EDA+HR+Temperature\", \"CE\", inplace=True)\n",
    "dfRegressor.replace(\"CGM+BVP+EDA+HR+Temperature+F1+F2+F5\", \"CEM\", inplace=True)\n",
    "components = [\"Carb\", \"Fat\", \"Protein\"]\n",
    "colors = [\"red\", \"green\", \"blue\", \"magenta\"]\n",
    "participants = list(set(dfRegressor[\"Participant\"].to_list()))\n",
    "combinations = list(set(dfRegressor[\"Combination\"].to_list()))\n",
    "\n",
    "dfPearson = []\n",
    "for participant in participants:\n",
    "    dfParticipant = dfRegressor[dfRegressor[\"Participant\"] == participant]\n",
    "    for component in components:\n",
    "        dfComponent = dfParticipant[dfParticipant[\"Component\"] == component]\n",
    "        myFig = plt.figure(figsize=(10, 10))\n",
    "        for counter in range(len(combinations)):\n",
    "            dfCombination = dfComponent[dfComponent[\"Combination\"] == combinations[counter]]\n",
    "            trueVal = dfCombination[\"TrueMacro\"].to_list()\n",
    "            trueVal = np.asarray(trueVal).astype(float)\n",
    "            trueVal = trueVal.flatten()\n",
    "\n",
    "            predVal = dfCombination[\"PredictedMacro\"].to_list()\n",
    "            predVal = np.asarray(predVal).astype(float)\n",
    "            predVal = predVal.flatten()\n",
    "\n",
    "            figName = \"Participant:\" + participant + \", Macro:\" + component\n",
    "            plt.scatter(x=trueVal, y=predVal, c=colors[counter], label=combinations[counter], alpha=0.5, s=20)\n",
    "            plt.title(figName)\n",
    "            plt.plot([0, np.max(trueVal)], [0, np.max(trueVal)], \"k--\")\n",
    "            tempVal = pearsonr(trueVal, predVal)[0]\n",
    "            dfPearson.append([participant, component, combinations[counter], tempVal])\n",
    "            plt.xlabel(\"True\")\n",
    "            plt.ylabel(\"Pred\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.savefig(os.path.join(addDataPrefix, \"Results\", figName + \".png\"), dpi=600)\n",
    "        plt.show()\n",
    "dfPearson = pd.DataFrame(dfPearson, columns=[\"Participant\", \"Component\", \"Combination\", \"Pearson\"])\n",
    "dfPearson.insert(len(dfPearson.columns), \"RelativePearson\", -100)\n",
    "dfPearson[\"RelativePearson\"] = dfPearson[\"Pearson\"]\n",
    "for participant in participants:  # Finding the difference of C with other Combinations\n",
    "    for component in components:\n",
    "        dfTemp = dfPearson[dfPearson[\"Participant\"] == participant]\n",
    "        dfTemp = dfTemp[dfTemp[\"Component\"] == component]\n",
    "        baseLine = dfTemp[dfTemp[\"Combination\"] == \"C\"]\n",
    "        baseLine = baseLine[\"RelativePearson\"].sum()\n",
    "        for counter in range(len(dfPearson)):\n",
    "            if dfPearson[\"Participant\"].iloc[counter] == participant and dfPearson[\"Component\"].iloc[counter] == component:\n",
    "                dfPearson[\"RelativePearson\"].iloc[counter] -= baseLine\n",
    "dfPearson.to_excel(os.path.join(addDataPrefix, \"Results\", \"Pearson.xlsx\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfTrain=pd.read_csv('/home/grads/s/sorush.omidvar/STMI-CGM-Predictor/train.csv')\n",
    "# dfTest=pd.read_csv('/home/grads/s/sorush.omidvar/STMI-CGM-Predictor/test.csv')\n",
    "# dfVal=pd.read_csv('/home/grads/s/sorush.omidvar/STMI-CGM-Predictor/val.csv')\n",
    "\n",
    "# trainData=dfTrain.to_numpy()\n",
    "# testData=dfTest.to_numpy()\n",
    "# valData=dfVal.to_numpy()\n",
    "\n",
    "# xTrain=trainData[:,0:-4]\n",
    "# xTest=testData[:,0:-4]\n",
    "# xVal=valData[:,0:-4]\n",
    "# print(xTrain.shape,xVal.shape,xVal.shape)\n",
    "\n",
    "# for counter in range(3):\n",
    "#     plt.figure()\n",
    "#     yTrain=trainData[:,-4+counter]\n",
    "#     yTest=testData[:,-4+counter]\n",
    "#     yVal=valData[:,-4+counter]\n",
    "\n",
    "#     # clf = xgb.XGBRegressor(n_jobs=24, objective=\"reg:squarederror\")\n",
    "#     # clf.fit(xTrain, yTrain)\n",
    "#     # predictionsVal = clf.predict(xTest)\n",
    "#     # plt.scatter(yTest,predictionsVal)\n",
    "#     # plt.plot([0,100],[0,100],'--k')\n",
    "#     rmseBest=1000000\n",
    "#     for maxDepth in np.arange(2, 8, 1):\n",
    "#         for estimator in np.arange(50, 200, 50):\n",
    "#             clf = xgb.XGBRegressor(n_jobs=24,n_estimators=estimator, max_depth=maxDepth, objective=\"reg:squarederror\")\n",
    "#             clf.fit(xTrain, yTrain)\n",
    "#             predictionsVal = clf.predict(xVal)\n",
    "#             rmse = rmseCalculator(yVal, predictionsVal)\n",
    "#             rmsle = rmsleCalculator(yVal, predictionsVal)\n",
    "#             if rmse < rmseBest:\n",
    "#                 rmseBest = rmse\n",
    "#                 modelBest = clf\n",
    "#                 bestDepth=maxDepth\n",
    "#                 bestEstimator=estimator\n",
    "#     predictionsTest = clf.predict(xTest)\n",
    "#     plt.scatter(yTest,predictionsTest)\n",
    "#     plt.plot([0,100],[0,100],'--k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stringFixer(df):\n",
    "#     dfColumns = [\"TrueMacro\", \"PredictedMacro\"]\n",
    "#     for dfColumn in dfColumns:\n",
    "#         for counter in range(len(df)):\n",
    "#             tempVal = df[dfColumn].iloc[counter]\n",
    "#             tempVal = tempVal.replace(\"[\", \"\")\n",
    "#             tempVal = tempVal.replace(\"]\", \"\")\n",
    "#             tempVal = list(tempVal.split(\" \"))\n",
    "#             tempVal = list(filter(None, tempVal))\n",
    "#             tempVal = np.asarray(tempVal).astype(float)\n",
    "#             df[dfColumn].iloc[counter] = tempVal\n",
    "#     return df\n",
    "\n",
    "\n",
    "# dfRegressor = pd.read_csv(os.path.join(addDataPrefix, (str(OUTTER_WINDOW_LENGTH) + \"-\" + str(FASTING_LENGTH) + \"-Final-Regressor.csv\")))\n",
    "# dfRegressor = stringFixer(dfRegressor)\n",
    "\n",
    "# dfRegressor.reset_index(drop=True, inplace=True)\n",
    "# dfRegressor.replace(\"CGM\", \"C\", inplace=True)\n",
    "# dfRegressor.replace(\"CGM+F1+F2+F5\", \"CM\", inplace=True)\n",
    "# dfRegressor.replace(\"CGM+BVP+EDA+HR+Temperature\", \"CE\", inplace=True)\n",
    "# dfRegressor.replace(\"CGM+BVP+EDA+HR+Temperature+F1+F2+F5\", \"CEM\", inplace=True)\n",
    "# components = [\"Carb\", \"Fat\", \"Protein\"]\n",
    "# colors = [\"red\", \"green\", \"blue\", \"magenta\"]\n",
    "# participants = list(set(dfRegressor[\"Participant\"].to_list()))\n",
    "# combinations = list(set(dfRegressor[\"Combination\"].to_list()))\n",
    "\n",
    "# dfPearson = []\n",
    "# for participant in participants:\n",
    "#     dfParticipant = dfRegressor[dfRegressor[\"Participant\"] == participant]\n",
    "#     for component in components:\n",
    "#         dfComponent = dfParticipant[dfParticipant[\"Component\"] == component]\n",
    "#         for counter in range(len(combinations)):\n",
    "#             # myFig=plt.figure()\n",
    "#             dfCombination = dfComponent[dfComponent[\"Combination\"] == combinations[counter]]\n",
    "#             trueVal = dfCombination[\"TrueMacro\"].to_list()\n",
    "#             trueVal = np.asarray(trueVal).astype(float)\n",
    "#             trueVal = trueVal.flatten()\n",
    "\n",
    "#             predVal = dfCombination[\"PredictedMacro\"].to_list()\n",
    "#             predVal = np.asarray(predVal).astype(float)\n",
    "#             predVal = predVal.flatten()\n",
    "\n",
    "#             plt.scatter(x=trueVal, y=predVal, c=colors[counter])\n",
    "#             plt.title(participant + component + combinations[counter])\n",
    "#             plt.plot([0, np.max(trueVal)], [0, np.max(trueVal)], \"k--\")\n",
    "#             tempVal = pearsonr(trueVal, predVal)[0]\n",
    "#             dfPearson.append([participant, component, combinations[counter], tempVal])\n",
    "#             plt.xlabel(\"True\")\n",
    "#             plt.ylabel(\"Pred\")\n",
    "#             plt.show()\n",
    "#             plt.savefig(pl)\n",
    "# dfPearson = pd.DataFrame(dfPearson, columns=[\"Participant\", \"Component\", \"Combination\", \"Pearson\"])\n",
    "# dfPearson.insert(len(dfPearson.columns),\"RelativePearson\",-100)\n",
    "# dfPearson[\"RelativePearson\"]=dfPearson[\"Pearson\"]\n",
    "# for participant in participants:  # Finding the difference of C with other Combinations\n",
    "#     for component in components:\n",
    "#         dfTemp = dfPearson[dfPearson[\"Participant\"] == participant]\n",
    "#         dfTemp = dfTemp[dfTemp[\"Component\"] == component]\n",
    "#         baseLine = dfTemp[dfTemp[\"Combination\"] == \"C\"]\n",
    "#         baseLine = baseLine[\"RelativePearson\"].sum()\n",
    "#         for counter in range(len(dfPearson)):\n",
    "#             if dfPearson[\"Participant\"].iloc[counter] == participant and dfPearson[\"Component\"].iloc[counter] == component:\n",
    "#                 dfPearson[\"RelativePearson\"].iloc[counter] -= baseLine\n",
    "# dfPearson.to_csv(os.path.join(addDataPrefix, \"Pearson.csv\"), index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65e249be015da37cfe4cfdd1da028fe4cb475c26be679b9e3e34696278a17c64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

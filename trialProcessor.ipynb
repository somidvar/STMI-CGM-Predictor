{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pytz\n",
    "from datetime import datetime,timedelta,timezone\n",
    "from dateutil.tz import tzutc\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import pearsonr, mode\n",
    "from scipy.signal import savgol_filter\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "addDataPrefix='/Users/sorush/My Drive/Documents/Educational/TAMU/Research/Trial/Data/11-5-21-11-15-21'\n",
    "if not os.path.exists(addDataPrefix):\n",
    "    addDataPrefix='/home/grads/s/sorush.omidvar/CGMDataset/Trial/Data/11-5-21-11-15-21'\n",
    "addUserInput=os.path.join(addDataPrefix,'User inputted')\n",
    "addHKCM=os.path.join(addDataPrefix,'hk+cm')\n",
    "addCGM=os.path.join(addDataPrefix,'CGM')\n",
    "addE4=os.path.join(addDataPrefix,'E4')\n",
    "\n",
    "exempts=['p2']\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plt.style.use({'figure.facecolor':'white'})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemption... p2Meals.csv\n",
      "Reading ... p4Meals.csv\n",
      "Reading ... p3Meals.csv\n",
      "Reading ... p1Meals.csv\n",
      "reading is done\n",
      "Reading ... p1Processed.csv\n",
      "Reading ... p4Processed.csv\n",
      "Reading ... p3Processed.csv\n",
      "Exemption... p2Processed.csv\n",
      "reading is done\n",
      "Reading ... p4_cm_all_modified.csv\n",
      "File is read\n",
      "modified\n",
      "sorted\n",
      "Reading ... p1_cm_all_modified.csv\n",
      "File is read\n",
      "modified\n",
      "sorted\n",
      "Exemption... p2_cm_all_modified.csv\n",
      "Reading ... p3_cm_all_modified.csv\n",
      "File is read\n",
      "modified\n",
      "sorted\n",
      "Processing is done\n"
     ]
    }
   ],
   "source": [
    "# if os.path.exists(os.path.join(addDataPrefix,'All_meals.pkl')):\n",
    "#     os.remove(os.path.join(addDataPrefix,'All_meals.pkl'))\n",
    "os.chdir(addUserInput)\n",
    "if not os.path.exists(os.path.join(addDataPrefix,'All_meals.pkl')):    \n",
    "    df=[]\n",
    "    for root, dirs, files in os.walk(addUserInput):\n",
    "        for file in files:\n",
    "            if '.csv' in file.lower():\n",
    "                if 'meals' in file.lower():\n",
    "                    participantName=file[:file.find('Meals')]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\",file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\",file)\n",
    "                    dfTemp = pd.read_csv(file)\n",
    "                    dfTemp.insert(0,'Participant',participantName)\n",
    "                    dfTemp.rename(columns={'startTime':'StartTime'}, inplace=True)\n",
    "                    dfTemp['StartTime']=pd.to_datetime(dfTemp['StartTime'])\n",
    "                    dfTemp['FinishTime']=pd.to_datetime(dfTemp['FinishTime'])\n",
    "\n",
    "                    dfTemp['StartTime']-=pd.DateOffset(hours=5)#fixing the time zone issue\n",
    "                    dfTemp['FinishTime']-=pd.DateOffset(hours=5)#fixing the time zone issue            \n",
    "                    dfTemp.sort_values([\"Participant\",'StartTime'],ascending = (True, True),inplace=True)\n",
    "                    if len(dfTemp.columns)!=10:\n",
    "                        print(\"MAYDAY. Error in reading csv\")\n",
    "                        break\n",
    "                    if len(df)!=0:\n",
    "                        frames=[dfTemp,df]\n",
    "                        df=pd.concat(frames)\n",
    "                    else:\n",
    "                        df=dfTemp\n",
    "    print(\"reading is done\")\n",
    "    dfMeal=df    \n",
    "    dfMeal.to_pickle(os.path.join(addDataPrefix,'All_meals.pkl')) \n",
    "else:\n",
    "    dfMeal=pd.read_pickle(os.path.join(addDataPrefix,'All_meals.pkl'))\n",
    "\n",
    "\n",
    "# if os.path.exists(os.path.join(addDataPrefix,'All_cgm.pkl')):\n",
    "#     os.remove(os.path.join(addDataPrefix,'All_cgm.pkl'))\n",
    "if not os.path.exists(os.path.join(addDataPrefix,'All_cgm.pkl')):\n",
    "    os.chdir(addCGM)\n",
    "    df=[]\n",
    "    for root, dirs, files in os.walk(addCGM):\n",
    "        for file in files:\n",
    "            if '.csv' in file.lower():\n",
    "                if 'processed' in file.lower():\n",
    "                    participantName=file[:file.find('Processed')]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\",file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\",file)\n",
    "                    dfTemp = pd.read_csv(file)\n",
    "                    dfTemp.insert(0,'Participant',participantName)\n",
    "                    dfTemp['Time']=pd.to_datetime(dfTemp['Time'])\n",
    "                    dfTemp.sort_values([\"Participant\",\"Time\"],ascending = (True,True),inplace=True)\n",
    "                    if len(dfTemp.columns)!=4:\n",
    "                        print(\"MAYDAY. Error in reading csv\")\n",
    "                        break\n",
    "                    if len(df)!=0:\n",
    "                        frames=[dfTemp,df]\n",
    "                        df=pd.concat(frames)\n",
    "                    else:\n",
    "                        df=dfTemp\n",
    "    print(\"reading is done\")\n",
    "    dfCGM=df\n",
    "    dfCGM['Abbot'].interpolate(method='linear',limit_direction='both',axis=0,inplace=True)\n",
    "    dfCGM['Dexcom'].interpolate(method='linear',limit_direction='both',axis=0,inplace=True)\n",
    "    dfCGM.to_pickle(os.path.join(addDataPrefix,'All_cgm.pkl')) \n",
    "else:\n",
    "    dfCGM=pd.read_pickle(os.path.join(addDataPrefix,'All_cgm.pkl'))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# if os.path.exists(os.path.join(addDataPrefix,'All_cm.pkl')):\n",
    "#     os.remove(os.path.join(addDataPrefix,'All_cm.pkl'))\n",
    "os.chdir(addHKCM)\n",
    "if not os.path.exists(os.path.join(addDataPrefix,'All_cm.pkl')):\n",
    "    df=[]\n",
    "    for root, dirs, files in os.walk(addHKCM):\n",
    "        for file in files:\n",
    "            if '.csv' in file.lower():\n",
    "                if 'cm' in file.lower() and 'modified' in file.lower():\n",
    "                    participantName=file[:file.find('_cm')]\n",
    "                    if participantName in exempts:\n",
    "                        print(\"Exemption...\",file)\n",
    "                        continue\n",
    "                    print(\"Reading ...\",file)\n",
    "                    dfTemp=pd.read_csv(file)\n",
    "                    print(\"File is read\")\n",
    "                    dfTemp.insert(0,'Participant',participantName)\n",
    "                    dfTemp.rename(columns={'Date':'Time'}, inplace=True)\n",
    "                    dfTemp['Time']=pd.to_datetime(dfTemp['Time'])\n",
    "                    dfTemp.insert(len(dfTemp.columns),'|Ax|+|Ay|+|Az|',dfTemp['Ax'].abs()+dfTemp['Ay'].abs()+dfTemp['Az'].abs()+0.001)#this is to avoid 0 later on for feature calculation\n",
    "                    dfTemp.insert(len(dfTemp.columns),'|Yaw|+|Roll|+|Pitch|',dfTemp['Yaw'].abs()+dfTemp['Roll'].abs()+dfTemp['Pitch'].abs())\n",
    "                    dfTemp.insert(len(dfTemp.columns),'RotationalToLinear',dfTemp['|Yaw|+|Roll|+|Pitch|']/dfTemp['|Ax|+|Ay|+|Az|'])\n",
    "                    print(\"modified\")\n",
    "                    dfTemp.sort_values(['Time'],ascending = (True),inplace=True)\n",
    "                    print(\"sorted\")\n",
    "                    if len(dfTemp.columns)!=14:\n",
    "                        print(\"MAYDAY. Error in reading csv\")\n",
    "                        print(dfTemp.columns)\n",
    "                        break\n",
    "                    if len(df)!=0:\n",
    "                        frames=[dfTemp,df]\n",
    "                        df=pd.concat(frames)\n",
    "                    else:\n",
    "                        df=dfTemp\n",
    "    dfCM=df\n",
    "    print(\"Processing is done\")\n",
    "    dfCM.to_pickle(os.path.join(addDataPrefix,'All_cm.pkl')) \n",
    "else:\n",
    "    dfCM = pd.read_pickle(os.path.join(addDataPrefix,'All_cm.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtractorMotion(df):\n",
    "    f1=df['RotationalToLinear'].mean()\n",
    "    f2=df['|Ax|+|Ay|+|Az|'].mean()\n",
    "    f5=df['|Yaw|+|Roll|+|Pitch|'].mean()\n",
    "    return [f1,f2,f5]\n",
    "\n",
    "def featureExtractorCGM(df):\n",
    "    mean=df['Abbot'].mean()\n",
    "    std=df['Abbot'].std()\n",
    "    max=df['Abbot'].max()\n",
    "    min=df['Abbot'].min()\n",
    "    return mean, std, min,max\n",
    "\n",
    "MINIMUM_POINT_CM=10\n",
    "WINDOW_LENGTH=timedelta(minutes=2)\n",
    "# WINDOW_STEP=timedelta(minutes=2)\n",
    "MEAL_PORTION_RELAXATION=timedelta(seconds=10)\n",
    "START_OF_TRIAL = datetime.strptime('11 06 2021-02:00:00', '%m %d %Y-%H:%M:%S')#to handle the daylight saving issue in apple watches\n",
    "END_OF_TRIAL = datetime.strptime('11 07 2021-00:00:00', '%m %d %Y-%H:%M:%S')\n",
    "\n",
    "dfCM=dfCM[dfCM['Time']>=START_OF_TRIAL]\n",
    "dfCM=dfCM[dfCM['Time']<END_OF_TRIAL]\n",
    "\n",
    "dfMeal=dfMeal[dfMeal['Time']>=START_OF_TRIAL]\n",
    "dfMeal=dfMeal[dfMeal['Time']<END_OF_TRIAL]\n",
    "\n",
    "participants=dfCM['Participant'].to_list()\n",
    "participants=list(set(participants))\n",
    "\n",
    "windowStart=START_OF_TRIAL\n",
    "windowEnd=windowStart+WINDOW_LENGTH\n",
    "for participant in participants:\n",
    "    participantDataList=[]\n",
    "    print(\"Participant \",participant,\" is started\")\n",
    "    for counter in tqdm(range(int((END_OF_TRIAL-START_OF_TRIAL).seconds/WINDOW_LENGTH.seconds))):\n",
    "        dfTempCM=dfCM[(dfCM['Time']>=windowStart) & (dfCM['Time']<=windowEnd) & (dfCM['Participant']==participant)]\n",
    "        dfTempMeal=dfMeal[(dfMeal['StartTime']-MEAL_PORTION_RELAXATION<=windowStart) & (dfMeal['FinishTime']+MEAL_PORTION_RELAXATION>=windowEnd)& (dfMeal['Participant']==participant)]\n",
    "        if(len(dfTempCM)<MINIMUM_POINT_CM):\n",
    "            windowStart+=WINDOW_LENGTH\n",
    "            windowEnd+=WINDOW_LENGTH\n",
    "            continue\n",
    "        tempList=featureExtractorMotion(dfTempCM)\n",
    "        if(len(dfTempMeal)!=0):\n",
    "            tempList.append(1)\n",
    "        else:\n",
    "            tempList.append(0)\n",
    "        participantDataList.append(tempList)\n",
    "        windowStart+=WINDOW_LENGTH\n",
    "        windowEnd+=WINDOW_LENGTH\n",
    "participantDataArray=np.asarray(participantDataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15228426395939088\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:27<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Val:\n",
      "[[109  11]\n",
      " [ 52  68]]\n",
      "Accuracy: 74.0 Recall: 57.0 Precision: 86.0\n",
      "***********Test:\n",
      "[[109  11]\n",
      " [ 52  68]]\n",
      "Accuracy: 89.0 Recall: 73.0 Precision: 61.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def STMI_XGBoost(xTrain,xVal,xTest,yTrain,yVal,yTest):\n",
    "    accuracyBest=0\n",
    "    for maxDepth in tqdm(np.arange(3,30)):\n",
    "        for estimator in np.arange(5,50,2):\n",
    "            for threshold in np.arange(0.3,0.7,0.1):\n",
    "                clf = xgb.XGBClassifier(n_jobs=16,n_estimators=estimator,max_depth=maxDepth,objective = \"binary:logistic\",eval_metric = \"logloss\",use_label_encoder =False)\n",
    "                clf.fit(xTrain,yTrain)\n",
    "                predictionsVal = clf.predict_proba(xVal)\n",
    "                predictionsVal=predictionsVal[:,1]\n",
    "                predictionsVal[predictionsVal>=threshold]=1\n",
    "                predictionsVal[predictionsVal<threshold]=0\n",
    "\n",
    "                confMatrix=sklearn.metrics.confusion_matrix(yVal,predictionsVal)\n",
    "                accuracy=sklearn.metrics.accuracy_score(yVal,predictionsVal)\n",
    "                recall=sklearn.metrics.recall_score(yVal,predictionsVal)\n",
    "                precision=sklearn.metrics.precision_score(yVal,predictionsVal)\n",
    "\n",
    "                if accuracy>accuracyBest:\n",
    "                    confMatrixBest=confMatrix\n",
    "                    accuracyBest=accuracy\n",
    "                    modelBest=clf\n",
    "                    recallBest=recall\n",
    "                    precisionBest=precision\n",
    "                    thresholdBest=threshold\n",
    "    print(\"***********Val:\")\n",
    "    print(confMatrixBest)\n",
    "    print(\"Accuracy:\",np.round(100*accuracyBest,0),\"Recall:\",np.round(100*recallBest,0),\"Precision:\",np.round(100*precisionBest,0))\n",
    "    print(\"***********Test:\")\n",
    "    predictionsTest=modelBest.predict_proba(xTest)\n",
    "    predictionsTest=predictionsTest[:,1]\n",
    "    predictionsTest[predictionsTest>=thresholdBest]=1\n",
    "    predictionsTest[predictionsTest<thresholdBest]=0    \n",
    "    \n",
    "    confMatrix=sklearn.metrics.confusion_matrix(yTest,predictionsTest)\n",
    "    accuracy=sklearn.metrics.accuracy_score(yTest,predictionsTest)\n",
    "    recall=sklearn.metrics.recall_score(yTest,predictionsTest)\n",
    "    precision=sklearn.metrics.precision_score(yTest,predictionsTest)\n",
    "    \n",
    "    print(confMatrixBest)\n",
    "    print(\"Accuracy:\",np.round(100*accuracy,0),\"Recall:\",np.round(100*recall,0),\"Precision:\",np.round(100*precision,0))\n",
    "\n",
    "def dataBalancer(xTrain,xVal,yTrain,yVal):\n",
    "    oversample = SMOTE()\n",
    "    xVal, yVal = oversample.fit_resample(xVal, yVal)\n",
    "    xTrain, yTrain = oversample.fit_resample(xTrain, yTrain)\n",
    "\n",
    "    return xTrain,xVal,yTrain,yVal\n",
    "\n",
    "def testTrainSplitFunc(data,randomSeed):\n",
    "    random.seed(randomSeed)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    dataX=data[:,0:3]\n",
    "    dataY=data[:,3]\n",
    "    dataXNorm=dataX-dataX.mean(axis=0)\n",
    "    dataXNorm/=dataXNorm.std(axis=0)\n",
    "\n",
    "    stratidiedSampling = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=randomSeed)\n",
    "    for trainIndex, testIndex in stratidiedSampling.split(dataX, dataY):\n",
    "        xTrain,xTest=dataX[trainIndex],dataX[testIndex]\n",
    "        yTrain,yTest=dataY[trainIndex],dataY[testIndex]\n",
    "\n",
    "    stratidiedSampling = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=randomSeed)\n",
    "    for trainIndex, valIndex in stratidiedSampling.split(xTrain, yTrain):\n",
    "        xTrain,xVal=dataX[trainIndex],dataX[valIndex]\n",
    "        yTrain,yVal=dataY[trainIndex],dataY[valIndex]\n",
    "\n",
    "    return xTrain,xVal,xTest,yTrain,yVal,yTest\n",
    "\n",
    "randomSeed=random.randrange(100)\n",
    "xTrain,xVal,xTest,yTrain,yVal,yTest=testTrainSplitFunc(participantDataArray,randomSeed=randomSeed)\n",
    "xTrain,xVal,yTrain,yVal=dataBalancer(xTrain,xVal,yTrain,yVal)\n",
    "print(yTest.mean())\n",
    "print(yTrain.mean())\n",
    "STMI_XGBoost(xTrain,xVal,xTest,yTrain,yVal,yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfInterp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r9/63tl846j3ksdwv99wh0gv21m0000gn/T/ipykernel_13142/1812573125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0mrandomSeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainLabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestTrainSplitFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfInterp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdfMacro\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandomSeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandomSeed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontextFlag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmreCGMCon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictionCGMCon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTMI_XGBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainLabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0mpearsonCGMCon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionCGMCon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfInterp' is not defined"
     ]
    }
   ],
   "source": [
    "def STMI_XGBoost(trainData,testData,trainLabels,testLabels):\n",
    "    mseBest=math.inf\n",
    "    depthBest=float('nan')\n",
    "    estimatorBest=float('nan')\n",
    "    modelBest=''\n",
    "\n",
    "    for numEstimator in np.arange(10,120,10):\n",
    "        for maxDepth in np.arange(3,10):\n",
    "            clf = xgb.XGBRegressor(n_estimators =numEstimator,max_depth=maxDepth,objective ='reg:squarederror',eval_metric = \"logloss\")\n",
    "            clf.fit(trainData, trainLabels)\n",
    "            testPrediction = clf.predict(testData)\n",
    "            mre=[]\n",
    "            for counter in range(0,len(testPrediction)):\n",
    "                mre.append(math.fabs(testPrediction[counter]-testLabels[counter])/testLabels[counter])\n",
    "            mre=np.asarray(mre)\n",
    "            mre=np.mean(mre)\n",
    "            if mre<mseBest:\n",
    "                mreBest=mre\n",
    "                depthBest=maxDepth\n",
    "                estimatorBest=numEstimator\n",
    "                modelBest=clf\n",
    "                predictionBest=testPrediction\n",
    "    print(clf.feature_importances_)\n",
    "    return modelBest,mreBest,predictionBest,depthBest,estimatorBest\n",
    "\n",
    "def aucCalculator(timeTemp,dataTemp,kernelNumber):\n",
    "    kernelMeans=np.linspace(0,24,kernelNumber+2)\n",
    "    kernelMeans=np.delete(kernelMeans,0)\n",
    "    kernelMeans=np.delete(kernelMeans,-1)\n",
    "    kernelMeans=kernelMeans*3600\n",
    "\n",
    "    kernelSTD=2*3600\n",
    "    aucData=[]\n",
    "    for kernelMean in kernelMeans:\n",
    "        kernelTemp=np.exp(-((timeTemp-kernelMean)**2)/(2*kernelSTD**2))\n",
    "        tempAuc=kernelTemp*dataTemp\n",
    "        tempAuc=np.trapz(tempAuc,x=timeTemp)\n",
    "        aucData.append(tempAuc/3600)\n",
    "\n",
    "    return aucData\n",
    "\n",
    "def statisticalFeature(timeTemp,dataTemp):\n",
    "    featureData=[]\n",
    "    featureList=[]\n",
    "\n",
    "    featureData.append(np.mean(dataTemp))\n",
    "    featureData.append(np.max(dataTemp)-np.min(dataTemp))\n",
    "    featureData.append(np.var(dataTemp))\n",
    "\n",
    "    featureData.append(np.mean(np.diff(dataTemp,n=1)))\n",
    "    # featureData.append(np.median(np.diff(dataTemp,n=1)))\n",
    "    featureData.append(np.max(np.diff(dataTemp,n=1)))\n",
    "    featureData.append(np.min(np.diff(dataTemp,n=1)))\n",
    "    # featureData.append(np.var(np.diff(dataTemp,n=1)))\n",
    "\n",
    "    featureData.append(np.mean(np.diff(dataTemp,n=2)))\n",
    "    # featureData.append(np.median(np.diff(dataTemp,n=2)))\n",
    "    # featureData.append(np.max(np.diff(dataTemp,n=2)))\n",
    "    # featureData.append(np.min(np.diff(dataTemp,n=2)))\n",
    "    # featureData.append(np.var(np.diff(dataTemp,n=2)))\n",
    "\n",
    "    featureData.append(np.trapz(dataTemp)/3600)\n",
    "    return featureData\n",
    "\n",
    "def featureExtractor(df,contextFlag):\n",
    "    featureList=[]\n",
    "    sensors=['CGM','EDA', 'Acc','Temp', 'HR','Cal']\n",
    "\n",
    "\n",
    "    #-----------------I removed the STEP as it was almost always zero. We should look into it later------------\n",
    "    featureLabels=[]\n",
    "    for sensor in sensors:\n",
    "\n",
    "        varData=df[df['Parameter']==sensor]\n",
    "        varTime=varData['Time'].tolist()\n",
    "        varData=varData['Value'].tolist()\n",
    "\n",
    "        varTime=np.asarray(varTime)\n",
    "        varData=np.asarray(varData)\n",
    "        if contextFlag:\n",
    "            featureList.extend(statisticalFeature(varTime,varData))\n",
    "            featureLabels.append([sensor,'mean'])\n",
    "            featureLabels.append([sensor,'max'])\n",
    "            featureLabels.append([sensor,'var'])\n",
    "            featureLabels.append([sensor,'S-mean'])\n",
    "            featureLabels.append([sensor,'S-max'])\n",
    "            featureLabels.append([sensor,'S-min'])\n",
    "            featureLabels.append([sensor,'SS-mean'])\n",
    "            featureLabels.append([sensor,'trapz'])\n",
    "        if sensor=='CGM':\n",
    "            featureList.extend(aucCalculator(varTime,varData,9))\n",
    "            for i in range(0,9):\n",
    "                featureLabels.append([sensor,'auc'+str(i)])\n",
    "            featureList.extend(aucCalculator(varTime,varData,6))\n",
    "            for i in range(0,6):\n",
    "                featureLabels.append([sensor,'auc'+str(i)])\n",
    "    print(\"Used features\",featureLabels)\n",
    "    #REDO IT WITH FITBIT AND E4\n",
    "    #FOR MEAL TO MEAL PERSPECTIVE, MAKE THE WHOLE DAY INTO 3 CHUNCKS (MEAL TO MEAL)\n",
    "    #PLOT DAY-DAY VS MEAL TO MEAL AND SHOW THEM HOW WE ARE SEAPRATING THINGS (CGM,EDA,...)\n",
    "\n",
    "    #https://github.com/fraunhoferportugal/tsfel/tree/9319db4368303cf10adb3aeb72cd4235a8085307\n",
    "    return featureList\n",
    "\n",
    "def testTrainSplitFuncAux(dfSensorTemp,dfMacroTemp,contextFlag):\n",
    "    sensorDataList=[]\n",
    "    macroDataList=[]\n",
    "    days=list(set(dfSensorTemp['Date']))\n",
    "\n",
    "    for day in days:\n",
    "        dfSensorTempDay=dfSensorTemp[dfSensorTemp['Date']==day]\n",
    "        sensorDataList.append(featureExtractor(dfSensorTempDay,contextFlag))\n",
    "        dfMacroTempDay=dfMacroTemp[dfMacroTemp['Date']==day]\n",
    "        totalCarb=dfMacroTempDay['Carb'].sum()\n",
    "        totalFat=dfMacroTempDay['Fat'].sum()\n",
    "        totalProtein=dfMacroTempDay['Protein'].sum()\n",
    "        macroDataList.append([totalCarb,totalFat,totalProtein])\n",
    "\n",
    "    return sensorDataList,macroDataList\n",
    "\n",
    "def featureNormalizer(trainData,testData):\n",
    "    # repeatingNum=trainData.shape\n",
    "    # repeatingNum=repeatingNum[0]\n",
    "    #\n",
    "    # columnMeans=np.mean(trainData,axis=0)\n",
    "    # columnMeans = np.repeat(columnMeans[:, np.newaxis], repeatingNum, axis=1)\n",
    "    # columnMeans=np.transpose(columnMeans)\n",
    "    # trainData-=columnMeans\n",
    "    #\n",
    "    # columnSTD=np.std(trainData,axis=0)\n",
    "    # columnSTD = np.repeat(columnSTD[:, np.newaxis], repeatingNum, axis=1)\n",
    "    # columnSTD=np.transpose(columnSTD)\n",
    "    # trainData/=columnSTD\n",
    "    #\n",
    "    # repeatingNum=testData.shape\n",
    "    # repeatingNum=repeatingNum[0]\n",
    "    #\n",
    "    # columnMeans=np.mean(trainData,axis=0)\n",
    "    # columnMeans = np.repeat(columnMeans[:, np.newaxis], repeatingNum, axis=1)\n",
    "    # columnMeans=np.transpose(columnMeans)\n",
    "    # testData-=columnMeans\n",
    "    #\n",
    "    # columnSTD=np.std(trainData,axis=0)\n",
    "    # columnSTD = np.repeat(columnSTD[:, np.newaxis], repeatingNum, axis=1)\n",
    "    # columnSTD=np.transpose(columnSTD)\n",
    "    # testData/=columnSTD\n",
    "    scaler = StandardScaler()\n",
    "    trainData = scaler.fit_transform(trainData)\n",
    "    testData = scaler.fit_transform(testData)\n",
    "\n",
    "    return trainData,testData\n",
    "\n",
    "def testTrainSplitFunc(dfSensor, dfMacro,randomSeed,contextFlag):\n",
    "    days=list(set(dfSensor['Date']))\n",
    "    random.seed(randomSeed)\n",
    "    random.shuffle(days)\n",
    "    daysTrain=days[0:6]\n",
    "    daysTest=days[6:9]\n",
    "\n",
    "    dfSensorTrain = dfSensor[dfSensor['Date'].isin(daysTrain)]\n",
    "    dfSensorTest = dfSensor[dfSensor['Date'].isin(daysTest)]\n",
    "\n",
    "    dfMacroTrain= dfMacro[dfMacro['Date'].isin(daysTrain)]\n",
    "    dfMacroTest= dfMacro[dfMacro['Date'].isin(daysTest)]\n",
    "\n",
    "    trainData,trainLabel=testTrainSplitFuncAux(dfSensorTrain,dfMacroTrain,contextFlag)\n",
    "    testData,testLabel=testTrainSplitFuncAux(dfSensorTest,dfMacroTest,contextFlag)\n",
    "\n",
    "    trainData=np.asarray(trainData)\n",
    "    testData=np.asarray(testData)\n",
    "\n",
    "    trainLabel=np.asarray(trainLabel)\n",
    "    testLabel=np.asarray(testLabel)\n",
    "\n",
    "    trainLabel=trainLabel[:,0]\n",
    "    testLabel=testLabel[:,0]\n",
    "    trainData,testData=featureNormalizer(trainData,testData)\n",
    "    # for i in range(0,testData.shape[1]):\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(testData[:,i])\n",
    "    #     plt.title(str(i))\n",
    "    return trainData,testData,trainLabel,testLabel\n",
    "\n",
    "randomSeed=random.randrange(100)\n",
    "trainData,testData,trainLabel,testLabel=testTrainSplitFunc(dfInterp,dfMacro,randomSeed=randomSeed,contextFlag=True)\n",
    "_,mreCGMCon,predictionCGMCon,_,_=STMI_XGBoost(trainData,testData,trainLabel,testLabel)\n",
    "pearsonCGMCon=pearsonr(predictionCGMCon,testLabel)\n",
    "pearsonCGMCon=pearsonCGMCon[0]\n",
    "\n",
    "trainData,testData,trainLabel,testLabel=testTrainSplitFunc(dfInterp,dfMacro,randomSeed=randomSeed,contextFlag=False)\n",
    "_,mreCGM,predictionCGM,_,_=STMI_XGBoost(trainData,testData,trainLabel,testLabel)\n",
    "pearsonCGM=pearsonr(predictionCGM,testLabel)\n",
    "pearsonCGM=pearsonCGM[0]\n",
    "\n",
    "plt.scatter(x=testLabel,y=predictionCGM,color='red',label='CGM')\n",
    "plt.scatter(x=testLabel,y=predictionCGMCon,color='blue',label='CGM+Context')\n",
    "plt.plot([200,600],[200,600],':k')\n",
    "# plt.scatter(x=daysTest,y=predictionBestCGMContext,label='Prediction CGM+Con')\n",
    "# plt.scatter(x=daysTest,y=predictionBestCGM,label='Prediction CGM')\n",
    "plt.annotate('CGM Error'+str(round(100*mreCGM,2))+'%', xy=(1.05, 0.95), xycoords='axes fraction')\n",
    "plt.annotate('CGMCon Error'+str(round(100*mreCGMCon,2))+'%', xy=(1.05, 0.85), xycoords='axes fraction')\n",
    "\n",
    "plt.annotate('CGM Pearson'+str(round(100*pearsonCGM,2))+'%', xy=(1.05, 0.75), xycoords='axes fraction')\n",
    "plt.annotate('CGMCon Pearson'+str(round(100*pearsonCGMCon,2))+'%', xy=(1.05, 0.65), xycoords='axes fraction')\n",
    "\n",
    "plt.xlabel('Actual Carb [gr]')\n",
    "plt.ylabel('Predicted Carb [gr]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('C:\\GitHub\\Carb.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65e249be015da37cfe4cfdd1da028fe4cb475c26be679b9e3e34696278a17c64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import pearsonr, mode\n",
    "from scipy.signal import savgol_filter\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import copy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "addressPrefix='C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/'\n",
    "if not os.path.exists(addressPrefix):\n",
    "    addressPrefix='C:/GDrive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/'\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plt.style.use({'figure.facecolor':'white'})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2001\\\\data\\\\csv\\\\P105_10_30_Session1_P105_10_30_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2011\\\\data\\\\fcsv\\\\P011_09_15_Session2_P011_9_15_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2012\\\\data\\\\csv\\\\P012_10_06_Session1_P012_10_06_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2013\\\\data\\\\csv\\\\P013_10_14_Session1_P013_10_14_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2014\\\\data\\\\csv\\\\P014_09_15_Session1_P014_09_15_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2015\\\\data\\\\csv\\\\P015_10_22_Session1_P015_10_22_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2016\\\\data\\\\csv\\\\P016_10_27_Session1_P016_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2017\\\\data\\\\csv\\\\P017_10_07_Session1_P017_10_07_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2018\\\\data\\\\csv\\\\P018_10_07_Session1_P018_10_07_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2020\\\\data\\\\csv\\\\P020_10_15_Session1_P020_10_15_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2030\\\\data\\\\csv\\\\P030_10_20_Session1_P030_10_20_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2031\\\\data\\\\csv\\\\P031_10_15_Session1_P031_10_15_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2033\\\\data\\\\csv\\\\P033_10_22_Session1_P033_10_22_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2035\\\\data\\\\fcsv\\\\P035_09_16_Session1_P035_09_16_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2036\\\\data\\\\csv\\\\P044_09_15_Session1_P036_09_15_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2038\\\\data\\\\csv\\\\P038_10_15_Session1_P038_10_15_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2044\\\\data\\\\csv\\\\P044_09_15_Session1_P044_09_15_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2046\\\\data\\\\csv\\\\P046_09_17_Session1_P046_09_17_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2051\\\\data\\\\csv\\\\P051_10_06_Session1_P051_10_06_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2055\\\\data\\\\csv\\\\P055_10_27_Session1_P055_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2057\\\\data\\\\csv\\\\P057_10_27_Session1_P057_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2058\\\\data\\\\csv\\\\P058_10_27_Session1_P058_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2063\\\\data\\\\csv\\\\P063_10_27_Session1_P063_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2100\\\\data\\\\csv\\\\P100_10_27_Session1_P100_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2101\\\\data\\\\csv\\\\P101_10_27_Session1_P101_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2102\\\\data\\\\csv\\\\P102_10_27_Session1_P102_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2103\\\\data\\\\csv\\\\P103_10_27_Session1_P103_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2105\\\\data\\\\csv\\\\105_10_27_Session1_P105_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2107\\\\data\\\\csv\\\\P107_10_27_Session1_P107_10_27_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2108\\\\data\\\\csv\\\\P109_10_28_Session1_P109_10_28_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2109\\\\data\\\\csv\\\\P101_11_3_Session1_P109_11_3_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2110\\\\data\\\\csv\\\\P110_11_3_Session1_P110_11_3_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2111\\\\data\\\\csv\\\\P111_11_3_Session1_P111_11_3_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2113\\\\data\\\\csv\\\\P113_11_3_Session1_P113_11_3_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2114\\\\data\\\\csv\\\\P114_11_3_Session1_P114_11_3_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2115\\\\data\\\\csv\\\\P115_11_3_Session1_P115_11_3_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2116\\\\data\\\\csv\\\\P116_11_3_Session1_P116_11_3_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2117\\\\data\\\\csv\\\\P117_11_3_Session1_P117_11_3_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2120\\\\data\\\\csv\\\\P120_11_4_Session1_P120_11_4_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2121\\\\data\\\\csv\\\\P122_11_4_Session1_P122_11_4_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2122\\\\data\\\\csv\\\\P121_11_4_Session1_P121_11_4_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2123\\\\data\\\\fcsv\\\\P123_11_4_Session1_P123_11_4_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2125\\\\data\\\\csv\\\\P125_11_4_Session1_P125_11_4_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2126\\\\data\\\\csv\\\\P126_11_4_Session1_P126_11_4_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2127\\\\data\\\\csv\\\\P127_11_4_Session1_P127_11_4_Calibrated-clip.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2128\\\\data\\\\csv\\\\P128_11_4_Session1_P128_11_4_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2129\\\\data\\\\csv\\\\P129_11_10_Session1_P129_11_10_Calibrated-split.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2130\\\\data\\\\csv\\\\P130_11_9_Session1_P130_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2131\\\\data\\\\csv\\\\P131_11_9_Session1_P131_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2132\\\\data\\\\csv\\\\P132_11_9_Session1_P132_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2133\\\\data\\\\csv\\\\P133_11_9_Session1_P133_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2134\\\\data\\\\csv\\\\P134_11_9_Session1_P134_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2135\\\\data\\\\csv\\\\P135_11_9_Session1_P135_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2136\\\\data\\\\csv\\\\P136_11_9_Session1_P136_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2138\\\\data\\\\csv\\\\P138_11_9_Session1_P138_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2139\\\\data\\\\csv\\\\P139_11_9_Session1_P139_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2140\\\\data\\\\csv\\\\P140_11_9_Session1_P140_11_9_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2141\\\\data\\\\csv\\\\P141_11_10_Session1_P141_11_10_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2142\\\\data\\\\csv\\\\P142_11_10_Session1_P142_11_10_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2143\\\\data\\\\csv\\\\P143_11_10_Session1_P143_11_10_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2144\\\\data\\\\csv\\\\P144_11_11_Session1_P144_11_11_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2145\\\\data\\\\csv\\\\P146_11_11_Session1_P146_11_11_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2146\\\\data\\\\csv\\\\P146_11_17_Session1_P146_11_17_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2148\\\\data\\\\csv\\\\P148_11_11_Session1_P148_11_11_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2149\\\\data\\\\csv\\\\P149_11_11_Session1_P149_11_11_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2150\\\\data\\\\csv\\\\P150_11_16_Session1_P150_11_16_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2151\\\\data\\\\csv\\\\P151_11_16_Session1_P151_11_16_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2153\\\\data\\\\csv\\\\P153_11_17_Session1_P153_11_17_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2154\\\\data\\\\csv\\\\P154_11_17_Session1_P154_11_17_Calibrated.csv']\n",
      "['C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/CSV1\\\\P2155\\\\data\\\\csv\\\\P155_11_17_Session1_P155_11_17_Calibrated.csv']\n",
      "Total particpant number= 66\n",
      "P2055 0.015151515151515152\n",
      "P2120 0.030303030303030304\n",
      "P2150 0.045454545454545456\n",
      "P2031 0.06060606060606061\n",
      "P2046 0.07575757575757576\n",
      "P2101 0.09090909090909091\n",
      "P2038 0.10606060606060606\n",
      "P2044 0.12121212121212122\n",
      "P2057 0.13636363636363635\n",
      "P2125 0.15151515151515152\n",
      "P2015 0.16666666666666666\n",
      "P2149 0.18181818181818182\n",
      "P2144 0.19696969696969696\n",
      "P2129 0.21212121212121213\n",
      "P2018 0.22727272727272727\n",
      "P2131 0.24242424242424243\n",
      "P2108 0.25757575757575757\n",
      "P2111 0.2727272727272727\n",
      "P2113 0.2878787878787879\n",
      "P2148 0.30303030303030304\n",
      "P2153 0.3181818181818182\n",
      "P2126 0.3333333333333333\n",
      "P2030 0.3484848484848485\n",
      "P2135 0.36363636363636365\n",
      "P2143 0.3787878787878788\n",
      "P2151 0.3939393939393939\n",
      "P2013 0.4090909090909091\n",
      "P2001 0.42424242424242425\n",
      "P2154 0.4393939393939394\n",
      "P2122 0.45454545454545453\n",
      "P2141 0.4696969696969697\n",
      "P2100 0.48484848484848486\n",
      "P2128 0.5\n",
      "P2033 0.5151515151515151\n",
      "P2036 0.5303030303030303\n",
      "P2020 0.5454545454545454\n",
      "P2102 0.5606060606060606\n",
      "P2109 0.5757575757575758\n",
      "P2146 0.5909090909090909\n",
      "P2145 0.6060606060606061\n",
      "P2121 0.6212121212121212\n",
      "P2127 0.6363636363636364\n",
      "P2134 0.6515151515151515\n",
      "P2139 0.6666666666666666\n",
      "P2058 0.6818181818181818\n",
      "P2051 0.696969696969697\n",
      "P2110 0.7121212121212122\n",
      "P2063 0.7272727272727273\n",
      "P2117 0.7424242424242424\n",
      "P2012 0.7575757575757576\n",
      "P2017 0.7727272727272727\n",
      "P2014 0.7878787878787878\n",
      "P2103 0.803030303030303\n",
      "P2130 0.8181818181818182\n",
      "P2133 0.8333333333333334\n",
      "P2142 0.8484848484848485\n",
      "P2140 0.8636363636363636\n",
      "P2116 0.8787878787878788\n",
      "P2016 0.8939393939393939\n",
      "P2132 0.9090909090909091\n",
      "P2138 0.9242424242424242\n",
      "P2115 0.9393939393939394\n",
      "P2136 0.9545454545454546\n",
      "P2114 0.9696969696969697\n",
      "P2107 0.9848484848484849\n",
      "P2105 1.0\n"
     ]
    }
   ],
   "source": [
    "def dfCompactor(df):\n",
    "    df['Date']=df['Date'].astype(int)\n",
    "    df['Time']=df['Time']*1000\n",
    "    df['Time']=df['Time'].astype(int)\n",
    "    df.rename(columns={\"Time\": \"Time[ms]\"})\n",
    "\n",
    "    df['GyroX']=df['GyroX'].astype(float)\n",
    "    df['GyroX']=df['GyroX']*1000*1000\n",
    "    df['GyroX']=df['GyroX'].astype(int)\n",
    "    df.rename(columns={\"GyroX\": \"GyroX[microD/s]\"})\n",
    "\n",
    "    df['GyroY']=df['GyroY'].astype(float)\n",
    "    df['GyroY']=df['GyroY']*1000*1000\n",
    "    df['GyroY']=df['GyroY'].astype(int)\n",
    "    df.rename(columns={\"GyroY\": \"GyroY[microD/s]\"})\n",
    "\n",
    "    df['GyroZ']=df['GyroZ'].astype(float)\n",
    "    df['GyroZ']=df['GyroZ']*1000*1000\n",
    "    df['GyroZ']=df['GyroZ'].astype(int)\n",
    "    df.rename(columns={\"GyroZ\": \"GyroZ[microD/s]\"})\n",
    "\n",
    "    df['AccelX']=df['AccelX'].astype(float)\n",
    "    df['AccelX']=df['AccelX']*1000*1000\n",
    "    df['AccelX']=df['AccelX'].astype(int)\n",
    "    df.rename(columns={\"AccelX\": \"AccelX[microm/s2]\"})\n",
    "\n",
    "    df['AccelY']=df['AccelY'].astype(float)\n",
    "    df['AccelY']=df['AccelY']*1000*1000\n",
    "    df['AccelY']=df['AccelY'].astype(int)\n",
    "    df.rename(columns={\"AccelY\": \"AccelY[microm/s2]\"})\n",
    "\n",
    "    df['AccelZ']=df['AccelZ'].astype(float)\n",
    "    df['AccelZ']=df['AccelZ']*1000*1000\n",
    "    df['AccelZ']=df['AccelZ'].astype(int)\n",
    "    df.rename(columns={\"AccelZ\": \"AccelZ[microm/s2]\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "def dfOrganizer(df):\n",
    "    df.columns.values[2]='TimeStamp'\n",
    "\n",
    "    df.columns.values[8]='GyroX'\n",
    "    df.columns.values[9]='GyroY'\n",
    "    df.columns.values[10]='GyroZ'\n",
    "\n",
    "    df.columns.values[11]='AccelX'\n",
    "    df.columns.values[12]='AccelY'\n",
    "    df.columns.values[13]='AccelZ'\n",
    "\n",
    "    df = df.filter(['Name','TimeStamp','GyroX','GyroY','GyroZ','AccelX','AccelY','AccelZ'])\n",
    "    df['TimeStamp'] = df['TimeStamp'].astype(float)\n",
    "    df['TimeStamp']=df['TimeStamp']-1000*3600*4 #fixing the timezone\n",
    "\n",
    "    df.insert(2,'Date',float('nan'))\n",
    "    df.insert(3,'Time',float('nan'))\n",
    "\n",
    "    df['Date']=pd.to_datetime(df['TimeStamp'],unit='ms')\n",
    "    df['Time']=pd.to_datetime(df['TimeStamp'],unit='ms')\n",
    "    df['Date']=df['Date'].dt.dayofyear\n",
    "    df['Time']=df['Time'].dt.hour*3600+df['Time'].dt.minute*60+df['Time'].dt.second+df['Time'].dt.microsecond*0.001*0.001\n",
    "\n",
    "    df.drop(columns=['TimeStamp'],inplace=True)\n",
    "    return df\n",
    "\n",
    "def csvReader(addressPrefix):\n",
    "    dataFiles=[]\n",
    "    for root, dirs, files in os.walk(addressPrefix, topdown=False):\n",
    "       for name in files:\n",
    "           if '.csv' in name:\n",
    "               dataFiles.append([os.path.join(root,name)])\n",
    "    for counter,element in enumerate(dataFiles):\n",
    "        print(element)\n",
    "        rows = []\n",
    "        with open(element[0], 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile,delimiter = \"\\t\")\n",
    "            next(csvreader) #skipping the first junk line\n",
    "            headers = next(csvreader) #column titles\n",
    "            while '' in headers:\n",
    "                headers.remove(\"\")\n",
    "            if len(headers)!=17:\n",
    "                continue\n",
    "            next(csvreader) #skipping the units\n",
    "            for row in csvreader:\n",
    "                rows.append(row)\n",
    "        df = pd.DataFrame(rows,columns=headers)\n",
    "        participantName=element[0]\n",
    "        participantName=participantName[participantName.find('CSV1')+5:participantName.find('CSV1')+10]\n",
    "        df.insert(0,'Name',participantName)\n",
    "\n",
    "        df=dfOrganizer(df)\n",
    "        df=dfCompactor(df)\n",
    "\n",
    "        if counter==0:\n",
    "            dfTotal=df\n",
    "        else:\n",
    "            frames=[dfTotal,df]\n",
    "            dfTotal=pd.concat(frames)\n",
    "\n",
    "        # if counter==1:\n",
    "        #     break\n",
    "    dfTotal.sort_values(by=['Name', 'Date','Time'],inplace=True)\n",
    "    return dfTotal\n",
    "\n",
    "def preProcessor(dfTotal,R):\n",
    "    columns=dfTotal.columns.values\n",
    "    names=dfTotal['Name'].tolist()\n",
    "    names=list(set(names))\n",
    "\n",
    "    dfProc=pd.DataFrame([],columns=columns)\n",
    "    for counter,name in enumerate(names):\n",
    "        print(name, (counter+1)/len(names))\n",
    "        df=dfTotal[dfTotal['Name']==name]\n",
    "        for column in columns:\n",
    "            if column!='Time' and column!='Date' and column!='Name':\n",
    "                df[column]=list(gaussian_filter1d(df[column].tolist(),sigma=R))\n",
    "            df['GyroX']=df['GyroX'].astype(float)\n",
    "\n",
    "        df['GyroX']=df['GyroX'].astype(int)\n",
    "        df['GyroY']=df['GyroY'].astype(int)\n",
    "        df['GyroZ']=df['GyroZ'].astype(int)\n",
    "        df['AccelX']=df['AccelX'].astype(int)\n",
    "        df['AccelY']=df['AccelY'].astype(int)\n",
    "        df['AccelZ']=df['AccelZ'].astype(int)\n",
    "\n",
    "        frames=[dfProc,df]\n",
    "        dfProc=pd.concat(frames)\n",
    "        nameIndex = dfTotal[(dfTotal.Name == name)].index\n",
    "        dfTotal.drop(nameIndex,inplace=True)\n",
    "    return dfProc\n",
    "\n",
    "def funcCaller(addressPrefix):\n",
    "    if os.path.exists(os.path.join(addressPrefix,'RawData.csv')):\n",
    "        dfRaw=pd.read_csv(os.path.join(addressPrefix,'RawData.csv'))\n",
    "    else:\n",
    "        dfRaw=csvReader(os.path.join(addressPrefix,'CSV1'))\n",
    "        dfRaw.to_csv(os.path.join(addressPrefix,'RawData.csv'),index=False)\n",
    "    names=dfRaw['Name'].tolist()\n",
    "    names=list(set(names))\n",
    "    print('Total particpant number=',len(names))\n",
    "\n",
    "    if os.path.exists(os.path.join(addressPrefix,'FilteredData.csv')):\n",
    "        dfProcessed=pd.read_csv(os.path.join(addressPrefix,'FilteredData.csv'))\n",
    "    else:\n",
    "        dfProcessed=preProcessor(dfRaw,R=3)\n",
    "        dfProcessed.to_csv(os.path.join(addressPrefix,'FilteredData.csv'),index=False)\n",
    "    return dfProcessed\n",
    "\n",
    "dfTotal=funcCaller(addressPrefix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def labelReader(addressPrefix):\n",
    "    dataFiles=[]\n",
    "    for root, dirs, files in os.walk(addressPrefix, topdown=False):\n",
    "       for name in files:\n",
    "           if '.txt' in name:\n",
    "               dataFiles.append([os.path.join(root,name),name])\n",
    "    mealTime=[]\n",
    "    for counter,element in enumerate(dataFiles):\n",
    "        nameTemp=element[1]\n",
    "        nameTemp=nameTemp[:nameTemp.find('-events')]\n",
    "        with open(element[0], 'r+') as txtfile:\n",
    "            fileData = txtfile.read()\n",
    "            fileData=fileData.splitlines()\n",
    "            while '' in fileData:\n",
    "                fileData.remove('')\n",
    "            for counter in range(1,len(fileData)-1):\n",
    "                tempStr=fileData[counter]\n",
    "                tempStr=tempStr.split()\n",
    "                mealTime.append([nameTemp,tempStr[1],tempStr[2]])\n",
    "    df=pd.DataFrame(mealTime,columns=['Name','Start','End'])\n",
    "    df['Start'] = pd.to_datetime(df['Start'],format= '%H:%M:%S',errors='coerce')\n",
    "    df = df[df['Start'].notna()]\n",
    "    df['Start']=df['Start'].dt.hour*3600*1000+df['Start'].dt.minute*60*1000+df['Start'].dt.second*1000\n",
    "\n",
    "    df['End'] = pd.to_datetime(df['End'],format= '%H:%M:%S',errors='coerce')\n",
    "    df = df[df['End'].notna()]\n",
    "    df['End']=df['End'].dt.hour*3600*1000+df['End'].dt.minute*60*1000+df['End'].dt.second*1000\n",
    "    df.sort_values(by=['Name', 'Start','End'],inplace=True)\n",
    "    return df\n",
    "\n",
    "def featureExtractor(df):\n",
    "    windowLength=30*1000\n",
    "    featureData=[]\n",
    "    names=df['Name'].tolist()\n",
    "    names=list(set(names))\n",
    "    for name in names:\n",
    "        dfName=df[df['Name']==name]\n",
    "        startTime=dfName['Time'].min()\n",
    "        endTime=startTime+windowLength\n",
    "        while startTime<24*3600*1000:\n",
    "            dfTemp=dfName[dfName['Time']>=startTime]\n",
    "            dfTemp=dfTemp[dfTemp['Time']<endTime]\n",
    "            if len(dfTemp)>5:\n",
    "                f2=abs(dfTemp['AccelX'].values)+abs(dfTemp['AccelY'].values)+abs(dfTemp['AccelZ'].values)\n",
    "                f1=abs(dfTemp['GyroX'].values)+abs(dfTemp['GyroY'].values)+abs(dfTemp['GyroZ'].values)\n",
    "                f1=f1/f2\n",
    "                f1=np.mean(f1)\n",
    "                f2=np.mean(f2)\n",
    "                featureData.append([name,startTime,endTime,f1,f2])\n",
    "            startTime+=windowLength\n",
    "            endTime+=windowLength\n",
    "        break\n",
    "    return featureData\n",
    "\n",
    "dfLabel=labelReader(os.path.join(addressPrefix,'EVENTfiles'))\n",
    "featureData=featureExtractor(dfTotal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def labelExtractor(dfLabel,features):\n",
    "    dataTotal=[]\n",
    "    for feature in features:\n",
    "        windowName=feature[0]\n",
    "        windowStart=feature[1]\n",
    "        windowEnd=feature[2]\n",
    "        f1=feature[3]\n",
    "        f2=feature[4]\n",
    "        dfTemp=dfLabel[dfLabel['Name']==windowName]\n",
    "        if len(dfTemp)==0:\n",
    "            print('skipped',windowName)\n",
    "            continue\n",
    "        eatingFlag=False\n",
    "        for counter in range(0,len(dfTemp)):\n",
    "            if dfTemp.iloc[counter,1]<windowEnd and dfTemp.iloc[counter,2]>windowStart:\n",
    "                eatingFlag=True\n",
    "                break\n",
    "        dataTotal.append([f1,f2,eatingFlag])\n",
    "    dataTotal=np.asarray(dataTotal)\n",
    "    return dataTotal\n",
    "\n",
    "data=labelExtractor(dfLabel,featureData)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on validation dataset:\n",
      "[[332   6]\n",
      " [  6   2]]\n",
      "Accuracy 97.0 Recall 25.0 Precision 25.0\n",
      "Testing on validation dataset:\n",
      "[[325  12]\n",
      " [ 11   1]]\n",
      "Accuracy 93.0 Recall 8.0 Precision 8.0\n"
     ]
    }
   ],
   "source": [
    "def XGClassifier(dataList, labelList,randomSeed):\n",
    "    trainData, testData, trainLabels, testLabels = train_test_split(dataList, labelList, test_size=0.25,random_state=randomSeed)\n",
    "    trainData, valData, trainLabels, valLabels = train_test_split(trainData, trainLabels, test_size=0.33,random_state=randomSeed)\n",
    "    accuracyBest=0\n",
    "    for maxDepth in np.arange(3,30):\n",
    "        for estimator in np.arange(5,50,2):\n",
    "            clf = xgb.XGBClassifier(n_estimators=estimator,max_depth=maxDepth,objective = \"binary:logistic\",\n",
    "                                    eval_metric = \"logloss\",use_label_encoder =False,scale_pos_weight=20)\n",
    "            clf.fit(trainData, trainLabels)\n",
    "            slidingWindowPrediction = clf.predict_proba(valData)\n",
    "            slidingWindowPrediction=slidingWindowPrediction[:,1]\n",
    "            slidingWindowPrediction[slidingWindowPrediction>=0.5]=1\n",
    "            slidingWindowPrediction[slidingWindowPrediction<0.5]=0\n",
    "\n",
    "            confMatrix=sklearn.metrics.confusion_matrix(valLabels,slidingWindowPrediction)\n",
    "            accuracy=sklearn.metrics.accuracy_score(valLabels,slidingWindowPrediction)\n",
    "            recall=sklearn.metrics.recall_score(valLabels,slidingWindowPrediction)\n",
    "            precision=sklearn.metrics.precision_score(valLabels,slidingWindowPrediction)\n",
    "\n",
    "            if accuracy>accuracyBest:\n",
    "                confMatrixBest=confMatrix\n",
    "                accuracyBest=accuracy\n",
    "                modelBest=clf\n",
    "                recallBest=recall\n",
    "                precisionBest=precision\n",
    "    print('Testing on validation dataset:')\n",
    "    print(confMatrixBest)\n",
    "    print('Accuracy',np.round(100*accuracyBest,0),'Recall',np.round(100*recallBest,0),'Precision',np.round(100*precisionBest,0))\n",
    "\n",
    "    slidingWindowPrediction = modelBest.predict_proba(testData)\n",
    "    slidingWindowPrediction=slidingWindowPrediction[:,1]\n",
    "    slidingWindowPrediction[slidingWindowPrediction>=0.5]=1\n",
    "    slidingWindowPrediction[slidingWindowPrediction<0.5]=0\n",
    "\n",
    "    confMatrix=sklearn.metrics.confusion_matrix(testLabels,slidingWindowPrediction)\n",
    "    accuracy=sklearn.metrics.accuracy_score(testLabels,slidingWindowPrediction)\n",
    "    recall=sklearn.metrics.recall_score(testLabels,slidingWindowPrediction)\n",
    "    precision=sklearn.metrics.precision_score(testLabels,slidingWindowPrediction)\n",
    "\n",
    "    print('Testing on validation dataset:')\n",
    "    print(confMatrix)\n",
    "    print('Accuracy',np.round(100*accuracy,0),'Recall',np.round(100*recall,0),'Precision',np.round(100*precision,0))\n",
    "\n",
    "allData=data[:,0:2]\n",
    "allLabel=data[:,2]\n",
    "XGClassifier(allData, allLabel,randomSeed=53)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
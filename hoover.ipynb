{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import struct\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import pearsonr, mode\n",
    "from scipy.signal import savgol_filter\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import struct\n",
    "\n",
    "addressPrefix='C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/'\n",
    "if not os.path.exists(addressPrefix):\n",
    "    addressPrefix='C:/GDrive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/'\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plt.style.use({'figure.facecolor':'white'})\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# def dfCompactor(df):\n",
    "#     df['Date']=df['Date'].astype(int)\n",
    "#     df['Time']=df['Time']*1000\n",
    "#     df['Time']=df['Time'].astype(int)\n",
    "#     df.rename(columns={\"Time\": \"Time[ms]\"})\n",
    "#\n",
    "#     df['GyroX']=df['GyroX'].astype(float)\n",
    "#     df['GyroX']=df['GyroX']*1000*1000\n",
    "#     df['GyroX']=df['GyroX'].astype(int)\n",
    "#     df.rename(columns={\"GyroX\": \"GyroX[microD/s]\"})\n",
    "#\n",
    "#     df['GyroY']=df['GyroY'].astype(float)\n",
    "#     df['GyroY']=df['GyroY']*1000*1000\n",
    "#     df['GyroY']=df['GyroY'].astype(int)\n",
    "#     df.rename(columns={\"GyroY\": \"GyroY[microD/s]\"})\n",
    "#\n",
    "#     df['GyroZ']=df['GyroZ'].astype(float)\n",
    "#     df['GyroZ']=df['GyroZ']*1000*1000\n",
    "#     df['GyroZ']=df['GyroZ'].astype(int)\n",
    "#     df.rename(columns={\"GyroZ\": \"GyroZ[microD/s]\"})\n",
    "#\n",
    "#     df['AccelX']=df['AccelX'].astype(float)\n",
    "#     df['AccelX']=df['AccelX']*1000*1000\n",
    "#     df['AccelX']=df['AccelX'].astype(int)\n",
    "#     df.rename(columns={\"AccelX\": \"AccelX[microm/s2]\"})\n",
    "#\n",
    "#     df['AccelY']=df['AccelY'].astype(float)\n",
    "#     df['AccelY']=df['AccelY']*1000*1000\n",
    "#     df['AccelY']=df['AccelY'].astype(int)\n",
    "#     df.rename(columns={\"AccelY\": \"AccelY[microm/s2]\"})\n",
    "#\n",
    "#     df['AccelZ']=df['AccelZ'].astype(float)\n",
    "#     df['AccelZ']=df['AccelZ']*1000*1000\n",
    "#     df['AccelZ']=df['AccelZ'].astype(int)\n",
    "#     df.rename(columns={\"AccelZ\": \"AccelZ[microm/s2]\"})\n",
    "#\n",
    "#     return df\n",
    "#\n",
    "# def dfOrganizer(df):\n",
    "#     df.columns.values[2]='TimeStamp'\n",
    "#\n",
    "#     df.columns.values[8]='GyroX'\n",
    "#     df.columns.values[9]='GyroY'\n",
    "#     df.columns.values[10]='GyroZ'\n",
    "#\n",
    "#     df.columns.values[11]='AccelX'\n",
    "#     df.columns.values[12]='AccelY'\n",
    "#     df.columns.values[13]='AccelZ'\n",
    "#\n",
    "#     df = df.filter(['Name','TimeStamp','GyroX','GyroY','GyroZ','AccelX','AccelY','AccelZ'])\n",
    "#     df['TimeStamp'] = df['TimeStamp'].astype(float)\n",
    "#     df['TimeStamp']=df['TimeStamp']-1000*3600*4 #fixing the timezone\n",
    "#\n",
    "#     df.insert(2,'Date',float('nan'))\n",
    "#     df.insert(3,'Time',float('nan'))\n",
    "#\n",
    "#     df['Date']=pd.to_datetime(df['TimeStamp'],unit='ms')\n",
    "#     df['Time']=pd.to_datetime(df['TimeStamp'],unit='ms')\n",
    "#     df['Date']=df['Date'].dt.dayofyear\n",
    "#     df['Time']=df['Time'].dt.hour*3600+df['Time'].dt.minute*60+df['Time'].dt.second+df['Time'].dt.microsecond*0.001*0.001\n",
    "#\n",
    "#     df.drop(columns=['TimeStamp'],inplace=True)\n",
    "#     return df\n",
    "#\n",
    "# def csvReader(addressPrefix):\n",
    "#     dataFiles=[]\n",
    "#     for root, dirs, files in os.walk(addressPrefix, topdown=False):\n",
    "#        for name in files:\n",
    "#            if '.csv' in name:\n",
    "#                dataFiles.append([os.path.join(root,name)])\n",
    "#     for counter,element in enumerate(dataFiles):\n",
    "#         print(element)\n",
    "#         rows = []\n",
    "#         with open(element[0], 'r') as csvfile:\n",
    "#             csvreader = csv.reader(csvfile,delimiter = \"\\t\")\n",
    "#             next(csvreader) #skipping the first junk line\n",
    "#             headers = next(csvreader) #column titles\n",
    "#             while '' in headers:\n",
    "#                 headers.remove(\"\")\n",
    "#             if len(headers)!=17:\n",
    "#                 continue\n",
    "#             next(csvreader) #skipping the units\n",
    "#             for row in csvreader:\n",
    "#                 rows.append(row)\n",
    "#         df = pd.DataFrame(rows,columns=headers)\n",
    "#         participantName=element[0]\n",
    "#         participantName=participantName[participantName.find('CSV1')+5:participantName.find('CSV1')+10]\n",
    "#         df.insert(0,'Name',participantName)\n",
    "#\n",
    "#         df=dfOrganizer(df)\n",
    "#         df=dfCompactor(df)\n",
    "#\n",
    "#         if counter==0:\n",
    "#             dfTotal=df\n",
    "#         else:\n",
    "#             frames=[dfTotal,df]\n",
    "#             dfTotal=pd.concat(frames)\n",
    "#\n",
    "#         # if counter==1:\n",
    "#         #     break\n",
    "#     dfTotal.sort_values(by=['Name', 'Date','Time'],inplace=True)\n",
    "#     return dfTotal\n",
    "#\n",
    "# def preProcessor(dfTotal,R):\n",
    "#     columns=dfTotal.columns.values\n",
    "#     names=dfTotal['Name'].tolist()\n",
    "#     names=list(set(names))\n",
    "#\n",
    "#     dfProc=pd.DataFrame([],columns=columns)\n",
    "#     for counter,name in enumerate(names):\n",
    "#         print(name, (counter+1)/len(names))\n",
    "#         df=dfTotal[dfTotal['Name']==name]\n",
    "#         for column in columns:\n",
    "#             if column!='Time' and column!='Date' and column!='Name':\n",
    "#                 df[column]=list(gaussian_filter1d(df[column].tolist(),sigma=R))\n",
    "#             df['GyroX']=df['GyroX'].astype(float)\n",
    "#\n",
    "#         df['GyroX']=df['GyroX'].astype(int)\n",
    "#         df['GyroY']=df['GyroY'].astype(int)\n",
    "#         df['GyroZ']=df['GyroZ'].astype(int)\n",
    "#         df['AccelX']=df['AccelX'].astype(int)\n",
    "#         df['AccelY']=df['AccelY'].astype(int)\n",
    "#         df['AccelZ']=df['AccelZ'].astype(int)\n",
    "#\n",
    "#         frames=[dfProc,df]\n",
    "#         dfProc=pd.concat(frames)\n",
    "#         nameIndex = dfTotal[(dfTotal.Name == name)].index\n",
    "#         dfTotal.drop(nameIndex,inplace=True)\n",
    "#     return dfProc\n",
    "#\n",
    "# def funcCaller(addressPrefix):\n",
    "#     if os.path.exists(os.path.join(addressPrefix,'RawData.csv')):\n",
    "#         dfRaw=pd.read_csv(os.path.join(addressPrefix,'RawData.csv'))\n",
    "#     else:\n",
    "#         dfRaw=csvReader(os.path.join(addressPrefix,'CSV1'))\n",
    "#         dfRaw.to_csv(os.path.join(addressPrefix,'RawData.csv'),index=False)\n",
    "#     names=dfRaw['Name'].tolist()\n",
    "#     names=list(set(names))\n",
    "#     print('Total particpant number=',len(names))\n",
    "#\n",
    "#     if os.path.exists(os.path.join(addressPrefix,'FilteredData.csv')):\n",
    "#         dfProcessed=pd.read_csv(os.path.join(addressPrefix,'FilteredData.csv'))\n",
    "#     else:\n",
    "#         dfProcessed=preProcessor(dfRaw,R=3)\n",
    "#         dfProcessed.to_csv(os.path.join(addressPrefix,'FilteredData.csv'),index=False)\n",
    "#     return dfProcessed\n",
    "#\n",
    "# dfTotal=funcCaller(addressPrefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def pdFromatter(df):\n",
    "    for counter in range(len(df)):\n",
    "        tempStr=df.iloc[counter,1]\n",
    "        tempVal=int(tempStr[0:2])*3600+int(tempStr[3:5])*60+int(tempStr[6:8])\n",
    "        tempVal*=1000\n",
    "        df.iloc[counter,1]=tempVal\n",
    "\n",
    "        tempStr=df.iloc[counter,2]\n",
    "        tempVal=int(tempStr[0:2])*3600+int(tempStr[3:5])*60+int(tempStr[6:8])\n",
    "        tempVal*=1000\n",
    "        df.iloc[counter,2]=tempVal\n",
    "        if df.iloc[counter,1]>df.iloc[counter,2]:\n",
    "            df.iloc[counter,2]+=24*3600*1000\n",
    "    df.sort_values(by=['Name','Start','End'],inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def labelReader(addressPrefix):\n",
    "    labelFiles=[]\n",
    "    for root, dirs, files in os.walk(addressPrefix, topdown=False):\n",
    "       for name in files:\n",
    "           if '.txt' in name:\n",
    "               labelFiles.append([os.path.join(root,name),name])\n",
    "    mealTime=[]\n",
    "    sensorTiming=[]\n",
    "    for element in labelFiles:\n",
    "        nameTemp=element[1]\n",
    "        nameTemp=nameTemp[:nameTemp.find('-events')]\n",
    "        with open(element[0], 'r+') as txtfile:\n",
    "            fileData = txtfile.read()\n",
    "            fileData=fileData.splitlines()\n",
    "            while '' in fileData:\n",
    "                fileData.remove('')\n",
    "            tempStart=fileData[0]\n",
    "            tempStart=tempStart.split()\n",
    "            tempStart=tempStart[2]\n",
    "\n",
    "            tempEnd=fileData[-1]\n",
    "            tempEnd=tempEnd.split()\n",
    "            tempEnd=tempEnd[2]\n",
    "\n",
    "            sensorTiming.append([nameTemp,tempStart,tempEnd])\n",
    "            for counter in range(1,len(fileData)-1):\n",
    "                tempStr=fileData[counter]\n",
    "                tempStr=tempStr.split()\n",
    "                mealTime.append([nameTemp,tempStr[1],tempStr[2]])\n",
    "\n",
    "    dfMeal=pd.DataFrame(mealTime,columns=['Name','Start','End'])\n",
    "    dfMeal=pdFromatter(dfMeal)\n",
    "\n",
    "    dfTime=pd.DataFrame(sensorTiming,columns=['Name','Start','End'])\n",
    "    dfTime=pdFromatter(dfTime)\n",
    "\n",
    "    return dfMeal,dfTime\n",
    "\n",
    "def shimmerReader(element):\n",
    "    nameTemp=element[1]\n",
    "    dataList=[]\n",
    "    tempList=[]\n",
    "    with open(element[0], mode='rb') as txtfile:\n",
    "        fileData = txtfile.read()\n",
    "        for i in range(int(len(fileData)/4)):\n",
    "            if i%6==0 and i!=0:\n",
    "                tempList.append(nameTemp)\n",
    "                dataList.append(tempList)\n",
    "                tempList=[]\n",
    "            tempVal=fileData[i*4:(i+1)*4]\n",
    "            tempVal=struct.unpack('f',tempVal)\n",
    "            tempVal=tempVal[0]\n",
    "            tempList.append(tempVal)\n",
    "    txtfile.close()\n",
    "\n",
    "    dfSensor=pd.DataFrame(dataList,columns=[ 'X','Y','Z','Yaw','Pitch','Roll','Name'])\n",
    "    dfSensor=dfSensor[['Name','X','Y','Z','Yaw','Pitch','Roll']]\n",
    "    del dataList\n",
    "    return dfSensor\n",
    "\n",
    "def timeFinder(dfSensor,dfTime):\n",
    "    dfSensor.insert(1,'Time',float('nan'))\n",
    "    name=dfSensor['Name'].tolist()\n",
    "    name=name[0]\n",
    "    dfTemp=dfTime[dfTime['Name']==name]\n",
    "\n",
    "    if len(dfTemp)>1:\n",
    "        print('More than one event file for:',name)\n",
    "        return\n",
    "    elif len(dfTemp)==0:\n",
    "        print('No event file for:',name)\n",
    "        return\n",
    "    startTemp=dfTemp['Start'].tolist()\n",
    "    endTemp=dfTemp['End'].tolist()\n",
    "    tempTimeStamp=np.linspace(startTemp,endTemp,num=len(dfSensor))\n",
    "    dfSensor['Time']=tempTimeStamp\n",
    "    return dfSensor\n",
    "\n",
    "def featureExtractor(df):\n",
    "    windowLength=30*1000\n",
    "    featureData=[]\n",
    "    name=df['Name'].tolist()\n",
    "    name=name[0]\n",
    "    dfName=df[df['Name']==name]\n",
    "    startTime=dfName['Time'].min()\n",
    "    endTime=startTime+windowLength\n",
    "\n",
    "    while startTime<24*3600*1000:\n",
    "        dfTemp=dfName[dfName['Time']>=startTime]\n",
    "        dfTemp=dfTemp[dfTemp['Time']<endTime]\n",
    "        if len(dfTemp)>5*15:\n",
    "            f1=abs(dfTemp['Yaw'].values)+abs(dfTemp['Roll'].values)+abs(dfTemp['Pitch'].values)\n",
    "            f2=abs(dfTemp['X'].values)+abs(dfTemp['Y'].values)+abs(dfTemp['Z'].values)\n",
    "            f2+=0.0001\n",
    "            f1=f1/f2\n",
    "            f1=np.mean(f1)\n",
    "            f2=np.mean(f2)\n",
    "            if np.isnan(f1):\n",
    "                print('Nan F1 Value')\n",
    "                continue\n",
    "            featureData.append([name,startTime,endTime,f1,f2])\n",
    "        startTime+=windowLength\n",
    "        endTime+=windowLength\n",
    "    return featureData\n",
    "\n",
    "def labelExtractor(dfMeal,features):\n",
    "    dataTotal=[]\n",
    "    for feature in features:\n",
    "        windowName=feature[0]\n",
    "        windowStart=feature[1]\n",
    "        windowEnd=feature[2]\n",
    "        f1=feature[3]\n",
    "        f2=feature[4]\n",
    "        dfTemp=dfMeal[dfMeal['Name']==windowName]\n",
    "        if len(dfTemp)==0:\n",
    "            print('skipped',windowName)\n",
    "            break\n",
    "        eatingFlag=False\n",
    "        for counter in range(0,len(dfTemp)):\n",
    "            if dfTemp.iloc[counter,1]<windowEnd and dfTemp.iloc[counter,2]>windowStart:\n",
    "                eatingFlag=True\n",
    "                break\n",
    "        dataTotal.append([windowName,f1,f2,eatingFlag])\n",
    "    return dataTotal\n",
    "\n",
    "def callerFunc(element,dfMeal,dfTime):\n",
    "    print(element)\n",
    "    allData=[]\n",
    "    dfSensor=shimmerReader(element)\n",
    "    dfSensor=timeFinder(dfSensor,dfTime)\n",
    "    featureData=featureExtractor(dfSensor)\n",
    "    allData.extend(labelExtractor(dfMeal,featureData))\n",
    "\n",
    "    return allData\n",
    "\n",
    "def main(addressPrefix):\n",
    "    shimmerFiles=[]\n",
    "    dfMeal,dfTime=labelReader(os.path.join(addressPrefix,'EVENTfiles'))\n",
    "\n",
    "    for root, dirs, files in os.walk(os.path.join(addressPrefix,'SHMfiles'), topdown=False):\n",
    "       for name in files:\n",
    "           if '.shm' in name:\n",
    "               shimmerFiles.append([os.path.join(root,name),name[:-4]])\n",
    "\n",
    "    allData=Parallel(n_jobs=num_cores)(delayed(callerFunc)(i, dfMeal,dfTime) for i in shimmerFiles)\n",
    "    allData = [ item for elem in allData for item in elem]\n",
    "    dfAllData=pd.DataFrame(allData,columns=['Name','F1','F2','EatingFlag'])\n",
    "    dfAllData.to_csv(r'C:\\GitHub\\AllDataParallel.csv',index=False)\n",
    "    return allData\n",
    "\n",
    "allData=main(addressPrefix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sorush.omidvar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sorush.omidvar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sorush.omidvar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sorush.omidvar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sorush.omidvar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sorush.omidvar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sorush.omidvar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sorush.omidvar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on validation dataset:\n",
      "[[0.84878391 0.09334579]\n",
      " [0.03597185 0.02189845]]\n",
      "Accuracy 87.0 Recall 38.0 Precision 19.0\n",
      "MaxDepth: 2 EstimatorBest: 50 Best threshold: 0.4\n",
      "Testing on test dataset:\n",
      "[[0.84624223 0.093778  ]\n",
      " [0.03694175 0.02303801]]\n",
      "Accuracy 87.0 Recall 38.0 Precision 20.0\n"
     ]
    }
   ],
   "source": [
    "def XGClassifier(dataList, labelList,randomSeed):\n",
    "    trainData, testData, trainLabels, testLabels = train_test_split(dataList, labelList, test_size=0.25,random_state=randomSeed)\n",
    "    trainData, valData, trainLabels, valLabels = train_test_split(trainData, trainLabels, test_size=0.33,random_state=randomSeed)\n",
    "    recallBest=0\n",
    "    for maxDepth in np.arange(2,6):\n",
    "        for estimator in [20,50,80,120,180,250]:\n",
    "            clf = xgb.XGBClassifier(n_estimators=estimator,max_depth=maxDepth,objective = \"binary:logistic\",\n",
    "                                    eval_metric = \"logloss\",use_label_encoder =False,scale_pos_weight=5)\n",
    "            clf.fit(trainData, trainLabels)\n",
    "            for threshold in [0.4,0.6,0.8]:\n",
    "                slidingWindowPrediction = clf.predict_proba(valData)\n",
    "                slidingWindowPrediction=slidingWindowPrediction[:,1]\n",
    "                slidingWindowPrediction[slidingWindowPrediction>=threshold]=1\n",
    "                slidingWindowPrediction[slidingWindowPrediction<threshold]=0\n",
    "\n",
    "                confMatrix=sklearn.metrics.confusion_matrix(valLabels,slidingWindowPrediction,normalize='all')\n",
    "                accuracy=sklearn.metrics.accuracy_score(valLabels,slidingWindowPrediction)\n",
    "                recall=sklearn.metrics.recall_score(valLabels,slidingWindowPrediction)\n",
    "                precision=sklearn.metrics.precision_score(valLabels,slidingWindowPrediction)\n",
    "                f1=sklearn.metrics.f1_score(valLabels,slidingWindowPrediction,average='weighted')\n",
    "\n",
    "                if recall>recallBest:\n",
    "                    f1Best=f1\n",
    "                    maxDepthBest=maxDepth\n",
    "                    estimatorBest=estimator\n",
    "                    confMatrixBest=confMatrix\n",
    "                    accuracyBest=accuracy\n",
    "                    modelBest=clf\n",
    "                    recallBest=recall\n",
    "                    precisionBest=precision\n",
    "                    thresholdBest=threshold\n",
    "    print('Testing on validation dataset:')\n",
    "    print(confMatrixBest)\n",
    "    print('Accuracy',np.round(100*accuracyBest,0),'Recall',np.round(100*recallBest,0),'Precision',np.round(100*precisionBest,0))\n",
    "    print('MaxDepth:',maxDepthBest,\"EstimatorBest:\",estimatorBest,'Best threshold:',thresholdBest)\n",
    "\n",
    "    slidingWindowPrediction = modelBest.predict_proba(testData)\n",
    "    slidingWindowPrediction=slidingWindowPrediction[:,1]\n",
    "    slidingWindowPrediction[slidingWindowPrediction>=thresholdBest]=1\n",
    "    slidingWindowPrediction[slidingWindowPrediction<thresholdBest]=0\n",
    "\n",
    "    confMatrix=sklearn.metrics.confusion_matrix(testLabels,slidingWindowPrediction,normalize='all')\n",
    "    accuracy=sklearn.metrics.accuracy_score(testLabels,slidingWindowPrediction)\n",
    "    recall=sklearn.metrics.recall_score(testLabels,slidingWindowPrediction)\n",
    "    precision=sklearn.metrics.precision_score(testLabels,slidingWindowPrediction)\n",
    "\n",
    "    print('Testing on test dataset:')\n",
    "    print(confMatrix)\n",
    "    print('Accuracy',np.round(100*accuracy,0),'Recall',np.round(100*recall,0),'Precision',np.round(100*precision,0))\n",
    "\n",
    "data=pd.read_csv(r'C:\\Users\\sorush.omidvar\\Google Drive\\Documents\\Educational\\TAMU\\Research\\CGM Dataset\\Hoover\\AllDataParallel.csv')\n",
    "f1Data=data['F1'].values\n",
    "f1Data=np.asarray(f1Data)\n",
    "\n",
    "f2Data=data['F2'].values\n",
    "f2Data=np.asarray(f2Data)\n",
    "\n",
    "allData=np.zeros((len(f1Data),2))\n",
    "allData[:,0]=f1Data\n",
    "allData[:,1]=f2Data\n",
    "\n",
    "allData[:,0]-=np.mean(allData[:,0])\n",
    "allData[:,0]/=np.std(allData[:,0])\n",
    "\n",
    "allData[:,1]-=np.mean(allData[:,1])\n",
    "allData[:,1]/=np.std(allData[:,1])\n",
    "\n",
    "allLabel=data['EatingFlag'].values\n",
    "allLabel=np.asarray(allLabel,dtype=int)\n",
    "\n",
    "XGClassifier(allData, allLabel,randomSeed=53)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
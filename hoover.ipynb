{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import struct\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import pearsonr, mode, kurtosis, skew\n",
    "from scipy.signal import savgol_filter\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import copy\n",
    "import warnings\n",
    "import multiprocessing\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import mean_squared_error, plot_confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from joblib import Parallel, delayed\n",
    "# import tsfel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "if os.path.exists(\"/home/grads/s/sorush.omidvar/CGMDataset/Hoover/\"):\n",
    "    addressPrefix=\"/home/grads/s/sorush.omidvar/CGMDataset/Hoover/\"\n",
    "    \n",
    "elif os.path.exists(\"C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/\"):\n",
    "    addressPrefix=\"C:/Users/sorush.omidvar/Google Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/\"\n",
    "    \n",
    "elif os.path.exists(\"C:/GDrive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/\"):\n",
    "    addressPrefix=\"C:/GDrive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover/\"\n",
    "    \n",
    "elif os.path.exists(\"/Users/sorush/Google Drive/My Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover\"):\n",
    "    addressPrefix=\"/Users/sorush/Google Drive/My Drive/Documents/Educational/TAMU/Research/CGM Dataset/Hoover\"\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plt.style.use({\"figure.facecolor\": \"white\"})\n",
    "coreNumber = np.min([multiprocessing.cpu_count(), 36])\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pdFormatter(df):\n",
    "    for counter in range(len(df)):\n",
    "        tempStr = df.iloc[counter, 1]\n",
    "        tempVal = int(tempStr[0:2]) * 3600 + int(tempStr[3:5]) * 60 + int(tempStr[6:8])\n",
    "        tempVal *= 1000\n",
    "        df.iloc[counter, 1] = tempVal\n",
    "\n",
    "        tempStr = df.iloc[counter, 2]\n",
    "        tempVal = int(tempStr[0:2]) * 3600 + int(tempStr[3:5]) * 60 + int(tempStr[6:8])\n",
    "        tempVal *= 1000\n",
    "        df.iloc[counter, 2] = tempVal\n",
    "        if df.iloc[counter, 1] > df.iloc[counter, 2]:\n",
    "            df.iloc[counter, 2] += 24 * 3600 * 1000\n",
    "    df.sort_values(by=[\"Name\", \"Start\", \"End\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def labelReader(addressPrefix):\n",
    "    labelFiles = []\n",
    "    for root, dirs, files in os.walk(addressPrefix, topdown=False):\n",
    "        for name in files:\n",
    "            if \".txt\" in name:\n",
    "                labelFiles.append([os.path.join(root, name), name])\n",
    "    mealTime = []\n",
    "    sensorTiming = []\n",
    "    for element in labelFiles:\n",
    "        nameTemp = element[1]\n",
    "        nameTemp = nameTemp[: nameTemp.find(\"-events\")]\n",
    "        with open(element[0], \"r+\") as txtfile:\n",
    "            fileData = txtfile.read()\n",
    "            fileData = fileData.splitlines()\n",
    "            while \"\" in fileData:\n",
    "                fileData.remove(\"\")\n",
    "            tempStart = fileData[0]\n",
    "            tempStart = tempStart.split()\n",
    "            tempStart = tempStart[2]\n",
    "\n",
    "            tempEnd = fileData[-1]\n",
    "            tempEnd = tempEnd.split()\n",
    "            tempEnd = tempEnd[2]\n",
    "\n",
    "            sensorTiming.append([nameTemp, tempStart, tempEnd])\n",
    "            for counter in range(1, len(fileData) - 1):\n",
    "                tempStr = fileData[counter]\n",
    "                tempStr = tempStr.split()\n",
    "                mealTime.append([nameTemp, tempStr[1], tempStr[2]])\n",
    "\n",
    "    dfMeal = pd.DataFrame(mealTime, columns=[\"Name\", \"Start\", \"End\"])\n",
    "    dfMeal = pdFormatter(dfMeal)\n",
    "\n",
    "    dfTime = pd.DataFrame(sensorTiming, columns=[\"Name\", \"Start\", \"End\"])\n",
    "    dfTime = pdFormatter(dfTime)\n",
    "\n",
    "    return dfMeal, dfTime\n",
    "\n",
    "\n",
    "def shimmerReader(element):\n",
    "    nameTemp = element[1]\n",
    "    dataList = []\n",
    "    tempList = []\n",
    "    with open(element[0], mode=\"rb\") as txtfile:\n",
    "        fileData = txtfile.read()\n",
    "        for i in range(int(len(fileData) / 4)):\n",
    "            if i % 6 == 0 and i != 0:\n",
    "                tempList.append(nameTemp)\n",
    "                dataList.append(tempList)\n",
    "                tempList = []\n",
    "            tempVal = fileData[i * 4 : (i + 1) * 4]\n",
    "            tempVal = struct.unpack(\"f\", tempVal)\n",
    "            tempVal = tempVal[0]\n",
    "            tempList.append(tempVal)\n",
    "    txtfile.close()\n",
    "\n",
    "    dfSensor = pd.DataFrame(dataList, columns=[\"X\", \"Y\", \"Z\", \"Yaw\", \"Pitch\", \"Roll\", \"Name\"])\n",
    "    dfSensor = dfSensor[[\"Name\", \"X\", \"Y\", \"Z\", \"Yaw\", \"Pitch\", \"Roll\"]]\n",
    "    del dataList\n",
    "    return dfSensor\n",
    "\n",
    "\n",
    "def timeFinder(dfSensor, dfTime):\n",
    "    dfSensor.insert(1, \"Time\", float(\"nan\"))\n",
    "    name = dfSensor[\"Name\"].tolist()\n",
    "    name = name[0]\n",
    "    dfTemp = dfTime[dfTime[\"Name\"] == name]\n",
    "\n",
    "    if len(dfTemp) > 1:\n",
    "        print(\"More than one event file for:\", name)\n",
    "        return\n",
    "    elif len(dfTemp) == 0:\n",
    "        print(\"No event file for:\", name)\n",
    "        return\n",
    "    startTemp = dfTemp[\"Start\"].tolist()\n",
    "    endTemp = dfTemp[\"End\"].tolist()\n",
    "    tempTimeStamp = np.linspace(startTemp, endTemp, num=len(dfSensor))\n",
    "    dfSensor[\"Time\"] = tempTimeStamp\n",
    "    return dfSensor\n",
    "\n",
    "\n",
    "def labelExtractor(dfMeal, features):\n",
    "    dataTotal = []\n",
    "    for feature in features:\n",
    "        dataTemp = []\n",
    "        windowName = feature[-3]\n",
    "        windowStart = feature[-2]\n",
    "        windowEnd = feature[-1]\n",
    "        dfTemp = dfMeal[dfMeal[\"Name\"] == windowName]\n",
    "        if len(dfTemp) == 0:\n",
    "            print(\"skipped\", windowName)\n",
    "            break\n",
    "        eatingFlag = False\n",
    "        for counter in range(0, len(dfTemp)):\n",
    "            if dfTemp.iloc[counter, 1] < windowEnd and dfTemp.iloc[counter, 2] > windowStart:\n",
    "                eatingFlag = True\n",
    "                break\n",
    "        dataTemp.extend(feature[: len(feature) - 3])\n",
    "        dataTemp.extend([windowName, eatingFlag])\n",
    "        dataTotal.append(dataTemp)\n",
    "\n",
    "    return dataTotal\n",
    "\n",
    "\n",
    "def labelFinder(dfMeals, windowLength, name, start, end):\n",
    "    dfMeal = dfMeals[dfMeals[\"Name\"] == name]\n",
    "    eatingFlag = False\n",
    "    for counter in range(0, len(dfMeal)):\n",
    "        if dfMeal.iloc[counter][\"Start\"] <= end - windowLength / 2 and dfMeal.iloc[counter].loc[\"End\"] >= start + windowLength / 2:\n",
    "            eatingFlag = True\n",
    "            break\n",
    "    return eatingFlag\n",
    "\n",
    "\n",
    "def windowMaker(dfSensor, dfMeal):\n",
    "    windowLength = 60 * 1000\n",
    "    features = []\n",
    "    startTime = dfSensor[\"Time\"].min()\n",
    "    endTime = startTime + windowLength\n",
    "    while startTime < 24 * 3600 * 1000:\n",
    "        dfTemp = dfSensor[dfSensor[\"Time\"] >= startTime]\n",
    "        dfTemp = dfTemp[dfTemp[\"Time\"] < endTime]\n",
    "        if len(dfTemp) > 40 * 15:\n",
    "            eatingFlag = labelFinder(dfMeal, windowLength, dfTemp.iloc[0].loc[\"Name\"], dfTemp[\"Time\"].min(), dfTemp[\"Time\"].max())\n",
    "            dfTemp.insert(8, \"EatingFlag\", eatingFlag)\n",
    "            features.append(featureExtractor(dfTemp))\n",
    "        startTime += windowLength\n",
    "        endTime += windowLength\n",
    "    columnList = [\"Name\", \"EatingFlag\", \"Start\", \"End\", \"F1-Mean\", \"F1-Std\", \"F1-Range\", \"F2-Mean\", \"F2-Std\", \"F2-Range\"]\n",
    "    features = pd.DataFrame(features, columns=columnList)\n",
    "    # print(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def featureExtractor(df):\n",
    "    # cfg = tsfel.get_features_by_domain()\n",
    "    name = df.iloc[0, 0]\n",
    "    start = df.iloc[:, 1].min()\n",
    "    end = df.iloc[:, 1].max()\n",
    "    eatingFlag = df[\"EatingFlag\"].iloc[0]\n",
    "\n",
    "    f2 = df[\"X\"].abs() + df[\"Y\"].abs() + df[\"Z\"].abs() + 0.0001  # To avoid getting nan for F1\n",
    "    f2 = np.asarray(f2)\n",
    "    f1 = df[\"Yaw\"].abs() + df[\"Pitch\"].abs() + df[\"Roll\"].abs()\n",
    "    f1 = np.asarray(f1)\n",
    "    f1 = f1 / f2\n",
    "    featureData = [name, eatingFlag, start, end, np.mean(f1), np.std(f1), np.max(f1) - np.min(f1), np.mean(f2), np.std(f2), np.max(f2) - np.min(f2)]\n",
    "    return featureData\n",
    "\n",
    "\n",
    "def parallelCall(element, dfMeal, dfTime):\n",
    "    dfSensor = shimmerReader(element)\n",
    "    dfSensor = timeFinder(dfSensor, dfTime)\n",
    "    features = windowMaker(dfSensor, dfMeal)\n",
    "    return features\n",
    "\n",
    "\n",
    "def main(addressPrefix):\n",
    "    shimmerFiles = []\n",
    "    dfMeal, dfTime = labelReader(os.path.join(addressPrefix, \"EVENTfiles\"))\n",
    "    for root, dirs, files in os.walk(os.path.join(addressPrefix, \"SHMfiles\"), topdown=False):\n",
    "        for name in files:\n",
    "            if \".shm\" in name:\n",
    "                shimmerFiles.append([os.path.join(root, name), name[:-4]])\n",
    "    # shimmerFiles=shimmerFiles[0:20]\n",
    "    print(\"The number of core to be used:\", coreNumber)\n",
    "    dfFeatures = Parallel(n_jobs=coreNumber)(delayed(parallelCall)(i, dfMeal, dfTime) for i in tqdm(shimmerFiles))\n",
    "    # dfFeatures=parallelCall(shimmerFiles[0],dfMeal,dfTime)\n",
    "    dfFeatures = pd.concat(dfFeatures)\n",
    "    dfFeatures.sort_values(by=[\"Name\", \"Start\"])\n",
    "    dfFeatures.reset_index(drop=True, inplace=True)\n",
    "    dfFeatures.to_csv(os.path.join(addressPrefix, \"dfFeatures.csv\"), index=False)\n",
    "\n",
    "    return dfFeatures\n",
    "\n",
    "if not os.path.exists(os.path.join(addressPrefix, \"dfFeatures.csv\")):\n",
    "    dfFeatures = main(addressPrefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def modelVisualizer(testData, testLabels, modelBest):\n",
    "    slidingWindowPrediction = modelBest.predict(testData)\n",
    "    confMatrix = sklearn.metrics.confusion_matrix(testLabels, slidingWindowPrediction, normalize=\"all\")\n",
    "    accuracy = sklearn.metrics.accuracy_score(testLabels, slidingWindowPrediction)\n",
    "    recall = sklearn.metrics.recall_score(testLabels, slidingWindowPrediction)\n",
    "    precision = sklearn.metrics.precision_score(testLabels, slidingWindowPrediction)\n",
    "\n",
    "    print(\"Testing on test dataset:\")\n",
    "    print(confMatrix)\n",
    "    print(\"Accuracy\", np.round(100 * accuracy, 0), \"Recall\", np.round(100 * recall, 0), \"Precision\", np.round(100 * precision, 0))\n",
    "\n",
    "    titles_options = [(\"Confusion matrix, without normalization\", None), (\"Normalized confusion matrix\", \"true\")]\n",
    "\n",
    "    for title, normalize in titles_options:\n",
    "        disp = plot_confusion_matrix(modelBest, testData, testLabels, display_labels=[\"Non Eating\", \"Eating\"], cmap=plt.cm.Blues, normalize=normalize)\n",
    "        disp.ax_.set_title(title)\n",
    "    plt.show()\n",
    "    return confMatrix\n",
    "\n",
    "\n",
    "def xgClassifier(xTrain, yTrain, xTest, yTest, randomSeed, normalFlag, SMOTEFlag):\n",
    "    print(\"********\")\n",
    "    if normalFlag:\n",
    "        for dimensionCounter in range(xTrain.shape[1]):\n",
    "            xTest[:, dimensionCounter] -= np.mean(xTrain[:, dimensionCounter])\n",
    "            xTest[:, dimensionCounter] /= np.std(xTrain[:, dimensionCounter])\n",
    "\n",
    "            xTrain[:, dimensionCounter] -= np.mean(xTrain[:, dimensionCounter])\n",
    "            xTrain[:, dimensionCounter] /= np.std(xTrain[:, dimensionCounter])\n",
    "\n",
    "    stratShuffle = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=randomSeed)\n",
    "    for trainInd, valInd in stratShuffle.split(xTrain, yTrain):\n",
    "        xTrain, xVal = xTrain[trainInd, :], xTrain[valInd, :]\n",
    "        yTrain, yVal = yTrain[trainInd], yTrain[valInd]\n",
    "\n",
    "    if SMOTEFlag:\n",
    "        oversample = SMOTE()\n",
    "        xTrain, yTrain = oversample.fit_resample(xTrain, yTrain)\n",
    "\n",
    "    print(xTrain.shape, yTrain.shape, xVal.shape, yVal.shape, xTest.shape, yTest.shape)\n",
    "    return np.nan, np.nan, np.nan\n",
    "\n",
    "    f1Best = -1\n",
    "    rocAucScoreBest = -1\n",
    "\n",
    "    for maxDepth in np.arange(3, 7, 1):\n",
    "        for estimator in np.arange(200, 400, 50):\n",
    "            for threshold in np.arange(0.2, 0.8, 0.1):\n",
    "                posWeight = len(yTrain) / np.sum(yTrain)\n",
    "                clf = xgb.XGBClassifier(n_estimators=estimator, max_depth=maxDepth, objective=\"binary:logistic\", scale_pos_weight=posWeight, random_state=randomSeed, n_jobs=coreNumber, eval_metric=\"logloss\", use_label_encoder=False)\n",
    "                clf.fit(xTrain, yTrain)\n",
    "                valPrediction = clf.predict_proba(xVal)\n",
    "                valPrediction = valPrediction[:, 0]\n",
    "                # valPrediction[valPrediction>=threshold]=1\n",
    "                # valPrediction[valPrediction<threshold]=0\n",
    "\n",
    "                rocAucScore = roc_auc_score(yVal, valPrediction, average=\"weighted\")\n",
    "\n",
    "                accuracy = sklearn.metrics.accuracy_score(yVal, valPrediction)\n",
    "                recall = sklearn.metrics.recall_score(yVal, valPrediction)\n",
    "                precision = sklearn.metrics.precision_score(yVal, valPrediction)\n",
    "                f1Score = sklearn.metrics.f1_score(yVal, valPrediction, average=\"weighted\")\n",
    "                print(accuracy, recall, precision, f1Score)\n",
    "\n",
    "                if rocAucScore > rocAucScoreBest:\n",
    "                    rocAucScoreBest = rocAucScore\n",
    "                    modelBest = clf\n",
    "\n",
    "    confMatrixTest = modelVisualizer(xTest, yTest, modelBest)\n",
    "    return clf, f1Score, confMatrixTest\n",
    "\n",
    "\n",
    "# def randomForestClassifier(dataList, labelList, randomSeed):\n",
    "#     trainData, testData, trainLabels, testLabels = train_test_split(dataList, labelList, test_size=0.25, random_state=randomSeed)\n",
    "#     trainData, valData, trainLabels, valLabels = train_test_split(trainData, trainLabels, test_size=0.33, random_state=randomSeed)\n",
    "#     oversample = SMOTE()\n",
    "#     trainData, trainLabels = oversample.fit_resample(trainData, trainLabels)\n",
    "#     f1Best = 0\n",
    "#     for treeNum in np.arange(10, 100, 10):\n",
    "#         for maxDepth in np.arange(3, 10):\n",
    "#             clf = RandomForestClassifier(n_estimators=treeNum, criterion=\"entropy\", random_state=0, max_depth=maxDepth, n_jobs=coreNumber)\n",
    "#             clf.fit(trainData, trainLabels)\n",
    "#             slidingWindowPrediction = clf.predict(valData)\n",
    "#             accuracy = sklearn.metrics.accuracy_score(valLabels, slidingWindowPrediction)\n",
    "#             recall = sklearn.metrics.recall_score(valLabels, slidingWindowPrediction)\n",
    "#             precision = sklearn.metrics.precision_score(valLabels, slidingWindowPrediction)\n",
    "#             f1 = sklearn.metrics.f1_score(valLabels, slidingWindowPrediction, average=\"weighted\")\n",
    "\n",
    "#             if f1 > f1Best:\n",
    "#                 f1Best = f1\n",
    "#                 maxDepthBest = maxDepth\n",
    "#                 treeNumBest = treeNum\n",
    "#                 accuracyBest = accuracy\n",
    "#                 modelBest = clf\n",
    "#                 recallBest = recall\n",
    "#                 precisionBest = precision\n",
    "\n",
    "#     modelVisualizer(testData, testLabels, modelBest)\n",
    "\n",
    "\n",
    "# def dataSplitter(normalFlag,randomSeed):\n",
    "\n",
    "\n",
    "def foldRunnerAux(df, xData, yData, xTrain, yTrain, xTest, yTest):\n",
    "    tempVal = df[\"Train\"].to_list()\n",
    "    tempVal = np.asarray(tempVal).astype(int)\n",
    "    tempVal = tempVal.flatten()\n",
    "    if len(yTrain) == 0:\n",
    "        xTrain = xData[tempVal, :]\n",
    "        yTrain = yData[tempVal]\n",
    "    else:\n",
    "        xTrain = np.concatenate((xTrain, xData[tempVal, :]), axis=0)\n",
    "        yTrain = np.concatenate((yTrain, yData[tempVal]), axis=0)\n",
    "\n",
    "    tempVal = df[\"Test\"].to_list()\n",
    "    tempVal = np.asarray(tempVal).astype(int)\n",
    "    tempVal = tempVal.flatten()\n",
    "    if len(yTest) == 0:\n",
    "        xTest = xData[tempVal, :]\n",
    "        yTest = yData[tempVal]\n",
    "    else:\n",
    "        xTest = np.concatenate((xTest, xData[tempVal, :]), axis=0)\n",
    "        yTest = np.concatenate((yTest, yData[tempVal]), axis=0)\n",
    "\n",
    "    return xTrain, yTrain, xTest, yTest\n",
    "\n",
    "\n",
    "def foldRunner(dfData, posXData, posYData, negXData, negYData, setData):\n",
    "    xTrain = []\n",
    "    yTrain = []\n",
    "    xTest = []\n",
    "    yTest = []\n",
    "\n",
    "    dfTemp = dfData[dfData[\"Set\"] == setData]\n",
    "    dfPos = dfTemp[dfTemp[\"Stat\"] == \"Pos\"]\n",
    "    xTrain, yTrain, xTest, yTest = foldRunnerAux(dfPos, posXData, posYData, xTrain, yTrain, xTest, yTest)\n",
    "\n",
    "    dfTemp = dfData[dfData[\"Set\"] == setData]\n",
    "    dfNeg = dfTemp[dfTemp[\"Stat\"] == \"Neg\"]\n",
    "    xTrain, yTrain, xTest, yTest = foldRunnerAux(dfNeg, negXData, negYData, xTrain, yTrain, xTest, yTest)\n",
    "\n",
    "    return xTrain, yTrain, xTest, yTest\n",
    "\n",
    "\n",
    "def posNegSeparator(df, statLabel):\n",
    "    dfTemp = df[df[\"EatingFlag\"] == statLabel]\n",
    "    yData = dfTemp[\"EatingFlag\"].to_list()\n",
    "    yData = np.asarray(yData).astype(float)\n",
    "\n",
    "    dfTemp.drop(columns=[\"EatingFlag\", \"Name\", \"Start\", \"End\"], inplace=True)\n",
    "    xData = dfTemp.values\n",
    "    xData = np.asarray(xData).astype(float)\n",
    "    return xData, yData\n",
    "\n",
    "\n",
    "def foldMaker(statLabel, yData, dfTemp):\n",
    "    setCounter = 0\n",
    "    kf = KFold(n_splits=5, shuffle=False)\n",
    "    for trainIndex, testIndex in kf.split(yData):\n",
    "        dfTemp.append([statLabel, trainIndex, testIndex, setCounter])\n",
    "        setCounter += 1\n",
    "    return dfTemp\n",
    "\n",
    "\n",
    "def dataSummarizer(f1ScoreSets, confMatrixSets):\n",
    "    f1ScoreSets = np.asarray(f1ScoreSets).astype(float)\n",
    "    f1ScoreSets = np.round(f1ScoreSets, 3)\n",
    "    confMatrixMean = np.mean(np.asarray(confMatrixSets), axis=0)\n",
    "    print(\"All F1 scores\", f1ScoreSets)\n",
    "    print(\"Average F1:\", np.mean(f1ScoreSets))\n",
    "    print(\"Average Conf Matrix:\", confMatrixMean)\n",
    "    accuracy = (confMatrixMean[0][0] + confMatrixMean[1][1]) / (confMatrixMean[0][0] + confMatrixMean[0][1] + confMatrixMean[1][0] + confMatrixMean[1][1])  # (TP+NP)/(TP+NP+FP+FN)\n",
    "    sensitivity = confMatrixMean[1][1] / (confMatrixMean[1][0] + confMatrixMean[1][1])  # TP/(TP+FN)\n",
    "    specificity = confMatrixMean[0][0] / (confMatrixMean[0][0] + confMatrixMean[0][1])  # TN/(TN+FP)\n",
    "    precision = confMatrixMean[1][1] / (confMatrixMean[1][1] + confMatrixMean[0][1])  # TP/(TP+FP)\n",
    "    # CONF Matrix Struct=[TN,FP;FN,TP]\n",
    "    print(\"Average value of accuracy:\", np.round(accuracy, 2), \"\\t recal:\", np.round(sensitivity, 2), \"\\t specificity:\", np.round(specificity, 2), \"\\t precisiion\", np.round(precision, 2))\n",
    "\n",
    "\n",
    "def dataSplitter(SMOTEFlag, normalFlag, randomSeed):\n",
    "    dfData = pd.read_excel(os.path.join(addressPrefix, \"dfFeatures.xlsx\"))\n",
    "    # dfData.sort_values([\"Name\", \"Start\"], ascending=(True, True), inplace=True)\n",
    "    # dfData.reset_index(drop=True, inplace=True)\n",
    "    print('chk1')\n",
    "    xDataPos, yDataPos = posNegSeparator(dfData, 1)\n",
    "    print('chk2')\n",
    "    xDataNeg, yDataNeg = posNegSeparator(dfData, 0)\n",
    "    print('chk3')\n",
    "    dfTemp = []\n",
    "    dfTemp = foldMaker(\"Pos\", yDataPos, dfTemp)\n",
    "    print('chk4')\n",
    "    dfTemp = foldMaker(\"Neg\", yDataNeg, dfTemp)\n",
    "    print('chk5')\n",
    "    dfData = pd.DataFrame(dfData, columns=[\"Stat\", \"Train\", \"Test\", \"Set\"])\n",
    "    setDatas = list(set(dfData[\"Set\"].to_list()))\n",
    "    f1ScoreSets = []\n",
    "    confMatrixSets = []\n",
    "    for setData in setDatas:\n",
    "        xTrain, yTrain, xTest, yTest = foldRunner(dfData, xDataPos, yDataPos, xDataNeg, yDataNeg, setData)\n",
    "        clf, f1ScoreTemp, confMatrixTest = xgClassifier(xTrain, yTrain, xTest, yTest, randomSeed, normalFlag, SMOTEFlag)\n",
    "        f1ScoreSets.append(f1ScoreTemp)\n",
    "        confMatrixSets.append(confMatrixTest)\n",
    "    dataSummarizer(f1ScoreTemp, confMatrixTest)\n",
    "\n",
    "\n",
    "normalFlag = True\n",
    "SMOTEFlag = True\n",
    "randomSeed = 78\n",
    "dataSplitter(SMOTEFlag, normalFlag, randomSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7414333e4a832e37764beae57bbbaf9ec9a12bc709a61fd48cde13ae0db02074"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('miniconda3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
